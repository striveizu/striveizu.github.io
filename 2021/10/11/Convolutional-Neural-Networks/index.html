<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="第六章 卷积神经网络                           一、适合计算机视觉的模型应有特性       不变性是对于处理图像来说非常好的特性。不变性是指目标的外观发生了某种变化，即图像中目标无论是被平移，被旋转，还是被缩放，甚至是不同的光照条件、视角，训练的模型仍然应该将其正确识别。 具体来说，不变性包括：  平移不变性：Translatio">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络">
<meta property="og:url" content="https://striveizu.tech/2021/10/11/Convolutional-Neural-Networks/index.html">
<meta property="og:site_name" content="Strive&#39;s Blog">
<meta property="og:description" content="第六章 卷积神经网络                           一、适合计算机视觉的模型应有特性       不变性是对于处理图像来说非常好的特性。不变性是指目标的外观发生了某种变化，即图像中目标无论是被平移，被旋转，还是被缩放，甚至是不同的光照条件、视角，训练的模型仍然应该将其正确识别。 具体来说，不变性包括：  平移不变性：Translatio">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211011095359772.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211011095224160.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211015175532717.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211015200610183.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017084611169.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017085013039.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017090938756.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017091005294.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017091018257.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017091845369.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017093508072.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017093554956.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017153347989.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017164411266.png">
<meta property="og:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211017172202113.png">
<meta property="article:published_time" content="2021-10-10T16:00:00.000Z">
<meta property="article:modified_time" content="2021-10-29T00:52:54.617Z">
<meta property="article:author" content="Strive">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="CNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://striveizu.tech/images/Convolutional%20Neural%20Networks/image-20211011095359772.png"><title>卷积神经网络 | Strive's Blog</title><link ref="canonical" href="https://striveizu.tech/2021/10/11/Convolutional-Neural-Networks/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-address-card"></i></span><span class="header-nav-menu-item__text">关于</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-edit"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Strive's Blog</div><div class="header-banner-info__subtitle">你我期许的绝非遥不可及</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">卷积神经网络</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-10-11</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-10-29</span></span></div></header><div class="post-body">
        <h3 id="第六章-卷积神经网络">
          <a href="#第六章-卷积神经网络" class="heading-link"><i class="fas fa-link"></i></a><a href="#第六章-卷积神经网络" class="headerlink" title="第六章 卷积神经网络"></a>第六章 卷积神经网络</h3>
      
        <h4 id="一、适合计算机视觉的模型应有特性">
          <a href="#一、适合计算机视觉的模型应有特性" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、适合计算机视觉的模型应有特性" class="headerlink" title="一、适合计算机视觉的模型应有特性"></a>一、适合计算机视觉的模型应有特性</h4>
      <p>不变性是对于处理图像来说非常好的特性。不变性是指目标的外观发生了某种变化，即图像中目标无论是被平移，被旋转，还是被缩放，甚至是不同的光照条件、视角，训练的模型仍然应该将其正确识别。</p>
<p>具体来说，不变性包括：</p>
<ul>
<li>平移不变性：Translation Invariance</li>
<li>旋转/视角不变性：Ratation/Viewpoint Invariance</li>
<li>尺度不变性：Size Invariance</li>
<li>光照不变性：Illumination Invariance</li>
</ul>

        <h5 id="1-1-平移不变性">
          <a href="#1-1-平移不变性" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-平移不变性" class="headerlink" title="1.1 平移不变性"></a>1.1 平移不变性</h5>
      <p>在欧几里得几何中，平移是一种几何变换，表示把一幅图像或一个空间中的每一个点在相同方向移动相同距离。比如对图像分类任务来说，图像中的目标不管被移动到图片的哪个位置，得到的结果（标签）应该是相同的，这就是卷积神经网络中的平移不变性。平移不变性意味着系统产生完全相同的响应（输出），不管它的输入是如何平移的 。</p>

        <h5 id="1-2-旋转不变性">
          <a href="#1-2-旋转不变性" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-旋转不变性" class="headerlink" title="1.2 旋转不变性"></a>1.2 旋转不变性</h5>
      <p>旋转不变性：只要对特征定义了方向，然后在同一个方向上进行特征描述就可以实现旋转不变性。</p>

        <h5 id="1-3-尺度不变性">
          <a href="#1-3-尺度不变性" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-3-尺度不变性" class="headerlink" title="1.3 尺度不变性"></a>1.3 尺度不变性</h5>
      <p>为了实现尺度不变性，需要给特征加上尺度因子。在进行特征描述的时候，将尺度统一就可以实现尺度不变性了。</p>

        <h5 id="对于旋转不变性和尺度不变性的理解">
          <a href="#对于旋转不变性和尺度不变性的理解" class="heading-link"><i class="fas fa-link"></i></a><a href="#对于旋转不变性和尺度不变性的理解" class="headerlink" title="对于旋转不变性和尺度不变性的理解"></a>对于旋转不变性和尺度不变性的理解</h5>
      <p>参考链接：<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://blog.csdn.net/julialove102123/article/details/80822076">特征提取（Detect）、特征描述（Descriptor）、特征匹配（Match）的通俗解释_女王の专属领地-CSDN博客</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>简而言之为了使图像在旋转、放缩后，模型仍然能做出正确的判断(即不会影响模型对此图像的特征提取)，需要给特征指定一个方向与尺度。对于旋转来说，旋转之后特征方向也跟着转，所以相对来说图像的特征方向没有发生变化，如图1所示。指定尺度之后也实现了放缩不影响，如图2。</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211011095359772.png" alt="image-20211011095359772"></p>
<p>​                                                                                                            图1 旋转不变性，指定了特征方向</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211011095224160.png" alt="image-20211011095224160"></p>
<p>​                                                                                                                图2 尺度不变性 </p>

        <h4 id="二、卷积操作">
          <a href="#二、卷积操作" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、卷积操作" class="headerlink" title="二、卷积操作"></a>二、卷积操作</h4>
      
        <h5 id="2-1-卷积运算">
          <a href="#2-1-卷积运算" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-卷积运算" class="headerlink" title="2.1 卷积运算"></a>2.1 卷积运算</h5>
      <p>卷积运算的定义非常简单，将卷积核从输入张量的左上角进行从上到下、从左到右的滑动。每滑动到一个新位置，卷积核与输入张量的对应位置上的元素依次相乘再累加，当卷积核滑动过整个输入张量的时候也就得到了卷积运算的结果。计算结果如图所示：</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211015175532717.png" alt="image-20211015175532717"></p>
<p>​                                                                                                                图3 卷积操作的运算过程</p>
<p>如上图所示，计算卷积操作的第一个结果19 = 0×0 + 1×1 +3×2 + 4×3，依次在这个3*3的输入张量上滑动卷积核得到2×2的卷积输出。</p>

        <h5 id="2-2-输出尺寸的计算">
          <a href="#2-2-输出尺寸的计算" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-输出尺寸的计算" class="headerlink" title="2.2 输出尺寸的计算"></a>2.2 输出尺寸的计算</h5>
      <p>如上图3所示，经过卷积操作之后，输出的尺寸会比原本的输入尺寸有所缩小。输出的尺寸计算公式为</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211015200610183.png" alt="image-20211015200610183"></p>
<p>如上图，3×3的输入张量，2×2的卷积核，（3-2+1）×（3-2+1）=2×2。</p>

        <h5 id="2-3-特征映射（特征图Feature-Map-与感受野">
          <a href="#2-3-特征映射（特征图Feature-Map-与感受野" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-特征映射（特征图Feature-Map-与感受野" class="headerlink" title="2.3 特征映射（特征图Feature Map)与感受野"></a>2.3 特征映射（特征图Feature Map)与感受野</h5>
      <p>卷积层的输出有时也被称为<strong>特征映射Feature Map</strong>。因为卷积层可以视为一个输入映射到下一层的空间维度的转换器。</p>
<p>对于每一层的元素<em><strong>x</strong></em>,其感受野（Receptive Field）是指前向传播期间可能影响x计算的来自所有先前层的所有元素。如上图3所示的卷积操作为例，元素19的感受野来自前置的0，1，3，4四个元素。如果在输出后面再加一层卷积层，卷积核为不变的2×2卷积核，可以得到一个单个元素的输出z，则这个单个元素的感受野为上层的4个元素，上层的4个元素又来自输入的全部9个元素，故感受野包括全部的13个元素。因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个更深的网络。</p>

        <h5 id="2-4-卷积的作用到底是什么">
          <a href="#2-4-卷积的作用到底是什么" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-卷积的作用到底是什么" class="headerlink" title="2.4 卷积的作用到底是什么"></a>2.4 卷积的作用到底是什么</h5>
      <p>回忆下全连接层在做什么：<strong>全连接的核心在于矩阵向量乘积</strong>，公式为<strong>y = Wx + b</strong>。以图片分类举例，采用全连接层首先要把二维的图片<strong>28×28拉成1×784的向量x</strong>，W为预置的参数向量，而<strong>y</strong>是对图片预测的分类label。全连接层完成的功能是提供一组参数w，将图片(向量x)映射到label(y)。</p>
<p><strong>如果将全连接层用于较大的图片分类会出现什么问题？</strong></p>
<ul>
<li><strong>一张28×28的图片，需要的参数数量是784个。即图片的每一个像素都需要一个参数，图片较大的时候参数的数量非常巨大，难以进行训练。很容易出现过拟合等问题</strong></li>
<li><strong>直接使用全连接层需要将图片拉平，在这个操作中像素之间的位置发生了变化，空间信息被破坏。</strong></li>
</ul>
<p>而卷积在做什么？</p>
<p>卷积核是一个在图片上滑动的小块，<strong>在卷积核的每个位置也是一个参数</strong>。卷积核在滑动的时候每次与图片的一个小区域进行计算，在这个过程中图片像素的位置没有发生变化，因此，<strong>在卷积操作中像素之间的空间位置信息得到了很好的保留。</strong></p>
<p>其次，卷积核每次与图片的每个小区域进行相乘再相加操作，计算完成之后得到一个特征映射Feature Map，这个Feature Map可以理解成本层卷积层提取出的特征，<strong>经过多层运算之后也将图片(向量x)映射到label(y)。</strong>所以卷积与全连接的最终都是完成了将图片的特征提取出来经过运算映射到label的功能。</p>
<p>但是在卷积过程中，很多个像素共同使用一个卷积核，即使用同一组参数。实现了<strong>权值共享</strong>，降低了参数的数量。</p>
<p><strong>卷积层可以看作是计算量和准确度的一种妥协。</strong></p>

        <h4 id="三、控制输出尺寸的手段–填充和步幅">
          <a href="#三、控制输出尺寸的手段–填充和步幅" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、控制输出尺寸的手段–填充和步幅" class="headerlink" title="三、控制输出尺寸的手段–填充和步幅"></a>三、控制输出尺寸的手段–填充和步幅</h4>
      <p>由第二章可知，进行卷积操作会改变输入的尺寸。并且输入张量的边界上的特征并没有经过充分的提取，原始图像的边界丢失了很多有用的信息。<strong>填充</strong>（padding）是解决这一问题的有效方法。经过填充操作可以将输出的尺寸控制的与输入尺寸相同。但有时我们我们发现原始的输入分辨率十分冗余。 <em><strong>步幅</strong></em>（stride）则可以在这类情况下提供帮助。</p>

        <h5 id="3-1-填充操作">
          <a href="#3-1-填充操作" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-填充操作" class="headerlink" title="3.1 填充操作"></a>3.1 填充操作</h5>
      <p>在应用多层卷积时，我们常常丢失边缘像素。 由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。 但随着我们应用许多连续卷积层，累积丢失的像素数就多了。 解决这个问题的简单方法即为<em>填充</em>（padding）：在输入图像的边界填充元素（通常填充元素是 0 ）。 如下图所示，对输入图像做填充操作后，输出的尺寸从2×2增加到了4×4。对于边缘的元素来说，以第一个0元素为例，在进行不填充的卷积操作之后（如图3），输入图片的第一个元素0只能映射到输出的第一个元素19上，经过多次卷积操作之后元素0所含的信息可能会丢失。而进行填充操作之后（如下图4），可见第一个0元素分别映射到了输出的0，3，9，19四个元素上。可见边缘信息得到了很好的提取与保留。</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017084611169.png" alt="image-20211017084611169"></p>
<p>​                                                                                    图4 填充之后的卷积操作，填充在原始图像上做了一行和一列的扩充</p>
<p>填充后的尺寸计算公式为：</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017085013039.png" alt="image-20211017085013039"></p>
<p>其中输入的尺寸为nh×nw，卷积核的尺寸为kh×kw，添加 ph 行填充（大约一半在顶部，一半在底部）和 pw列填充（左侧大约一半，右侧一半）。</p>

        <h5 id="3-2-Pytorch的卷积操作的一些参数小细节">
          <a href="#3-2-Pytorch的卷积操作的一些参数小细节" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2-Pytorch的卷积操作的一些参数小细节" class="headerlink" title="3.2 Pytorch的卷积操作的一些参数小细节"></a>3.2 Pytorch的卷积操作的一些参数小细节</h5>
      <p>Pytorch的卷积操作Conv2d的参数如图所示：</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017090938756.png" alt="image-20211017090938756"></p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017091005294.png" alt="image-20211017091005294"></p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017091018257.png" alt="image-20211017091018257"></p>
<p>其实需要特别注意的只有第三张图片，kernel_size,stride,padding的值可以设置为单个int也可以设置成tuple形式。当设置成单个int的形式的时候，如设置kernel_size=3，实际上是将kernel_size设置成3×3，设置成单个int的时候实际上是把该参数的height和width维度设置成相同的int值。当设置成tuple的时候就是按照该tuple来设置高度和宽度。</p>
<p>对于填充padding操作，一个小细节是当padding设置成单个int的时候，实际上是对上下左右四条边上分别填充了int行和int列。如设置padding=1时就是如下图的效果，在使用上述的计算公式的时候要注意，此时的ph应当=2而不是1。因为添加ph行填充一半在顶部一半在底部。padding=1分别对顶部和底部以及左右分别添加了1行或1列。因此ph应该=2。当输入为一个8×8的Tensor的时候，输出的size应该为（8-3+2+1）×（8-3+2+1）=8×8。</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017091845369.png" alt="image-20211017091845369"></p>

        <h5 id="3-3-Tips">
          <a href="#3-3-Tips" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-Tips" class="headerlink" title="3.3 Tips"></a>3.3 Tips</h5>
      <p><strong>卷积神经网络中卷积核的高度和宽度通常为奇数，例如 1、3、5 或 7。</strong> 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。</p>
<p>此外，使用奇数核和填充也提供了书写上的便利。对于任何二维张量 <code>X</code>，当满足： 1. 内核的大小是奇数； 2. 所有边的填充行数和列数相同； 3. 输出与输入具有相同高度和宽度 则可以得出：输出 <code>Y[i, j]</code> 是通过以输入 <code>X[i, j]</code> 为中心，与卷积核进行互相关计算得到的。</p>

        <h5 id="3-4-步幅stride">
          <a href="#3-4-步幅stride" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-4-步幅stride" class="headerlink" title="3.4 步幅stride"></a>3.4 步幅stride</h5>
      <p>在卷积计算中，卷积核从输入张量左上角开始，向下和向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。我们将每次滑动元素的数量称为 <em>步幅</em> （stride）。</p>
<p>可以看到，为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列。但是，当卷积窗口继续向右滑动两列时，没有输出，因为输入元素无法填充窗口（除非我们添加另一列填充）。</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017093508072.png" alt="image-20211017093508072"></p>
<p>通常，当垂直步幅为 sh 、水平步幅为 sw 时，输出形状为</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017093554956.png" alt="image-20211017093554956"></p>

        <h4 id="四、通道">
          <a href="#四、通道" class="heading-link"><i class="fas fa-link"></i></a><a href="#四、通道" class="headerlink" title="四、通道"></a>四、通道</h4>
      <p>标准的彩色图像一般都具有RGB三个通道来分别指示红绿蓝。因此一个标准的RGB图像的张量表示应当是一个三维的张量，size为3×h×w。我们将第一个维度称为通道channel。</p>

        <h5 id="4-1-多输入通道上的卷积操作">
          <a href="#4-1-多输入通道上的卷积操作" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-多输入通道上的卷积操作" class="headerlink" title="4.1 多输入通道上的卷积操作"></a>4.1 多输入通道上的卷积操作</h5>
      <p>一句话概括就是在多输入通道上的卷积操作其实是分别对每个通道上的二维张量对该通道上的卷积核做卷积运算。如下图，以两通道为例：</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017153347989.png" alt="image-20211017153347989"></p>

        <h5 id="4-2-控制输出通道数量的手段–1-1卷积层">
          <a href="#4-2-控制输出通道数量的手段–1-1卷积层" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-2-控制输出通道数量的手段–1-1卷积层" class="headerlink" title="4.2 控制输出通道数量的手段–1*1卷积层"></a>4.2 控制输出通道数量的手段–1*1卷积层</h5>
      <p>由卷积操作的定义可知，卷积操作的本质是提取相邻像素之间的相关特征，因此如果仅对单通道图像采用1×1卷积核做卷积运算似乎并没有什么意义。但是引入多通道之后，1×1的卷积在不同通道上有了累加运算。</p>
<p>1×1卷积的运算规则与普通卷积并没有什么区别。但是当卷积层包含多组1×1卷积核的时候，可以实现控制调整输出的通道数量，即<strong>数据的depth</strong>。如下图所示，输入为3通道的3×3的Tensor，卷积层包括两组3通道的1×1卷积核，最终输入Tensor分别对两组卷积核分别做1×1卷积计算，最终输出的通道数，即depth为2。</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017164411266.png" alt="image-20211017164411266"></p>
<p>卷积的输出输入是长方体，所以1x1卷积实际上是对每个像素点，在不同的channels上进行线性组合（信息整合），且保留了图片的原有平面结构，调控depth，从而完成升维或降维的功能。 如上图所示，如果选择2个filters的1x1卷积层，那么数据就从原本的depth 3 降到了2。若用4个filters，则起到了升维的作用。</p>

        <h4 id="五、池化层">
          <a href="#五、池化层" class="heading-link"><i class="fas fa-link"></i></a><a href="#五、池化层" class="headerlink" title="五、池化层"></a>五、池化层</h4>
      
        <h5 id="5-1-池化操作">
          <a href="#5-1-池化操作" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-1-池化操作" class="headerlink" title="5.1 池化操作"></a>5.1 池化操作</h5>
      <p>与卷积层相似，池化层同样有一个固定大小的窗口<strong>称为（池化窗口）</strong>，并根据步幅大小在输入的所有区域上滑动。通常来说池化运算是池化窗口在输入区域上滑动，求输入张量在池化窗口内的最大值或平均值。<strong>求最大值的称为最大值池化MaxPooling，求平均值的称为平均值池化average pooling</strong>。</p>
<p>在这两种情况下，与卷积操作一样，池化窗口从输入张量的左上角开始，从左到右、从上到下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值，具体取决于是使用了最大池化层还是平均池化层。</p>
<p><img src="/images/Convolutional%20Neural%20Networks/image-20211017172202113.png" alt="image-20211017172202113"></p>

        <h5 id="5-2-池化的意义">
          <a href="#5-2-池化的意义" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-2-池化的意义" class="headerlink" title="5.2 池化的意义"></a>5.2 池化的意义</h5>
      <p>通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率，聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。池化层有以下的一些作用：</p>
<p>（1）首要作用，下采样（downsamping），由于汇合操作的降采样作用，汇合结果中的一个元素对应于原输入数据的一个子区域（sub-region），因此汇合相当于在空间范围内做了维度约减（spatially dimension reduction），从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。</p>
<p>（2）降维、去除冗余信息、对特征进行压缩、简化网络复杂度、减小计算量、减小内存消耗等等。各种说辞吧，总的理解就是减少参数量。</p>
<p>（3）实现非线性（这个可以想一下，relu函数，是不是有点类似的感觉？）。</p>
<p>（4）可以扩大感知野。</p>
<p>（5）可以实现不变性，其中不变形性包括，平移不变性、旋转不变性和尺度不变性。汇合操作使模型更关注是否存在某些特征而不是特征具体的位置<br>，可看作是一种很强的先验，使特征学习包含某种程度自由度，能容忍一些特征微小的位移。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://striveizu.tech">Strive</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://striveizu.tech/2021/10/11/Convolutional-Neural-Networks/">https://striveizu.tech/2021/10/11/Convolutional-Neural-Networks/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://striveizu.tech/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://striveizu.tech/tags/CNN/">CNN</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/10/17/zip/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Python中的zip()函数</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/10/09/week-summary001/"><span class="paginator-prev__text">每周计划与总结001</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">
          第六章 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%80%82%E5%90%88%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BA%94%E6%9C%89%E7%89%B9%E6%80%A7"><span class="toc-number">1.1.</span> <span class="toc-text">
          一、适合计算机视觉的模型应有特性</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="toc-number">1.1.1.</span> <span class="toc-text">
          1.1 平移不变性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-%E6%97%8B%E8%BD%AC%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="toc-number">1.1.2.</span> <span class="toc-text">
          1.2 旋转不变性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-%E5%B0%BA%E5%BA%A6%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="toc-number">1.1.3.</span> <span class="toc-text">
          1.3 尺度不变性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%97%8B%E8%BD%AC%E4%B8%8D%E5%8F%98%E6%80%A7%E5%92%8C%E5%B0%BA%E5%BA%A6%E4%B8%8D%E5%8F%98%E6%80%A7%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">1.1.4.</span> <span class="toc-text">
          对于旋转不变性和尺度不变性的理解</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">
          二、卷积操作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97"><span class="toc-number">1.2.1.</span> <span class="toc-text">
          2.1 卷积运算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-%E8%BE%93%E5%87%BA%E5%B0%BA%E5%AF%B8%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-number">1.2.2.</span> <span class="toc-text">
          2.2 输出尺寸的计算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84%EF%BC%88%E7%89%B9%E5%BE%81%E5%9B%BEFeature-Map-%E4%B8%8E%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">1.2.3.</span> <span class="toc-text">
          2.3 特征映射（特征图Feature Map)与感受野</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%BD%9C%E7%94%A8%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.2.4.</span> <span class="toc-text">
          2.4 卷积的作用到底是什么</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA%E5%B0%BA%E5%AF%B8%E7%9A%84%E6%89%8B%E6%AE%B5%E2%80%93%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">1.3.</span> <span class="toc-text">
          三、控制输出尺寸的手段–填充和步幅</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-%E5%A1%AB%E5%85%85%E6%93%8D%E4%BD%9C"><span class="toc-number">1.3.1.</span> <span class="toc-text">
          3.1 填充操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-Pytorch%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8F%82%E6%95%B0%E5%B0%8F%E7%BB%86%E8%8A%82"><span class="toc-number">1.3.2.</span> <span class="toc-text">
          3.2 Pytorch的卷积操作的一些参数小细节</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-3-Tips"><span class="toc-number">1.3.3.</span> <span class="toc-text">
          3.3 Tips</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-%E6%AD%A5%E5%B9%85stride"><span class="toc-number">1.3.4.</span> <span class="toc-text">
          3.4 步幅stride</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E9%80%9A%E9%81%93"><span class="toc-number">1.4.</span> <span class="toc-text">
          四、通道</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E4%B8%8A%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">1.4.1.</span> <span class="toc-text">
          4.1 多输入通道上的卷积操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93%E6%95%B0%E9%87%8F%E7%9A%84%E6%89%8B%E6%AE%B5%E2%80%931-1%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.4.2.</span> <span class="toc-text">
          4.2 控制输出通道数量的手段–1*1卷积层</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">1.5.</span> <span class="toc-text">
          五、池化层</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#5-1-%E6%B1%A0%E5%8C%96%E6%93%8D%E4%BD%9C"><span class="toc-number">1.5.1.</span> <span class="toc-text">
          5.1 池化操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-2-%E6%B1%A0%E5%8C%96%E7%9A%84%E6%84%8F%E4%B9%89"><span class="toc-number">1.5.2.</span> <span class="toc-text">
          5.2 池化的意义</span></a></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/photo.png" alt="avatar"></div><p class="sidebar-ov-author__text">To be a great person.</p></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2024</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Strive</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>