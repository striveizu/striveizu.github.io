<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="传统网络实现之LeNet                           LeNet简介       LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由 AT&amp;T 贝尔实验室的研究员 Yann LeCun 在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeC">
<meta property="og:type" content="article">
<meta property="og:title" content="传统网络实现之LeNet">
<meta property="og:url" content="https://striveizu.top/2021/10/20/LeNet/index.html">
<meta property="og:site_name" content="Strive&#39;s Blog">
<meta property="og:description" content="传统网络实现之LeNet                           LeNet简介       LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由 AT&amp;T 贝尔实验室的研究员 Yann LeCun 在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeC">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://striveizu.top/images/LeNet/image-20211020191956320.png">
<meta property="og:image" content="https://striveizu.top/images/LeNet/image-20211020192838633.png">
<meta property="og:image" content="https://striveizu.top/images/LeNet/image-20211020195102120.png">
<meta property="og:image" content="https://striveizu.top/images/LeNet/image-20211022205227138.png">
<meta property="og:image" content="https://striveizu.top/images/LeNet/image-20211022210608129.png">
<meta property="article:published_time" content="2021-10-19T16:00:00.000Z">
<meta property="article:modified_time" content="2021-10-22T13:14:27.635Z">
<meta property="article:author" content="Strive">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="网络模型实现">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://striveizu.top/images/LeNet/image-20211020191956320.png"><title>传统网络实现之LeNet | Strive's Blog</title><link ref="canonical" href="https://striveizu.top/2021/10/20/LeNet/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-address-card"></i></span><span class="header-nav-menu-item__text">关于</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-edit"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Strive's Blog</div><div class="header-banner-info__subtitle">你我期许的绝非遥不可及</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">传统网络实现之LeNet</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-10-20</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-10-22</span></span></div></header><div class="post-body">
        <h3 id="传统网络实现之LeNet">
          <a href="#传统网络实现之LeNet" class="heading-link"><i class="fas fa-link"></i></a><a href="#传统网络实现之LeNet" class="headerlink" title="传统网络实现之LeNet"></a>传统网络实现之LeNet</h3>
      
        <h4 id="LeNet简介">
          <a href="#LeNet简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#LeNet简介" class="headerlink" title="LeNet简介"></a>LeNet简介</h4>
      <p>LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由 AT&amp;T 贝尔实验室的研究员 Yann LeCun 在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeCun 发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p>

        <h4 id="1-网络结构">
          <a href="#1-网络结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-网络结构" class="headerlink" title="1 网络结构"></a>1 网络结构</h4>
      <p>LeNet-5的网络结构如下图所示，由结构图可知LeNet是一个较为简单的神经网络，它包含了深度学习的基本模块如卷积层、池化层、全连接层等等。</p>
<img src="/images/LeNet/image-20211020191956320.png" alt="image-20211020191956320" style="zoom:50%;">

<p>​                                图1 LeNet-5的简化版示意图</p>
<p>LeNet的每个卷积块的基本单元是一个卷积层、一个sigmod激活函数和一个平均池化层。每个卷积层使用5×5的卷积核和一个sigmoid激活函数。</p>
<p>本次实验使用的数据集仍然是Fashion-mnist数据集，输入大小为28×28的图片。</p>

        <h4 id="2-代码实现">
          <a href="#2-代码实现" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-代码实现" class="headerlink" title="2 代码实现"></a>2 代码实现</h4>
      
        <h5 id="2-1-使用到的库">
          <a href="#2-1-使用到的库" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-使用到的库" class="headerlink" title="2.1 使用到的库"></a>2.1 使用到的库</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="2-2-定义网络结构">
          <a href="#2-2-定义网络结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-定义网络结构" class="headerlink" title="2.2 定义网络结构"></a>2.2 定义网络结构</h5>
      <p>为了避免错误输入尺寸不是28×28的图片，因此定义的输入层应当具有将数据resize成28×28的格式的功能。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义reshape层，实现的功能为继承Module类，将输入reshape成单通道28*28的黑白图片，第一维是数据的批量大小</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reshape</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># LeNet</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    Reshape(),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(), <span class="comment"># &quot;(28+4-5+1)=28&quot; 公式中的ph实际上是2倍的padding值</span></span><br><span class="line">    <span class="comment"># 因为torch分别在上下左右都加上padding值</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># (28+2-2)/2=14</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),  <span class="comment"># (14-5+1)=10</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># (10+2-2)/2=5</span></span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="2-3-工具函数类">
          <a href="#2-3-工具函数类" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-工具函数类" class="headerlink" title="2.3 工具函数类"></a>2.3 工具函数类</h5>
      <p>定义在SoftMax那章的工具函数类，实现的功能是创建一个Accumulator类，Accumulator类创建的对象有着data属性，它是一个长度为n的列表，可以调用add（）函数实现data列表对应下标累加。在本代码中作用为创建一个Accumulator(3)对象，该列表的两个下标分别保存训练集总损失、训练集预测正确的总数与全部标签的数量，这样就可以利用前两个除以第三个来计算平均loss与预测正确率。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Accumulator</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br></pre></td></tr></table></div></figure>

<p>`</p>
<p>定义两个计算准确率的函数</p>
<p><code>accuracy(y_hat, y)</code>函数实现的功能是统计一批输出的预测的数量。实现的功能是对一个batch_size个数据做预测，输出的y_hat是batch_size个10分类one_hot编码，对它做行方向上的argmax可以得到类别，如[0,0,1,0,0,0,0,0,0,0]作argmax得到预测的类别是第2类。y是标签数据，标注编号描述0T-shirt/top（T恤）1Trouser（裤子）2Pullover（套衫）3Dress（裙子）4Coat（外套）5Sandal（凉鞋）6Shirt（汗衫）7Sneaker（运动鞋）8Bag（包）9Ankle boot（踝靴）。cmp是一个保存y和y_hat相等结果布尔值的tensor，对这个tensor做sum（）可以得到预测正确的数量。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;y_hat为一批预测的张量，为batch_size个长度为10的one_hot编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;对one_hot编码在行（axis0为这批预测的数量）上做argmax得出预测的类别，y为batch_size个实数标签&quot;&quot;&quot;</span></span><br><span class="line">    y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y <span class="comment"># argmax之后的y_hat是一个0-9的实数，与y作比较，相同的</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></div></figure>

<p>`</p>
<p><code>evaluate_accuracy(net, data_iter)</code>函数实现的功能是评估网络在测试集上的准确率。创建一个长度为2的Accumulator()对象，它的data第一个元素保存预测正确的数量，第二个利用y.numel()得到预测的总数并累加到第二个元素，最后利用第一个元素÷第二个元素的到正确率。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>]/metric[<span class="number">1</span>]</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="2-4-初始化网络参数与优化器">
          <a href="#2-4-初始化网络参数与优化器" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-初始化网络参数与优化器" class="headerlink" title="2.4 初始化网络参数与优化器"></a>2.4 初始化网络参数与优化器</h5>
      <p>对线性层和卷积层的参数做Xavier初始化，采用随机梯度下降SGD算法作为优化器，并采用交叉熵损失作为损失函数。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化网络参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="2-5-训练函数">
          <a href="#2-5-训练函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-训练函数" class="headerlink" title="2.5 训练函数"></a>2.5 训练函数</h5>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_eopch</span>(<span class="params">net, train_iter, loss, updater</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        updater.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        updater.step()</span><br><span class="line">        metric.add(l, accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, updater, loss, train_iter, test_iter, num_epoch</span>):</span></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">            train_metrics = train_eopch(net, train_iter, loss, updater)</span><br><span class="line">            test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch:<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,训练集损失:<span class="subst">&#123;train_metrics[<span class="number">0</span>]&#125;</span>,训练集准确率:<span class="subst">&#123;train_metrics[<span class="number">1</span>]&#125;</span>,测试集准确率:<span class="subst">&#123;test_acc&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">20</span></span><br><span class="line">train(net, optimizer, loss, train_iter, test_iter, num_epochs)</span><br></pre></td></tr></table></div></figure>




        <h4 id="3-结果展示">
          <a href="#3-结果展示" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-结果展示" class="headerlink" title="3 结果展示"></a>3 结果展示</h4>
      
        <h5 id="原网络的数据比较">
          <a href="#原网络的数据比较" class="heading-link"><i class="fas fa-link"></i></a><a href="#原网络的数据比较" class="headerlink" title="原网络的数据比较"></a>原网络的数据比较</h5>
      <p>原网络使用了Sigmoid激活函数与平均值池化，20轮epoch，学习率采用0.9，可以见到最终的测试集准确率来到了0.845左右，相较于softmax似乎提升不大</p>
<p><img src="/images/LeNet/image-20211020192838633.png" alt="image-20211020192838633"></p>

        <h5 id="将平均池化改成最大值池化，其他不变，上涨了一点">
          <a href="#将平均池化改成最大值池化，其他不变，上涨了一点" class="heading-link"><i class="fas fa-link"></i></a><a href="#将平均池化改成最大值池化，其他不变，上涨了一点" class="headerlink" title="将平均池化改成最大值池化，其他不变，上涨了一点"></a>将平均池化改成最大值池化，其他不变，上涨了一点</h5>
      <p><img src="/images/LeNet/image-20211020195102120.png" alt="image-20211020195102120"></p>

        <h5 id="保持学习率0-9不变，采取最大值池化-ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况">
          <a href="#保持学习率0-9不变，采取最大值池化-ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况" class="heading-link"><i class="fas fa-link"></i></a><a href="#保持学习率0-9不变，采取最大值池化-ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况" class="headerlink" title="保持学习率0.9不变，采取最大值池化+ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况"></a>保持学习率0.9不变，采取最大值池化+ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况</h5>
      <p><img src="/images/LeNet/image-20211022205227138.png" alt="image-20211022205227138"></p>
<p>百度了相关资料，推测可能是学习率过大且ReLU函数对学习率敏感，较大的学习率可能出现较大的梯度，较大梯度冲击导致神经元死亡。</p>

        <h5 id="尝试降低学习率至0-3">
          <a href="#尝试降低学习率至0-3" class="heading-link"><i class="fas fa-link"></i></a><a href="#尝试降低学习率至0-3" class="headerlink" title="尝试降低学习率至0.3"></a>尝试降低学习率至0.3</h5>
      <p><img src="/images/LeNet/image-20211022210608129.png" alt="image-20211022210608129"></p>
<p>达到了最好成绩约0.89</p>

        <h4 id="4-总结">
          <a href="#4-总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h4>
      <p>LeNet作为较早的卷积神经网络模型，比起只采用一层softmax的方法有了一定的提升。接下来可以尝试采取更深层的网络模型进一步提升精度。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://striveizu.top">Strive</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://striveizu.top/2021/10/20/LeNet/">https://striveizu.top/2021/10/20/LeNet/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://striveizu.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://striveizu.top/tags/CNN/">CNN</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://striveizu.top/tags/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/">网络模型实现</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/10/24/dalunwen/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">《**基于深度学习跨模态行人再识别系统的研究与实现**》阅读</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/10/17/week-summary002/"><span class="paginator-prev__text">每周计划与总结002</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%B9%8BLeNet"><span class="toc-number">1.</span> <span class="toc-text">
          传统网络实现之LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LeNet%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">
          LeNet简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.</span> <span class="toc-text">
          1 网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.3.</span> <span class="toc-text">
          2 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-%E4%BD%BF%E7%94%A8%E5%88%B0%E7%9A%84%E5%BA%93"><span class="toc-number">1.3.1.</span> <span class="toc-text">
          2.1 使用到的库</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.2.</span> <span class="toc-text">
          2.2 定义网络结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-%E5%B7%A5%E5%85%B7%E5%87%BD%E6%95%B0%E7%B1%BB"><span class="toc-number">1.3.3.</span> <span class="toc-text">
          2.3 工具函数类</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-%E5%88%9D%E5%A7%8B%E5%8C%96%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.3.4.</span> <span class="toc-text">
          2.4 初始化网络参数与优化器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-5-%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.5.</span> <span class="toc-text">
          2.5 训练函数</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="toc-number">1.4.</span> <span class="toc-text">
          3 结果展示</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8E%9F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E6%8D%AE%E6%AF%94%E8%BE%83"><span class="toc-number">1.4.1.</span> <span class="toc-text">
          原网络的数据比较</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B0%86%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%E6%94%B9%E6%88%90%E6%9C%80%E5%A4%A7%E5%80%BC%E6%B1%A0%E5%8C%96%EF%BC%8C%E5%85%B6%E4%BB%96%E4%B8%8D%E5%8F%98%EF%BC%8C%E4%B8%8A%E6%B6%A8%E4%BA%86%E4%B8%80%E7%82%B9"><span class="toc-number">1.4.2.</span> <span class="toc-text">
          将平均池化改成最大值池化，其他不变，上涨了一点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BF%9D%E6%8C%81%E5%AD%A6%E4%B9%A0%E7%8E%870-9%E4%B8%8D%E5%8F%98%EF%BC%8C%E9%87%87%E5%8F%96%E6%9C%80%E5%A4%A7%E5%80%BC%E6%B1%A0%E5%8C%96-ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%8C%E5%87%BA%E7%8E%B0%E4%BA%86%E6%8D%9F%E5%A4%B1%E5%BE%88%E5%A4%A7%E5%AE%8C%E5%85%A8%E4%B8%8D%E9%99%8D%E4%BD%8E%EF%BC%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E5%87%86%E7%A1%AE%E7%8E%87%E4%B8%80%E7%9B%B4%E5%BE%88%E4%BD%8E%E7%9A%84%E6%83%85%E5%86%B5"><span class="toc-number">1.4.3.</span> <span class="toc-text">
          保持学习率0.9不变，采取最大值池化+ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E9%99%8D%E4%BD%8E%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%87%B30-3"><span class="toc-number">1.4.4.</span> <span class="toc-text">
          尝试降低学习率至0.3</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%80%BB%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">
          4 总结</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/photo.png" alt="avatar"></div><p class="sidebar-ov-author__text">To be a great person.</p></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2022</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Strive</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>