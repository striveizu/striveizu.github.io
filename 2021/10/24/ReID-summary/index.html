<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="《无监督领域自适应行人重识别研究进展》阅读                           一、无监督领域自适应行人重识别（Unsupervised  Domain  Adaptation Person  Re-       identification, UDA Re-ID）                     1.1 领域自适应 Domain A">
<meta property="og:type" content="article">
<meta property="og:title" content="《无监督领域自适应行人重识别研究进展》阅读">
<meta property="og:url" content="https://striveizu.tech/2021/10/24/ReID-summary/index.html">
<meta property="og:site_name" content="Strive&#39;s Blog">
<meta property="og:description" content="《无监督领域自适应行人重识别研究进展》阅读                           一、无监督领域自适应行人重识别（Unsupervised  Domain  Adaptation Person  Re-       identification, UDA Re-ID）                     1.1 领域自适应 Domain A">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://striveizu.tech/images/ReID-summary/image-20211206143021042.png">
<meta property="og:image" content="https://striveizu.tech/images/ReID-summary/image-20211206095839729.png">
<meta property="og:image" content="https://striveizu.tech/images/ReID-summary/image-20211207161911344.png">
<meta property="og:image" content="https://striveizu.tech/images/ReID-summary/image-20211207161941214.png">
<meta property="article:published_time" content="2021-10-23T16:00:00.000Z">
<meta property="article:modified_time" content="2021-12-07T08:23:20.868Z">
<meta property="article:author" content="Strive">
<meta property="article:tag" content="行人再识别">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="文献阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://striveizu.tech/images/ReID-summary/image-20211206143021042.png"><title>《无监督领域自适应行人重识别研究进展》阅读 | Strive's Blog</title><link ref="canonical" href="https://striveizu.tech/2021/10/24/ReID-summary/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-address-card"></i></span><span class="header-nav-menu-item__text">关于</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-edit"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Strive's Blog</div><div class="header-banner-info__subtitle">你我期许的绝非遥不可及</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">《无监督领域自适应行人重识别研究进展》阅读</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-10-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-07</span></span></div></header><div class="post-body">
        <h3 id="《无监督领域自适应行人重识别研究进展》阅读">
          <a href="#《无监督领域自适应行人重识别研究进展》阅读" class="heading-link"><i class="fas fa-link"></i></a><a href="#《无监督领域自适应行人重识别研究进展》阅读" class="headerlink" title="《无监督领域自适应行人重识别研究进展》阅读"></a>《<strong>无监督领域自适应行人重识别研究进展</strong>》阅读</h3>
      
        <h4 id="一、无监督领域自适应行人重识别（Unsupervised-Domain-Adaptation-Person-Re">
          <a href="#一、无监督领域自适应行人重识别（Unsupervised-Domain-Adaptation-Person-Re" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、无监督领域自适应行人重识别（Unsupervised-Domain-Adaptation-Person-Re" class="headerlink" title="一、无监督领域自适应行人重识别（Unsupervised  Domain  Adaptation Person  Re-"></a>一、无监督领域自适应行人重识别（Unsupervised  Domain  Adaptation Person  Re-</h4>
      <p>identification, UDA Re-ID）</p>

        <h5 id="1-1-领域自适应-Domain-Adaption">
          <a href="#1-1-领域自适应-Domain-Adaption" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-领域自适应-Domain-Adaption" class="headerlink" title="1.1 领域自适应 Domain Adaption"></a>1.1 领域自适应 Domain Adaption</h5>
      <p><em>Domain Adaption</em>是<em>transfer leanring</em>（迁移学习）中很重要的一项内容。主要目的是将具有不同分布的（<em>data distribution</em>）的具有标签（<em>label</em>）的源域（<em>source domain</em>）和不带标签的目标域（<em>target domain</em>） 映射（<em>map</em>）到同一个特征空间（<em>embedding mainfold</em>）。即使训练好的模型能够很好的泛化到其他领域中。</p>
<p><em><strong>Domain</strong></em>可以看作是一个服从相同分布的一类数据，而训练集由一个或多个<em>Domain</em>组成。</p>
<p>DA的一个基础理论如下图所示</p>
<p><img src="/images/ReID-summary/image-20211206143021042.png" alt="image-20211206143021042"></p>
<p>其中<em>Target risk</em>的上界由<em>Source risk，Complexity of H</em>,与<em>Source-target distribution divergence</em>组成，其中<em>Source risk</em>直接由源域给出，<em>Complexity of H</em>指的是模型的复杂程度，一般来说是一个常量，因此一般来说方便优化或者下降的点就是<em>Source-target distribution divergence</em>，即想办法减小源域与目标域分布的差距，所以在训练的过程中<em>DA</em>需要直接访问目标域的数据。</p>

        <h5 id="1-2-无监督">
          <a href="#1-2-无监督" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-无监督" class="headerlink" title="1.2 无监督"></a>1.2 无监督</h5>
      <p>要求其它领域中的数据样本没有与任务有关的标签信息,  无法参与对模型的微调。由于不同领域存在不同的数据分布,  一般将数据的分布差异认为是无监督领域自适应行人重识别的关键问题,  因此研究的重点是让模型可以适应不同领域之间的差异(<em>Domain Gap</em>),减少域差对模型性能的影响。</p>
<p>无监督领域自适应一般包含两个领域（数据集），即训练集是带标签的（有监督的）源域（<em>source domain</em>），和无监督的目标域（<em>Target Domain</em>）。任务目标是能够在无重叠视阈的目标域中能够检索出同一行人.</p>

        <h4 id="二、无监督领域自适应行人重识别的算法">
          <a href="#二、无监督领域自适应行人重识别的算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、无监督领域自适应行人重识别的算法" class="headerlink" title="二、无监督领域自适应行人重识别的算法"></a>二、无监督领域自适应行人重识别的算法</h4>
      <p>无监督领域自适应行人重识别的研究方向大致可以分为以下三类：</p>
<p>1）生成满足其它领域数据分布的伪样本,在数据增广的同时,  能够缩小不同领域之间的领域差异；常见的方法是基于图像的风格迁移，即通过图像的风格迁移模型将有标签的源域目标通过图像的风格迁移生成与目标域相近的风格图片作为满足目标领域数据分布的伪样本投入模型的训练。</p>
<p>2）训练后的模型具有提取高鲁棒领域不变性特征的能力,  缓解领域间的差异；常见的方法是基于表示学习的无监<br>督领域自适应行人重识别。</p>
<p>3）提高目标域中样本特征伪标签生成的准确度,  用于目标域中模型有监督的微调.常见的方法是基于伪标签生成的无监督领域自适应行人重识别。</p>
<p><img src="/images/ReID-summary/image-20211206095839729.png" alt="image-20211206095839729"></p>
<p>​                                                 图1 无监督领域自适应行人重识别的三类研究方向</p>

        <h5 id="2-1-基于图像风格迁移的方法">
          <a href="#2-1-基于图像风格迁移的方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-基于图像风格迁移的方法" class="headerlink" title="2.1 基于图像风格迁移的方法"></a>2.1 基于图像风格迁移的方法</h5>
      
        <h6 id="2-1-1-问题的提出">
          <a href="#2-1-1-问题的提出" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-1-问题的提出" class="headerlink" title="2.1.1 问题的提出"></a>2.1.1 问题的提出</h6>
      <p>一直以来,  在基于无监督的领域自适应行人重识别中,  不同领域内的光照、行人背景、相机角度、行人姿势、分辨率以及行人衣着风格等差异造成的领域差异被专家学者认为是造成重识别性能低的首要原因。因此如何缩小领域差异成为基于无监督的领域自适应行人重识别研究的一项关键科学问题。</p>

        <h6 id="2-1-2-主要方法">
          <a href="#2-1-2-主要方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-2-主要方法" class="headerlink" title="2.1.2 主要方法"></a>2.1.2 主要方法</h6>
      <p>基于图像风格迁移的主要思路是将有监督的源域图像通过神经网络生成具有目标域图像风格的源域图像投入训练，实现减小源域与目标域分布的差距。主要的思路大致有：</p>
<p><strong>1、基于领域差异的风格迁移</strong></p>
<p>生成较高质量的有着目标域图像风格的源域图像,  方便特征提取网络可以有监督的进行训练。</p>
<p><strong>2、基于背景差异的风格迁移</strong></p>
<p>背景差异被认为是造成领域差异的主要因素之一,  将解决无监督领域自适应行人重识别问题的工作重点放在行人图像的背景上。该方法主要训练了一个应用于行人重识别的行人转换生成对抗网络(Person  Transfer  GAN, PTGAN),  该网络能够在尽可能保证行人前景不变的前提下,  将背景转换成期望数据集的图像背景风格。</p>
<p><strong>3、基于相机差异的风格迁移</strong></p>
<p>域间和域内摄像机间由于所在位置和角度的不同而产生的视角差异也是影响识别效果的重要因素。该种方法是对源域相同行人的图片作不同视角差异的风格迁移，生成的图片将用于数据增强以提升网络对因相机视角变化导致样本间特征差异的适应能力。</p>
<p><strong>4、基于其他差异的风格迁移</strong></p>
<p>基于无监督的领域自适应行人重识别中,  除了行人背景差异和相机差异对领域差异造成的巨大影响,  不同领域间行人图像也存在着其它差异, 诸如光照、行人姿势、分辨率的变化.这些差异也是加大领域间差异的原因之一。</p>

        <h5 id="2-2-基于表示学习的方法">
          <a href="#2-2-基于表示学习的方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-基于表示学习的方法" class="headerlink" title="2.2 基于表示学习的方法"></a>2.2 基于表示学习的方法</h5>
      <p>基于表示学习的无监督领域自适应行人重识别<strong>旨在让模型从未标记的跨域数据中学习有效的嵌入空间</strong>,  获得的嵌入特征具有与领域变化无关的高鉴别力和很好的领域适应性。</p>

        <h6 id="2-2-1-基于损失函数的表示学习方法">
          <a href="#2-2-1-基于损失函数的表示学习方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-1-基于损失函数的表示学习方法" class="headerlink" title="2.2.1 基于损失函数的表示学习方法"></a>2.2.1 基于损失函数的表示学习方法</h6>
      <p>基于损失函数的表示学习方法是数据训练过程中利用损失函数来约束网络,  让网络朝着损失函数减小的方向进行优化.基于无监督的领域自适应行人重识别常用的损失函数根据功能进行分类, 可分为分类损失函数、度量学习损失函数和分布损失函数三种类型。</p>
<p>（1）、分类损失函数</p>
<p>分类损失函数是将行人再识别问题看作一个多分类问题。分类损失函数是衡量网络预测和真实分类的一种损失函数,  通过训练使网络具有预测分类的能力。常见的有行人身份损失、行人属性损失和视角不变损失。</p>
<p>行人身份损失是将源域中k个行人身份的N张图片看作K分类问题，用SoftMax进行计算分类损失函数。</p>
<p>行人属性损失是将每张图片都具有M个属性的标注，则行人属性损失可看作是属性二分类预测的累加形式,  即预测图片𝑥是否拥有第𝑚个属性，该损失采用Sigmoid交叉熵损失函数。</p>
<p>视角不变损失是为了增强补贴相机视角下的模型泛化性，减轻目标中不同视角风格的影响。实际工作是将不同摄像机的视角加入考虑，对摄像机进行标签标注，标注源域每张图片来自的摄像机标签。即预测图片来自的相机的索引。</p>
<p>（2）、度量学习损失函数</p>
<p>度量学习损失函数常用于评价数据分布中的距离,  其中对比损失函数(Contrastive loss)、三元组损失函数(Triplet  loss)和四元组损失函数（Quadruplet loss）[31]是行人重识别最常使用度量损失函数。</p>

        <h6 id="2-2-2-基于注意力机制的表示学习方法">
          <a href="#2-2-2-基于注意力机制的表示学习方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-2-基于注意力机制的表示学习方法" class="headerlink" title="2.2.2 基于注意力机制的表示学习方法"></a>2.2.2 基于注意力机制的表示学习方法</h6>
      <p>注意力机制（Attention Mechanism）是机器学习常用的数据处理方法之一,  被广泛应用于自然语言处理、计算机视觉、语音识别 等深度学习任务中。通过对人类注意机制的模仿和模型在大量数据上的训练,  能让模型具备在众多信息中关注对当前任务更关键的信息提取能力。</p>
<p>注意力机制大多应用在弱化行人图像背景、增强行人目标信息等方面。</p>

        <h6 id="2-2-3-基于局部特征的表示方法">
          <a href="#2-2-3-基于局部特征的表示方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-3-基于局部特征的表示方法" class="headerlink" title="2.2.3 基于局部特征的表示方法"></a>2.2.3 基于局部特征的表示方法</h6>
      <p>在无监督领域自适应行人重识别研究中,  一些工作认为局部特征相比全局特征,  不容易受到领域差异的影响,  具有较高的领域适应能力。因此基于局部特征的方法也常作为有效的手段被研究人员所采用。局部特征最常见的获取方式有手动分块或人体关键点定位分块两种形式。</p>

        <h5 id="2-3-基于伪标签生成的方法">
          <a href="#2-3-基于伪标签生成的方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-基于伪标签生成的方法" class="headerlink" title="2.3 基于伪标签生成的方法"></a>2.3 基于伪标签生成的方法</h5>
      <p>由于基于无监督的领域自适应行人重识别中目标域图像不包含行人身份标签,  给基于无监督的领域自适应行人<br>重识别研究带来极大的挑战。</p>
<p>基于伪标签生成的方法大多是训练模型自动在目标域上生成较为可靠的伪标签，利用生成的伪标签去有监督的指导网络优化，提高模型的泛化水平。这里的伪标签不仅仅局限在给予行人对应身份的身份标签（ID label）,  也可以以能够正确的区分样本的正例样本对和负例样本的样本关系形式存在以满足样本度量学习的需要。</p>
<p>一类是基于样本特征间距离比较的排序建立伪标签；第二类是利用在特征空间聚类后形成的簇给予对应的伪标签；最后一类则是综合前两类的优势,  联合排序和聚类的伪标签生成方法。生成的这些伪标签将作为目标域中行人身份标签或正负样本对对已在源域训练后的模型进行微调,  最终获得模型。</p>

        <h6 id="2-3-1-基于排序的伪标签生成方法">
          <a href="#2-3-1-基于排序的伪标签生成方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-1-基于排序的伪标签生成方法" class="headerlink" title="2.3.1 基于排序的伪标签生成方法"></a>2.3.1 基于排序的伪标签生成方法</h6>
      <p>基于排序（Ranking-based）的伪标签生成方法也称为基于邻近（Neighbor-based）的伪标签生成方法,  是一种较易理解的伪标签生成方法,  即根据样本在特征空间间的距离排序来划分类别的一种方法。经过由源域训练后的神经网络模型提取出目标域行人图片的全部特征。并对目标域样本的特征以两两对应的形式作距离（相似度）的计算。距离可以使用马氏距离、欧氏距离以及余弦距离。距离计算之后对距离进行排序，常用的算法有K-近邻（K-NN),相互近邻（𝑘-reciprocal nearest neighbors, 𝑘-RNN ）排序算法以及𝑘互近邻编码（𝑘-reciprocal encoding）,大致的思路均是将距离最邻近的K个样本视为有可能相同的一类，并以此作为伪标签从而微调网络。</p>
<p>而基于距离排序的伪标签生成工作又可以分为基于目标域与源域间的距离排序以及基于目标域的距离排序两种。基于目标域与源域之间的距离排序，由于源域中是有身份标签的。 对目标域无标签的行人图像可根据该图像特征与源域间的多细粒度特征间的余弦距离排序给予对应的源域身份标签.这些筛选过的目标域图像将和源域一起组成训练集,  共同指导模型微调。</p>
<p>其它的基于距离排序的伪标签方法使用的是目标域间距离排序的方式。由于目标域的样本无身份标签,  因此基于目标域的距离排序生成的伪标签是伪关系标签。</p>

        <h6 id="2-3-2-基于聚类的伪标签生成">
          <a href="#2-3-2-基于聚类的伪标签生成" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-2-基于聚类的伪标签生成" class="headerlink" title="2.3.2 基于聚类的伪标签生成"></a>2.3.2 基于聚类的伪标签生成</h6>
      <p>基于聚类的伪标签生成算法会使用聚类算法, 例如𝐾-mean、DBSCAN,  通过样本间距离或者样本分布密度迭代完成聚类,  最终形成的同簇内样本对应相同的行人身份标签,  不同的簇间样本对应不同的行人身份标签。</p>
<p>基于聚类的伪标签生成方法有着理解简单、容易实现的优势,  能够预测出具体的伪身份标签。但是聚类算法对噪声样本较为敏感,  容易对模型的优化和后续的聚类产生影响.因此噪声样本是所有基于聚类的伪标签生成方法亟待解决的挑战之一。</p>

        <h4 id="三、数据集与性能评估指标">
          <a href="#三、数据集与性能评估指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、数据集与性能评估指标" class="headerlink" title="三、数据集与性能评估指标"></a>三、数据集与性能评估指标</h4>
      
        <h5 id="3-1-什么是图库集（Gallery-Set）和探针集-Probe-Set">
          <a href="#3-1-什么是图库集（Gallery-Set）和探针集-Probe-Set" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-什么是图库集（Gallery-Set）和探针集-Probe-Set" class="headerlink" title="3.1 什么是图库集（Gallery Set）和探针集(Probe Set)"></a>3.1 什么是图库集（Gallery Set）和探针集(Probe Set)</h5>
      <p>一般来说图片数据集会分为<strong>训练集（Train Set）与测试集（Test Set)<strong>。而在Re-ID的数据集中一般</strong>测试集</strong>又会细分成<strong>图库集（Gallery Set）和探针集(Probe Set)或查询集</strong>。</p>
<p><strong>无论是gallery还是probe都是仅在测试集出现的概念。</strong>Re-ID的任务是提供一张行人照片，从众多的数据库中寻找与之具有相同身份的图片。</p>
<p>下面参考博客：原文链接：<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44273380/article/details/108949031">https://blog.csdn.net/weixin_44273380/article/details/108949031</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>gallery<br>gallery原意：画廊，这里博主喜欢将它翻译为注册集，也有同学翻译为参考集。就像我们说的“注册”，它的作用就好比一个人脸识别系统，每个人都进去注册了几张自己脸的图像，并和自己的身份绑定起来，从而形成我们上面提到的这个巨大系统的“数据库”。测试的时候，我们需要把一张新的照片去“画廊”，也就是注册数据库中一个一个匹配，得到结果。</p>
<p>这里注意，gallery是只有测试集才有的，因为训练的时候我们的期望是模型能根据两张标注好的图像更好地提取特征，以及判断相似度。这个过程的数据来源是标注好的图像，目标仅仅是训练模型对提供的图片的特征提取能力，也就不需要gallery来提供参考。</p>
<p>probe<br>probe原意：探针、调查。这里博主就通俗地翻译为查询集，就是说，我们在测试的时候是在probe中选取元素来到gallery寻找的，最终测试阶段对模型性能的评估是根据probe中元素查询的效果来反映的。</p>

        <h5 id="3-2-常用数据集">
          <a href="#3-2-常用数据集" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2-常用数据集" class="headerlink" title="3.2 常用数据集"></a>3.2 常用数据集</h5>
      <ul>
<li><p>CUHK03 图像数据集<br>CUHK03是香港中文大学2014 年开源的第一个可供深度学习使用的大型图像行人重识别数据集.数据集采集自香港中文大学校园内的 10 个摄像头,总共收集包含 1467 个行人的 13164 张图片, 行人包围框由 DPM 和手工检测并标示. 在CUHK03 的行人重识别中,分为两种训练测试标准.第一种是随机选出 100 个行人作为测试集,1160个行人作为训练集,100 个行人作为验证集,重复二十次,这种训练测试标准被称为 Single-shot setting;第二种则是类似于 Market-1501,它将数据集分为包含 767 个行人的训练集和包含 700 个行人的测试集。</p>
</li>
<li><p>Market-1501 图像数据集</p>
<p>Market-1501是清华大学 2015 年开源的图像行人重识别数据集。数据集中的行人图像采集自校园超市的6 个摄像头,包含1501 个行人的32668张行人图片。开源者采用 DPM 算法将图片检测和裁剪成128×64 大小的行人图片。数据集分为3 个集合,包括由751 个行人,总计12936 张图片组成的训练集;测试集中则包含剩余 750 个人,总计 12936张图片组成的图库集(Gallery Set),以及750 人的其它3368 张图片组成的探针集(Probe Set)。</p>
</li>
<li><p>DukeMTMC-Re-ID 图像数据集</p>
<p>DukeMTMC-Re-ID是悉尼科技大学2017 年开源的图像行人重识别数据集。数据集采集自 8 个不同角度的摄像头,总计包含1812 个行人的36411张图片。数据集中行人边界框由人工手动裁剪。与Market1501 相同,数据集分为3 个集合,包括由702个行人的 16522 张图像组成的训练集;测试集包含1110 个行人的 17661 张图片组成的图库集和图库集中存在的702 个行人的其它2228 张图片组成探针集。</p>
</li>
<li><p>MSMT17 图像数据集</p>
<p>MSMT17是由北京大学 2019 年开源的大型图像行人重识别数据集.数据集的采集涵盖了多个场景和多个时段,更接近于真实场景.数据集由 15个摄像头采集,包含 12 个户外摄像头和 3 个室内摄像头.采集过程中,选择了具有不同天气条件的 4天时间,涵盖每天早上、中午和下午三个时间段的3 个小时的视频,总计 180 个小时的视频.在数据集整理和标注上,采用 FasterRCNN[81]作为行人检测器,并用了 2 个月的时间进行人工标注,最终得到4101 个行人的 126441 张图片.与 Market1501 和DukeMTMC-Re-ID 不同,数据集中训练集和测试集的行人大约按照 1:3 的比例划分,其中训练集包含1041 个行人的 32621 个包围框,测试集包含 3060个行人的93820 个包围框.其中测试集中,随机选择11659 个包围框作为 ProbeSet,其它 82161 个包围框作为Gallery。        </p>
</li>
</ul>

        <h5 id="3-3性能评价指标">
          <a href="#3-3性能评价指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3性能评价指标" class="headerlink" title="3.3性能评价指标"></a>3.3性能评价指标</h5>
      <p>行人重识别模型性能的评价指标通常采用累计匹配曲线(Cumulative  Match  Characteristics,CMC)和平均精度均值(Mean Average Precision,mAP)进行评估。</p>
<p>累积匹配特征曲线 CMC[k]或 Rank-k表示在测试集中当查找的探针（Probe）在图库集（Gallery Set）中进行距离比较后,  将查询集中行人按照距离的远近由小到大进行排序,  前 k个搜索结果中行人匹配到的概率。例如 Rank-1 表示第一次就能在 Gallery  Set 中正确匹配的 Probe 数量与 Probe Set 数量之比,  Rank-5 表示前五次能在 Gallery  Set中正确匹配的Probe 数量与Probe Set 数量之比。假设测试集探针集总共包含𝑁个行人,  即共进行𝑁次查询和排序后,  每次查询目标行人能匹配到的排序结果用𝑟 = (𝑟1,𝑟2,…,𝑟A)表示,  𝑟i表示探针集中第𝑖个样本在Gallery Set 中正确匹配排序, CMC 曲线表示为：</p>
<p><img src="/images/ReID-summary/image-20211207161911344.png" alt="image-20211207161911344"></p>
<p>平均精度均值是对查询集中查询样本的平均精度（Average  Precision, AP）取平均值,  表示为： </p>
<p><img src="/images/ReID-summary/image-20211207161941214.png" alt="image-20211207161941214"></p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://striveizu.tech">Strive</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://striveizu.tech/2021/10/24/ReID-summary/">https://striveizu.tech/2021/10/24/ReID-summary/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://striveizu.tech/tags/%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB/">行人再识别</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://striveizu.tech/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://striveizu.tech/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/11/02/labsever/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">实验室服务器探索</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/10/24/dalunwen/"><span class="paginator-prev__text">《**基于深度学习跨模态行人再识别系统的研究与实现**》阅读</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%8A%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%E3%80%8B%E9%98%85%E8%AF%BB"><span class="toc-number">1.</span> <span class="toc-text">
          《无监督领域自适应行人重识别研究进展》阅读</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%EF%BC%88Unsupervised-Domain-Adaptation-Person-Re"><span class="toc-number">1.1.</span> <span class="toc-text">
          一、无监督领域自适应行人重识别（Unsupervised  Domain  Adaptation Person  Re-</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94-Domain-Adaption"><span class="toc-number">1.1.1.</span> <span class="toc-text">
          1.1 领域自适应 Domain Adaption</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-%E6%97%A0%E7%9B%91%E7%9D%A3"><span class="toc-number">1.1.2.</span> <span class="toc-text">
          1.2 无监督</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E7%9A%84%E7%AE%97%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">
          二、无监督领域自适应行人重识别的算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.1.</span> <span class="toc-text">
          2.1 基于图像风格迁移的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2-1-1-%E9%97%AE%E9%A2%98%E7%9A%84%E6%8F%90%E5%87%BA"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">
          2.1.1 问题的提出</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-1-2-%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">
          2.1.2 主要方法</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-%E5%9F%BA%E4%BA%8E%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.2.</span> <span class="toc-text">
          2.2 基于表示学习的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2-2-1-%E5%9F%BA%E4%BA%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">
          2.2.1 基于损失函数的表示学习方法</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-2-2-%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">
          2.2.2 基于注意力机制的表示学习方法</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-2-3-%E5%9F%BA%E4%BA%8E%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">
          2.2.3 基于局部特征的表示方法</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-%E5%9F%BA%E4%BA%8E%E4%BC%AA%E6%A0%87%E7%AD%BE%E7%94%9F%E6%88%90%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.3.</span> <span class="toc-text">
          2.3 基于伪标签生成的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2-3-1-%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E7%9A%84%E4%BC%AA%E6%A0%87%E7%AD%BE%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">
          2.3.1 基于排序的伪标签生成方法</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-3-2-%E5%9F%BA%E4%BA%8E%E8%81%9A%E7%B1%BB%E7%9A%84%E4%BC%AA%E6%A0%87%E7%AD%BE%E7%94%9F%E6%88%90"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">
          2.3.2 基于聚类的伪标签生成</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">1.3.</span> <span class="toc-text">
          三、数据集与性能评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%BE%E5%BA%93%E9%9B%86%EF%BC%88Gallery-Set%EF%BC%89%E5%92%8C%E6%8E%A2%E9%92%88%E9%9B%86-Probe-Set"><span class="toc-number">1.3.1.</span> <span class="toc-text">
          3.1 什么是图库集（Gallery Set）和探针集(Probe Set)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.3.2.</span> <span class="toc-text">
          3.2 常用数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-3%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">1.3.3.</span> <span class="toc-text">
          3.3性能评价指标</span></a></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/photo.png" alt="avatar"></div><p class="sidebar-ov-author__text">To be a great person.</p></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Strive</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>