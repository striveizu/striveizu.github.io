<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="一、Softmax回归的相关原理                           1、Softmax的引入       在机器学习和深度学习中，分类和回归是常见的两个问题。其中回归模型往往是通过输入一系列的特征，经过一定的处理输出一个预测值，如通过输入房屋的面积、房间数量等特征通过回归模型可以预测得到这间房屋的价格。而分类问题往往希望通过输入一些特征得到">
<meta property="og:type" content="article">
<meta property="og:title" content="Softmax的Pytorch实现分类任务">
<meta property="og:url" content="http://example.com/2021/09/08/Softmax/index.html">
<meta property="og:site_name" content="Strive&#39;s Blog">
<meta property="og:description" content="一、Softmax回归的相关原理                           1、Softmax的引入       在机器学习和深度学习中，分类和回归是常见的两个问题。其中回归模型往往是通过输入一系列的特征，经过一定的处理输出一个预测值，如通过输入房屋的面积、房间数量等特征通过回归模型可以预测得到这间房屋的价格。而分类问题往往希望通过输入一些特征得到">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/softmax-1.png">
<meta property="og:image" content="http://example.com/images/softmax-2.png">
<meta property="og:image" content="http://example.com/images/softmax-3.png">
<meta property="og:image" content="http://example.com/images/softmax-4.png">
<meta property="og:image" content="http://example.com/images/softmax1.png">
<meta property="og:image" content="http://example.com/images/softmax.png">
<meta property="og:image" content="http://example.com/images/softmax3.png">
<meta property="og:image" content="http://example.com/images/softmax-result.png">
<meta property="article:published_time" content="2021-09-08T14:21:00.000Z">
<meta property="article:modified_time" content="2021-09-15T14:24:13.267Z">
<meta property="article:author" content="Strive">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Softmax">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/softmax-1.png"><title>Softmax的Pytorch实现分类任务 | Strive's Blog</title><link ref="canonical" href="http://example.com/2021/09/08/Softmax/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-address-card"></i></span><span class="header-nav-menu-item__text">关于</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-edit"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Strive's Blog</div><div class="header-banner-info__subtitle">You gotta conquer the monster in your head and then you'll fly.</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Softmax的Pytorch实现分类任务</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-09-08</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-09-15</span></span></div></header><div class="post-body">
        <h4 id="一、Softmax回归的相关原理">
          <a href="#一、Softmax回归的相关原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、Softmax回归的相关原理" class="headerlink" title="一、Softmax回归的相关原理"></a>一、Softmax回归的相关原理</h4>
      
        <h4 id="1、Softmax的引入">
          <a href="#1、Softmax的引入" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、Softmax的引入" class="headerlink" title="1、Softmax的引入"></a>1、Softmax的引入</h4>
      <p>在机器学习和深度学习中，分类和回归是常见的两个问题。其中回归模型往往是通过输入一系列的特征，经过一定的处理输出一个预测值，如通过输入房屋的面积、房间数量等特征通过回归模型可以预测得到这间房屋的价格。而分类问题往往希望通过输入一些特征得到一个分类。如输入一张图像输入这张图像的类别(如是猫还是狗)。在实际的操作中，我们对硬性类别感兴趣，即属于何种类别。但我们往往得到的是软性类别，即得到属于每个类别的概率，概率最大的类别即为类别的预测值。得到这种概率的结果往往并不困难，只需要在简单的线性模型的输出层前套一层softmax函数即可实现。</p>

        <h4 id="2、Softmax函数">
          <a href="#2、Softmax函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、Softmax函数" class="headerlink" title="2、Softmax函数"></a>2、Softmax函数</h4>
      <p>由上所述，分类问题的重点是如何将模型的输出映射成概率。SoftMax函数的功能就是将多个神经元的输出映射到（0，1）的区间内，从而将这种输出看作概率。下图非常清晰的显示了Softmax的计算过程。</p>
<p><img src="/images/softmax-1.png" alt="upload successful"><br>假设有一个数组V，Vi表示数组中第i个元素，则该元素的Softmax值为</p>
<p><img src="/images/softmax-2.png" alt="upload successful"><br>softmax直白来说就是将原来神经元的输出3,1,-3套上softmax函数映射成为取值范围为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测结果进行输出。</p>

        <h4 id="3、交叉熵损失函数">
          <a href="#3、交叉熵损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、交叉熵损失函数" class="headerlink" title="3、交叉熵损失函数"></a>3、交叉熵损失函数</h4>
      <p>在分类问题中，尤其是在神经网络中，交叉熵函数非常常见。因为经常涉及到分类问题，需要计算各类别的概率，所以交叉熵损失函数又都是与sigmoid函数或者softmax函数成对出现。</p>
<p>比如用神经网络最后一层作为概率输出，一般最后一层神经网络的计算方式如下：<br>1.网络的最后一层得到每个类别的scores。<br>2.score与sigmoid函数或者softmax函数进行计算得到概率输出。<br>3.第二步得到的类别概率与真实类别标签的one-hot形式进行交叉熵计算。<br>熵，熵的本质是香农信息量的期望。<br>熵在信息论中代表随机变量不确定度的度量。一个离散型随机变量X的熵 H(X)定义为：</p>
<p><img src="/images/softmax-3.png" alt="upload successful"><br>交叉熵刻画的是实际输出概率和期望输出概率的距离，交叉熵的值越小，则两个概率分布越接近，即实际与期望差距越小。假设概率分布p(xi)为期望输出，概率分布为q(xi)为实际输出，H(X)为交叉熵。则交叉熵的计算表达式为:</p>
<p><img src="/images/softmax-4.png" alt="upload successful"></p>

        <h3 id="二、Fashion-MNIST数据集">
          <a href="#二、Fashion-MNIST数据集" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、Fashion-MNIST数据集" class="headerlink" title="二、Fashion-MNIST数据集"></a>二、Fashion-MNIST数据集</h3>
      
        <h4 id="1、简介">
          <a href="#1、简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4>
      <p>Fashion-MNIST 是一个替代 MNIST 手写数字集 的图像数据集。 它是由 Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自 10 种类别的共 7 万个不同商品的正面图片。<br>Fashion-MNIST 的大小、格式和训练集/测试集划分与原始的 MNIST 完全一致。60000/10000 的训练测试数据划分，28x28 的灰度图片。你可以直接用它来测试你的机器学习和深度学习算法性能，且不需要改动任何的代码。</p>

        <h4 id="2、数据集的内容">
          <a href="#2、数据集的内容" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、数据集的内容" class="headerlink" title="2、数据集的内容"></a>2、数据集的内容</h4>
      <p>训练集和测试集数据的格式相同，通过获取迭代器的第一个元素可以得知，数据格式是一个具有两个元素的列表。第一个列表表示28*28图像的Tensor表示，第二个元素是一个1个元素的Tensor，它的取值为0-9，分别代表10类图像。</p>
<p><img src="/images/softmax1.png" alt="upload successful"></p>
<p><img src="/images/softmax.png" alt="upload successful"></p>

        <h3 id="三、具体代码实现">
          <a href="#三、具体代码实现" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、具体代码实现" class="headerlink" title="三、具体代码实现"></a>三、具体代码实现</h3>
      
        <h4 id="1、使用到的包">
          <a href="#1、使用到的包" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、使用到的包" class="headerlink" title="1、使用到的包"></a>1、使用到的包</h4>
      <p>``</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br></pre></td></tr></table></div></figure>


        <h4 id="2、加载数据集">
          <a href="#2、加载数据集" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、加载数据集" class="headerlink" title="2、加载数据集"></a>2、加载数据集</h4>
      <p>通过Pytorch中的<code>torchvision.datasets</code>提供的函数进行数据集的加载</p>
<p>``</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;获得数据集，并加载成iterable类型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#定义转换函数trans，使用transforms.ToTensor()将图像转化成Tensor形式</span></span><br><span class="line">    <span class="comment">#class torchvision.transforms.ToTensor</span></span><br><span class="line">    <span class="comment">#把一个取值范围是[0,255]的PIL.Image或者shape为(H,W,C)的numpy.ndarray，转换成形状为[C,H,W]，取值范围是[0,1.0]的torch.FloadTensor</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>))</span><br></pre></td></tr></table></div></figure>

<p>其中，使用<code>ToTensor()</code>方法实际上是一种对图像的归一化处理。假设原图像是8位灰度图像，那么读入的像素矩阵最大值为256，最小值为1，定义矩阵为I，J＝I／256，就是归一化的图像矩阵，就是说归一化之后所有的像素值都在［0，1］区间内。</p>
<p>归一化处理的好处在于：<br>(1)归一化能够防止净输入绝对值过大引起的神经元输出饱和现象。<br>(2)归一化可以加快训练网络的收敛性.<del>（实际上不知道为什么要归一化，待进一步学习回来填坑）</del></p>
<p><code>transforms.Compose()</code>可以将多个转换函数组合在一起，参数是一个由转换函数组成的列表，如trans表示的那样。</p>
<p><code>torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=True, transform=trans, download=True)</code><br>函数功能：使用<code>torchvision.datasets</code>提供的数据集加载方法下载并加载MNIST数据集<br>参数的解析：<code>root</code>：存放数据集的目录，<code>train</code>：True表示训练集，False表示测试集。 <code>download</code> : <code>True</code> = 从互联网上下载数据集, <code>transform</code>：转换函数，对数据集图像进行一些处理。</p>
<p><code>data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)</code><br>函数功能：</p>
<p><img src="/images/softmax3.png" alt="upload successful"></p>

        <h4 id="3、计算精确度">
          <a href="#3、计算精确度" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、计算精确度" class="headerlink" title="3、计算精确度"></a>3、计算精确度</h4>
      <p>``</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;函数功能：统计预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;网络中输入batch_size*784,网络线性层784*10，结果为batch_size*10的矩阵，第二个维度为预测结果，最大的索引即为预测类别&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;numpy 返回最大的元素索引，def argmax(a, axis=None, out=None)</span></span><br><span class="line"><span class="string">    a—-输入array</span></span><br><span class="line"><span class="string">    axis—-为0代表列方向，为1代表行方向</span></span><br><span class="line"><span class="string">    out—-结果写到这个array里面&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;y.type:[序号]&quot;&quot;&quot;</span></span><br><span class="line">    y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Accumulator</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#新建一个长度为n的列表，列表中每个元素都是一个待累加的变量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line">    <span class="comment">#add方法实现将data的值与add方法参数传入的值进行加和，zip方法实现将iterable对象打包成元组的列表</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt;a = [1,2,3]</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; b = [4,5,6]</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; c = [4,5,6,7,8]</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; zipped = zip(a,b)     # 打包为元组的列表</span></span><br><span class="line"><span class="string">    [(1, 4), (2, 5), (3, 6)]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()<span class="comment">#将模型置于评估模式，不计算梯度</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        metric.add(accuracy(net(X), y), y.numel())<span class="comment">#numel()返回数组中元素个数</span></span><br><span class="line">    <span class="comment">#此时metric[0]存放的是网络中预测准确的个数，metric[1]存放数据集中的标签的总数</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></div></figure>


        <h4 id="4、定义网络结构">
          <a href="#4、定义网络结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、定义网络结构" class="headerlink" title="4、定义网络结构"></a>4、定义网络结构</h4>
      <p>使用torch提供的<code>nn.Sequential</code>方法创建一个顺序容器，<code>Modules</code> 会以他们传入的顺序被添加到容器中。并使用使用torch提供的<code>nn.init</code>方法对线性层的权值<code>weight</code>初始化为均值为0，标准差为0.01的正态分布。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。实际完成的功能为将28*28的图像矩阵转换成1*784的一维向量。</span></span><br><span class="line"><span class="comment">#FashionMnist数据集为28*28=784的灰度图像，共有10个分类，因此为784*10的线性层</span></span><br><span class="line"><span class="comment">#class torch.nn.Linear(in_features, out_features, bias=True)</span></span><br><span class="line"><span class="comment">#Linear的两个参数：weight和bias</span></span><br><span class="line"><span class="comment">#定义网络结构</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#网络权值初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></div></figure>
<p>通过阅读代码可以发现在顺序容器nn.Sequential里面并没有定义和Softmax相关的层次，实际上Pytorch实现中将交叉熵损失和softmax结合在了一起，网络采用具有10个输出的线性模型即可。</p>

        <h4 id="五、定义优化器与损失函数">
          <a href="#五、定义优化器与损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#五、定义优化器与损失函数" class="headerlink" title="五、定义优化器与损失函数"></a>五、定义优化器与损失函数</h4>
      <p>使用交叉熵损失函数度量预测概率的准确性。并使用随机梯度下降作为模型的优化器。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化器</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></div></figure>


        <h4 id="六、定义训练器与训练函数">
          <a href="#六、定义训练器与训练函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#六、定义训练器与训练函数" class="headerlink" title="六、定义训练器与训练函数"></a>六、定义训练器与训练函数</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment">#统计每一个epoch,损失函数总和、预测正确的数量、样本总数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        y_hat = net(X)<span class="comment">#预测值</span></span><br><span class="line">        l = loss(y_hat, y)<span class="comment">#计算交叉熵损失</span></span><br><span class="line">        updater.zero_grad()<span class="comment">#</span></span><br><span class="line">        l.backward()</span><br><span class="line">        updater.step()</span><br><span class="line">        metric.add(l, accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment">#返回训练损失和训练准确率</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):</span></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">            test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch:<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,训练集损失:<span class="subst">&#123;train_metrics[<span class="number">0</span>]&#125;</span>,训练集准确率:<span class="subst">&#123;train_metrics[<span class="number">1</span>]&#125;</span>,测试集准确率:<span class="subst">&#123;test_acc&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></div></figure>


        <h4 id="七、数据加载与调用训练器进行训练">
          <a href="#七、数据加载与调用训练器进行训练" class="heading-link"><i class="fas fa-link"></i></a><a href="#七、数据加载与调用训练器进行训练" class="headerlink" title="七、数据加载与调用训练器进行训练"></a>七、数据加载与调用训练器进行训练</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></div></figure>

        <h4 id="八、结果展示">
          <a href="#八、结果展示" class="heading-link"><i class="fas fa-link"></i></a><a href="#八、结果展示" class="headerlink" title="八、结果展示"></a>八、结果展示</h4>
      <p><img src="/images/softmax-result.png" alt="upload successful"></p>

        <h4 id="九、总结">
          <a href="#九、总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#九、总结" class="headerlink" title="九、总结"></a>九、总结</h4>
      <p>如结果图展示，仅仅在线性模型的基础上加上一层Softmax并采用交叉熵损失即可实现分类任务。但是准确率较低只有0.85左右。以后可以尝试更多的Epoch和更深的网络层次查看是否可以达到更强的效果。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="http://example.com">Strive</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="http://example.com/2021/09/08/Softmax/">http://example.com/2021/09/08/Softmax/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/Softmax/">Softmax</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/10/08/labsever/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">深度学习计算</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/09/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA-1/"><span class="paginator-prev__text">深度学习环境的搭建</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81Softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">
          一、Softmax回归的相关原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81Softmax%E7%9A%84%E5%BC%95%E5%85%A5"><span class="toc-number">2.</span> <span class="toc-text">
          1、Softmax的引入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81Softmax%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">
          2、Softmax函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">
          3、交叉熵损失函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Fashion-MNIST%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number"></span> <span class="toc-text">
          二、Fashion-MNIST数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">
          1、简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%86%85%E5%AE%B9"><span class="toc-number">2.</span> <span class="toc-text">
          2、数据集的内容</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%85%B7%E4%BD%93%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number"></span> <span class="toc-text">
          三、具体代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E4%BD%BF%E7%94%A8%E5%88%B0%E7%9A%84%E5%8C%85"><span class="toc-number">1.</span> <span class="toc-text">
          1、使用到的包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.</span> <span class="toc-text">
          2、加载数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E8%AE%A1%E7%AE%97%E7%B2%BE%E7%A1%AE%E5%BA%A6"><span class="toc-number">3.</span> <span class="toc-text">
          3、计算精确度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E3%80%81%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">4.</span> <span class="toc-text">
          4、定义网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.</span> <span class="toc-text">
          五、定义优化器与损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%99%A8%E4%B8%8E%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-number">6.</span> <span class="toc-text">
          六、定义训练器与训练函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E8%B0%83%E7%94%A8%E8%AE%AD%E7%BB%83%E5%99%A8%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">7.</span> <span class="toc-text">
          七、数据加载与调用训练器进行训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="toc-number">8.</span> <span class="toc-text">
          八、结果展示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-number">9.</span> <span class="toc-text">
          九、总结</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/photo.png" alt="avatar"></div><p class="sidebar-ov-author__text">To be a great person.</p></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Strive</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>