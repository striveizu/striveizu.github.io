<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="行人重识别需要的一些基础                           一、回顾神经网络的一些基础                           1.1 卷积回顾                           1.1.1 普通卷积       卷积的功能是特征提取。步长不等于0的时候起到下采样的作用。卷积层的参数量的计算公式如下：  一个小">
<meta property="og:type" content="article">
<meta property="og:title" content="行人重识别需要的一些基础">
<meta property="og:url" content="http://example.com/2021/12/27/ReID-base/index.html">
<meta property="og:site_name" content="Strive&#39;s Blog">
<meta property="og:description" content="行人重识别需要的一些基础                           一、回顾神经网络的一些基础                           1.1 卷积回顾                           1.1.1 普通卷积       卷积的功能是特征提取。步长不等于0的时候起到下采样的作用。卷积层的参数量的计算公式如下：  一个小">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224100331547.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224100452800.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224102629624.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224103125148.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224103153758.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224103739036.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224142608718.png">
<meta property="og:image" content="http://example.com/images/ReID-base/dilation.gif">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224150656786.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211224163346434.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225090519930.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225090645290.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225092537689.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225092830122.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225100413384.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225101543620.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225102201277.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225102415691.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225103052466.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225143228526.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225143426922.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225143943301.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225144337836.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225145235952.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225145942198.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225150221293.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211225151200154.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227094927928.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227102409369.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227104924310.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227105706609.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227110756868.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227111029204.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227111131013.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227111507714.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227112344030.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227134125333.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227142426071.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227145701227.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227150111074.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227150524190.png">
<meta property="og:image" content="http://example.com/images/ReID-base/image-20211227151111989.png">
<meta property="article:published_time" content="2021-12-26T16:00:00.000Z">
<meta property="article:modified_time" content="2021-12-27T07:17:10.589Z">
<meta property="article:author" content="Strive">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="行人重识别">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/ReID-base/image-20211224100331547.png"><title>行人重识别需要的一些基础 | Strive's Blog</title><link ref="canonical" href="http://example.com/2021/12/27/ReID-base/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-address-card"></i></span><span class="header-nav-menu-item__text">关于</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-edit"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Strive's Blog</div><div class="header-banner-info__subtitle">你我期许的绝非遥不可及</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">行人重识别需要的一些基础</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-27</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-27</span></span></div></header><div class="post-body">
        <h3 id="行人重识别需要的一些基础">
          <a href="#行人重识别需要的一些基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#行人重识别需要的一些基础" class="headerlink" title="行人重识别需要的一些基础"></a>行人重识别需要的一些基础</h3>
      
        <h4 id="一、回顾神经网络的一些基础">
          <a href="#一、回顾神经网络的一些基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、回顾神经网络的一些基础" class="headerlink" title="一、回顾神经网络的一些基础"></a>一、回顾神经网络的一些基础</h4>
      
        <h4 id="1-1-卷积回顾">
          <a href="#1-1-卷积回顾" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-卷积回顾" class="headerlink" title="1.1 卷积回顾"></a>1.1 卷积回顾</h4>
      
        <h5 id="1-1-1-普通卷积">
          <a href="#1-1-1-普通卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-1-普通卷积" class="headerlink" title="1.1.1 普通卷积"></a>1.1.1 普通卷积</h5>
      <p>卷积的功能是特征提取。步长不等于0的时候起到下采样的作用。卷积层的参数量的计算公式如下：</p>
<p><img src="/images/ReID-base/image-20211224100331547.png" alt="image-20211224100331547"></p>
<p>一个小Tips：填充padding和kernel_size往往需要配套使用，主要设计是让<strong>2*padding+1=k</strong>。从而使输出size只与stride和输入的尺寸有关。举例如kernel_size选择为3*3，一般选择padding=1，k=5选择padding=2。输出特征图的size计算公式如下</p>
<p><img src="/images/ReID-base/image-20211224100452800.png" alt="image-20211224100452800"></p>

        <h5 id="1-1-2-1-1卷积">
          <a href="#1-1-2-1-1卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-2-1-1卷积" class="headerlink" title="1.1.2 1*1卷积"></a>1.1.2 1*1卷积</h5>
      <p><img src="/images/ReID-base/image-20211224102629624.png" alt="image-20211224102629624"></p>
<p>一般不独立使用。主要是作为降维操作使用，如ResNet的BottleNeck层的快速连接中为了考虑训练时间成本对特征图先用1*1卷积层降维再升维。</p>

        <h5 id="1-1-3-分组卷积">
          <a href="#1-1-3-分组卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-3-分组卷积" class="headerlink" title="1.1.3 分组卷积"></a>1.1.3 分组卷积</h5>
      <p>完成的功能：对输入特征图在通道层面平均分成几组，每组在对应的卷积层做卷积之后再在通道维度上做拼接得到输出。</p>
<p><img src="/images/ReID-base/image-20211224103125148.png" alt="image-20211224103125148"></p>
<p>代码实现：修改Conv2d的group参数值</p>
<p><img src="/images/ReID-base/image-20211224103153758.png" alt="image-20211224103153758"></p>
<p>常用的情形：</p>
<p>由于分组卷积需要的参数量较少，常用于模型特别大作为模型压缩的一种手段。常作为对显存不够的一种妥协。</p>
<p>缺点：</p>
<p>由于分组卷积并不是整个feature map参与卷积特征提取，而是每一部分的输出只与一部分的特征图有关系，特征图之间的局部信息没有完全打通。两部分的输出之间可能存在的信息被忽略。</p>

        <h5 id="1-1-4-Channel-wise-Depthwise-Convolution">
          <a href="#1-1-4-Channel-wise-Depthwise-Convolution" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-4-Channel-wise-Depthwise-Convolution" class="headerlink" title="1.1.4 Channel-wise/Depthwise Convolution"></a>1.1.4 Channel-wise/Depthwise Convolution</h5>
      <p><img src="/images/ReID-base/image-20211224103739036.png" alt="image-20211224103739036"></p>

        <h5 id="1-1-5-空洞卷积（Dilated-Convolution">
          <a href="#1-1-5-空洞卷积（Dilated-Convolution" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-5-空洞卷积（Dilated-Convolution" class="headerlink" title="1.1.5 空洞卷积（Dilated Convolution)"></a>1.1.5 空洞卷积（Dilated Convolution)</h5>
      <p>空洞卷积引入了一个称为 “**扩张率(dilation rate)**”的超参数(hyper-parameter)，该参数定义了卷积核处理数据时各值的间距。扩张率中文也叫空洞数(Hole Size)。空洞卷积完成的操作如下图所示：</p>
<p><img src="/images/ReID-base/image-20211224142608718.png" alt="image-20211224142608718"></p>
<p><img src="/images/ReID-base/dilation.gif" alt="image-20211224142608718"></p>
<p>上图中，黑色的圆点表示3×3卷积核，灰色地带表示卷积之后的感受野。a，b，c为：</p>
<ul>
<li>a是普通的卷积过程**(dilation rate = 1)**,卷积后的感受野为3，正常的卷积过程dilation默认值为1</li>
<li>b是dilation rate = 2的空洞卷积,卷积后的感受野为5</li>
<li>c是dilation rate = 3的空洞卷积,卷积后的感受野为7</li>
</ul>
<p>使用空洞卷积的优点在于：不改变输出特征图的尺寸的情况下，增大了感受野。神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；相反，值越小则表示其所包含的特征越趋向局部和细节。因此感受野的值可以用来大致判断每一层的抽象层次。</p>
<p>当每层的卷积核大小设为3×3不变，每一层设置不同的dilation rate，那么每层的感受野也就不同。每层得到的输出特征图中也就获得了多尺度的信息，而且特征图的尺寸并没有发生变化。如果采用较大的卷积核实现扩大感受野，后续还需要加入池化层等下采样从而丢失信息。空洞卷积就避免了这一信息损失。</p>
<p>感受野计算过程如下：</p>
<p><img src="/images/ReID-base/image-20211224150656786.png" alt="image-20211224150656786"></p>

        <h4 id="1-2-模型压缩与加速">
          <a href="#1-2-模型压缩与加速" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-模型压缩与加速" class="headerlink" title="1.2 模型压缩与加速"></a>1.2 模型压缩与加速</h4>
      
        <h5 id="1-2-1-理论基础">
          <a href="#1-2-1-理论基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-1-理论基础" class="headerlink" title="1.2.1 理论基础"></a>1.2.1 理论基础</h5>
      <p>必要性<br> 在许多网络结构中，如VGG-16网络，参数数量1亿3千多万，占用500MB空间，需要进行309亿次浮点运算才能完成一次图像识别任务。</p>
<p>可行性<br> 论文<Predicting parameters in deep learning>提出，其实在很多深度的神经网络中存在着显著的冗余。仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。</Predicting></p>
<p>最终目的<br> 最大程度的减小模型复杂度，减少模型存储需要的空间，也致力于加速模型的训练和推测</p>

        <h5 id="1-2-2-方法分类">
          <a href="#1-2-2-方法分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-2-方法分类" class="headerlink" title="1.2.2 方法分类"></a>1.2.2 方法分类</h5>
      <p><img src="/images/ReID-base/image-20211224163346434.png" alt="image-20211224163346434"></p>

        <h5 id="1-2-3-前端压缩：">
          <a href="#1-2-3-前端压缩：" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-3-前端压缩：" class="headerlink" title="1.2.3 前端压缩："></a>1.2.3 前端压缩：</h5>
      <p>①、知识蒸馏</p>
<p>采取的方法是迁移学习，通过预训练好的教师模型(Teacher Model)的输出作为监督信号取训练另外一个轻量化的网络(Student Model)</p>
<p><img src="/images/ReID-base/image-20211225090519930.png" alt="image-20211225090519930"></p>
<p><img src="/images/ReID-base/image-20211225090645290.png" alt="image-20211225090645290"></p>
<p>学生和老师都对图片分别预测，目的是让学生和老师预测结果的概率分布尽可能像。老师和学生的预测结果p1,p2。对p1和p2计算KL散度作为一种loss加入到学生网络的loss Lc2中共同优化。</p>

        <h4 id="二-行人重识别-度量学习与表征学习">
          <a href="#二-行人重识别-度量学习与表征学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#二-行人重识别-度量学习与表征学习" class="headerlink" title="二 行人重识别   度量学习与表征学习"></a>二 行人重识别   度量学习与表征学习</h4>
      <p><img src="/images/ReID-base/image-20211225092537689.png" alt="image-20211225092537689"></p>

        <h5 id="2-1-系统构成">
          <a href="#2-1-系统构成" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-系统构成" class="headerlink" title="2.1 系统构成"></a>2.1 系统构成</h5>
      <p><img src="/images/ReID-base/image-20211225092830122.png" alt="image-20211225092830122"></p>
<p>原始视频帧进行行人检测模块生成Gallery集，待检索图片集称之为Probe。虽然系统分为两个模块但是学术研究大多只集中在行人重识别的特征提取部分。</p>
<p><img src="/images/ReID-base/image-20211225100413384.png" alt="image-20211225100413384"></p>

        <h5 id="2-2-评价指标">
          <a href="#2-2-评价指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-评价指标" class="headerlink" title="2.2 评价指标"></a>2.2 评价指标</h5>
      <p>①rank-k/top-k</p>
<p><img src="/images/ReID-base/image-20211225101543620.png" alt="image-20211225101543620"></p>
<p>rank-k计算的是整个探针集Probe中的所有图片的命中概率。</p>
<p>对Probe1，和Probe3，Rank1击中，那么整个探针集的rank1就是2/5=0.4</p>
<p>Probe2 Rank-4击中，与Probe1和Probe3加起来就是3/5=0.6的Rank-5</p>
<p>② CMC曲线</p>
<p>其实就是Rank-k的曲线</p>
<p><img src="/images/ReID-base/image-20211225102201277.png" alt="image-20211225102201277"></p>
<p>③ mAP(mean average precision)</p>
<p>反映检索的人在数据库中所有正确图片排在排序列表前面的成都，更加全面的衡量ReID算法的性能。</p>
<p>以下图Probe1为例，排序列表里面有3张正确的身份图片。分别是Top1、Top4、Top9</p>
<p>ap的计算如下图所示，mAP是对Query集里全部的AP取平均。</p>
<p><img src="/images/ReID-base/image-20211225102415691.png" alt="image-20211225102415691"></p>
<p>④ 评价模式</p>
<p><img src="/images/ReID-base/image-20211225103052466.png" alt="image-20211225103052466"></p>

        <h5 id="2-3-表征学习常用的损失Loss">
          <a href="#2-3-表征学习常用的损失Loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-表征学习常用的损失Loss" class="headerlink" title="2.3 表征学习常用的损失Loss"></a>2.3 表征学习常用的损失Loss</h5>
      
        <h6 id="2-3-1-损失的分类">
          <a href="#2-3-1-损失的分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-1-损失的分类" class="headerlink" title="2.3.1 损失的分类"></a>2.3.1 损失的分类</h6>
      <p><img src="/images/ReID-base/image-20211225143228526.png" alt="image-20211225143228526"></p>
<p><img src="/images/ReID-base/image-20211225143426922.png" alt="image-20211225143426922"></p>

        <h6 id="2-3-2-分类损失">
          <a href="#2-3-2-分类损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-2-分类损失" class="headerlink" title="2.3.2 分类损失"></a>2.3.2 分类损失</h6>
      <p><img src="/images/ReID-base/image-20211225143943301.png" alt="image-20211225143943301"></p>
<p>为什么在测试阶段需要丢弃分类FC层？因为在训练集和测试集中的行人ID并不相同，直接用训练集的FC输出分类ID毫无意义。因此在测试阶段直接使用训练的特征提取层提取的Probe的特征向量与Gallery比对检索。</p>

        <h6 id="2-3-3-属性损失">
          <a href="#2-3-3-属性损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-3-属性损失" class="headerlink" title="2.3.3 属性损失"></a>2.3.3 属性损失</h6>
      <p><img src="/images/ReID-base/image-20211225144337836.png" alt="image-20211225144337836"></p>
<p>由于行人具有一系列的属性，比如头发的颜色，上衣的颜色，裤子颜色，鞋子的种类以及颜色等等。这些每一个属性都可以和ID一样作为一个分类来输出分类结果。因此每个属性都可以通过softmax计算交叉熵损失与ID损失一起组成总的loss进行优化。</p>
<p>当然属性损失作为分类损失的类似，在测试阶段由于ID、属性等等与训练集不同，在测试阶段还是将所有的分类FC全部丢弃掉，指使用特征提取层。</p>

        <h6 id="2-3-4-验证损失">
          <a href="#2-3-4-验证损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-4-验证损失" class="headerlink" title="2.3.4 验证损失"></a>2.3.4 验证损失</h6>
      <p><img src="/images/ReID-base/image-20211225145235952.png" alt="image-20211225145235952"></p>
<p>验证损失一般是训练一个特征提取网络，同时输入两张图片提取特征之后将两个特征进行特征融合(比如直接相减计算特征向量的差异)，融合后的特征输入到后续网络通过特征提取计算作为一个二分类问题输出是/否属于一个ID。用于二分类的损失成为验证损失(如上图右上角的Verification Subnet)。</p>
<p>验证损失往往和ID损失一起使用，共同优化整个网络。如右下角的(Classification Subnet)，两张图片可以分别计算ID Loss。</p>

        <h6 id="2-3-5-表征学习的总结">
          <a href="#2-3-5-表征学习的总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-5-表征学习的总结" class="headerlink" title="2.3.5 表征学习的总结"></a>2.3.5 表征学习的总结</h6>
      <p><img src="/images/ReID-base/image-20211225145942198.png" alt="image-20211225145942198"></p>

        <h5 id="2-4-度量学习">
          <a href="#2-4-度量学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-度量学习" class="headerlink" title="2.4 度量学习"></a>2.4 度量学习</h5>
      <p><img src="/images/ReID-base/image-20211225150221293.png" alt="image-20211225150221293"></p>
<p>使用简单化的描述语言来说就是，度量学习ReID任务主要需要做以下工作</p>
<p>①、训练出一个特征提取网络，对图片提取出特征向量。</p>
<p>②、定义一个距离度量损失函数，计算两张图片特征向量之间的度量距离。</p>
<p>③、计算度量损失函数，最优化度量损失函数使相同行人的图片对之间的距离尽可能小，不同行人的图片对之间的距离尽可能大。通过最优化度量损失函数去优化特征提取网络。</p>
<p><em><strong>深度学习解决ReID问题的目标在于提取更优的特征，更加具有度量属性的特征。</strong></em></p>

        <h6 id="2-4-1-度量学习的流程">
          <a href="#2-4-1-度量学习的流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-1-度量学习的流程" class="headerlink" title="2.4.1 度量学习的流程"></a>2.4.1 度量学习的流程</h6>
      <p>基本也是行人检测–特征提取–训练的流程。</p>
<p><img src="/images/ReID-base/image-20211225151200154.png" alt="image-20211225151200154"></p>

        <h5 id="2-5-度量学习的损失函数">
          <a href="#2-5-度量学习的损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-度量学习的损失函数" class="headerlink" title="2.5 度量学习的损失函数"></a>2.5 度量学习的损失函数</h5>
      
        <h6 id="2-5-1-对比损失Contrastive-loss">
          <a href="#2-5-1-对比损失Contrastive-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-1-对比损失Contrastive-loss" class="headerlink" title="2.5.1 对比损失Contrastive loss"></a>2.5.1 对比损失Contrastive loss</h6>
      <p><img src="/images/ReID-base/image-20211227094927928.png" alt="image-20211227094927928"></p>
<p>对比损失的损失函数如上图<em>Lc</em>所示。</p>
<p>当输入的一对图片是正样本对a,b时，因为正样本y=1，<em>Lc</em>的后半部分为0，优化目标就是<em>Lc</em>的前半部分，也就是a和b特征向量的距离。通过优化器最小化损失函数使正样本对间的距离趋于0。</p>
<p>当输入的一对图片是负样本对a,b时，因为负样本y=0，<em>Lc</em>的前半部分为0，优化目标就是<em>Lc</em>的后半部分。由于<img src="/images/ReID-base/image-20211227102409369.png" alt="image-20211227102409369" style="zoom:40%;"></p>
<p>z = α-距离。当二者之间的距离大于α时, z小于0，max(z, 0) = 0，无需优化。当二者之间的距离越小于α时, z大于0， <em>Lc</em>越大，优化器将优化 <em>Lc</em>使其向0的方向优化，即使二者之间的距离向大于α的方向优化。也就实现了推开负样本的期望功能。其中α是自己设置的一个超参数，作为负样本分开的阈值。</p>

        <h6 id="2-5-2-三元组损失Triplet-loss">
          <a href="#2-5-2-三元组损失Triplet-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-2-三元组损失Triplet-loss" class="headerlink" title="2.5.2 三元组损失Triplet loss"></a>2.5.2 三元组损失Triplet loss</h6>
      <p><img src="/images/ReID-base/image-20211227104924310.png" alt="image-20211227104924310"></p>
<p>三元组损失主要将样本分为Anchor、Positive、Negative。一个三元组的构成方式是从训练集中随机选取一个样本，该样本称为Anchor，记为x_a， 然后再随机抽取一个与Anchor属于同一类的样本x_p和一个与Anchor属于不同类的样本x_n。由此构成一个(Anchor、Positive、Negative)三元组。Triplet Loss的目的在于通过学习，让x_a和x_p特征表达之间的距离尽可能小，而x_a和x_n之间的距离尽可能大。并且要让_a和x_n之间的距离和x_a和x_p之间的距离之间存在一个最小间隔α。即目标函数的优化方向为：</p>
<p><img src="/images/ReID-base/image-20211227105706609.png" alt="image-20211227105706609"></p>
<p>因此损失函数的形式为</p>
<p><img src="/images/ReID-base/image-20211227110756868.png" alt="image-20211227110756868"></p>
<p><img src="/images/ReID-base/image-20211227111029204.png" alt="image-20211227111029204"></p>

        <h6 id="2-5-3-改进的三元组损失">
          <a href="#2-5-3-改进的三元组损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-3-改进的三元组损失" class="headerlink" title="2.5.3 改进的三元组损失"></a>2.5.3 改进的三元组损失</h6>
      <p><img src="/images/ReID-base/image-20211227111131013.png" alt="image-20211227111131013"></p>
<p>其实就在原三元组损失的基础上加上了一项x_a和x_p之间的距离。使得在满足三元组损失优化目标的同时使x_a和x_p之间的距离尽可能的小。</p>

        <h6 id="2-5-4-四元组损失-Quadruplet-loss">
          <a href="#2-5-4-四元组损失-Quadruplet-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-4-四元组损失-Quadruplet-loss" class="headerlink" title="2.5.4 四元组损失 Quadruplet loss"></a>2.5.4 四元组损失 Quadruplet loss</h6>
      <p><img src="/images/ReID-base/image-20211227111507714.png" alt="image-20211227111507714"></p>
<p>四元组损失也是基于三元组损失的基础上，但是需要四张图片组成一个四元组。一个四元组的构成为从训练集中随机选取一个样本，该样本称为Anchor，记为x_a， 然后再随机抽取一个与Anchor属于同一类的样本x_p和<strong>两个</strong>与Anchor属于不同类的样本x_n1和x_n2。并且这两个负样本n1和n2<strong>分别属于两个不同的ID</strong>。</p>
<p>四元组损失的公式如上图所示。第一项使正常的三元组损失。而第二项分别计算Anchor和正样本的距离da,p和两个负样本的距离dn1,n2。使得正样本之间的距离不仅小于正、负样本对的距离，也小于两张来自不同类负样本之间的距离。</p>
<p>功能：</p>
<p>进一步缩小正样本之间的距离，在推开正负样本对的距离的同时推开不同类的负样本对之间的距离。</p>

        <h6 id="2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失">
          <a href="#2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失" class="headerlink" title="2.5.5 TriHard Loss/Batch-Hard Loss 批难三元组损失"></a>2.5.5 TriHard Loss/Batch-Hard Loss 批难三元组损失</h6>
      <p><img src="/images/ReID-base/image-20211227112344030.png" alt="image-20211227112344030"></p>
<p><img src="/images/ReID-base/image-20211227134125333.png" alt="image-20211227134125333"></p>
<p><img src="/images/ReID-base/image-20211227142426071.png" alt="image-20211227142426071"></p>
<p>如图所示，TriHard Loss实现的方式是构建这样一个距离矩阵。如上图，每个batch选取N个ID的行人，每个ID选取3张图片。将这个batch内的全部图片两两计算距离填入上方的矩阵。对角线上表示同一张图片自己到自己的距离，因此为0。红色矩阵块表示同一个ID的三张图片之间的距离，绿色矩阵块表示不同ID的图片之间的距离。衡量是否Hrad然是通过距离来衡量。<strong>对于红色矩阵，也就是正样本对来说，红色矩阵的每一行的最大值，以第一行为例，这一行代表行人1-1这张图片与三个正样本的距离，对于正样本来说，正样本之间的距离越大，说明越Hard。</strong>因此将红色矩阵组合起来求每一行的最大值组成一列，这一列就代表了这个batch里面的每一张图片与其对应的最Hard样本的距离。</p>
<p><strong>同理，对正负样本对来说，距离越小越Hard</strong>。将绿色矩阵拼在一起组成方阵，求每一行的最小值。每一行的最小值组成的一列Tensor就是正负样本对的最难距离。</p>
<p><img src="/images/ReID-base/image-20211227145701227.png" alt="image-20211227145701227"></p>
<p>根据上述公式，累加之后除以批量大小就得到了TriHard loss。</p>

        <h6 id="2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组">
          <a href="#2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组" class="headerlink" title="2.5.6 Triplet loss with adaptive weights 自适应权重三元组"></a>2.5.6 Triplet loss with adaptive weights 自适应权重三元组</h6>
      <p>批难三元组由于只考虑了极端样本的信息，有些信息没有考虑到。</p>
<p>有文章指出批难三元组损失在某些极端情况下，比如数据集有一定数量的标注错误可能会因为训练时梯度特别大而导致网络崩溃。当网络使用批难三元组难以收敛的情况下可以考虑自适应权重三元组。</p>
<p><img src="/images/ReID-base/image-20211227150111074.png" alt="image-20211227150111074"></p>

        <h6 id="2-5-7-度量学习的总结">
          <a href="#2-5-7-度量学习的总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-7-度量学习的总结" class="headerlink" title="2.5.7 度量学习的总结"></a>2.5.7 度量学习的总结</h6>
      <p><img src="/images/ReID-base/image-20211227150524190.png" alt="image-20211227150524190"></p>
<p>度量学习和表征学习往往可以用来共同训练一个模型，将多个损失组合在一起共同优化一个模型。这种做法也是业界精确度比较高的一种做法。</p>
<p><img src="/images/ReID-base/image-20211227151111989.png" alt="image-20211227151111989"></p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="http://example.com">Strive</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="http://example.com/2021/12/27/ReID-base/">http://example.com/2021/12/27/ReID-base/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/CNN/">CNN</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB/">行人重识别</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/12/28/ReID-features/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">行人重识别常用的局部特征</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/12/24/C-Chapter2/"><span class="paginator-prev__text">C++Primer 第二章学习笔记</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E9%9C%80%E8%A6%81%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">
          行人重识别需要的一些基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%9B%9E%E9%A1%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80"><span class="toc-number">1.1.</span> <span class="toc-text">
          一、回顾神经网络的一些基础</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E5%8D%B7%E7%A7%AF%E5%9B%9E%E9%A1%BE"><span class="toc-number">1.2.</span> <span class="toc-text">
          1.1 卷积回顾</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-1-%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.2.1.</span> <span class="toc-text">
          1.1.1 普通卷积</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-2-1-1%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.2.2.</span> <span class="toc-text">
          1.1.2 1*1卷积</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-3-%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.2.3.</span> <span class="toc-text">
          1.1.3 分组卷积</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-4-Channel-wise-Depthwise-Convolution"><span class="toc-number">1.2.4.</span> <span class="toc-text">
          1.1.4 Channel-wise&#x2F;Depthwise Convolution</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-5-%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%EF%BC%88Dilated-Convolution"><span class="toc-number">1.2.5.</span> <span class="toc-text">
          1.1.5 空洞卷积（Dilated Convolution)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E5%8A%A0%E9%80%9F"><span class="toc-number">1.3.</span> <span class="toc-text">
          1.2 模型压缩与加速</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-1-%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="toc-number">1.3.1.</span> <span class="toc-text">
          1.2.1 理论基础</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-2-%E6%96%B9%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">1.3.2.</span> <span class="toc-text">
          1.2.2 方法分类</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-3-%E5%89%8D%E7%AB%AF%E5%8E%8B%E7%BC%A9%EF%BC%9A"><span class="toc-number">1.3.3.</span> <span class="toc-text">
          1.2.3 前端压缩：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.</span> <span class="toc-text">
          二 行人重识别   度量学习与表征学习</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-%E7%B3%BB%E7%BB%9F%E6%9E%84%E6%88%90"><span class="toc-number">1.4.1.</span> <span class="toc-text">
          2.1 系统构成</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">1.4.2.</span> <span class="toc-text">
          2.2 评价指标</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1Loss"><span class="toc-number">1.4.3.</span> <span class="toc-text">
          2.3 表征学习常用的损失Loss</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2-3-1-%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">
          2.3.1 损失的分类</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-3-2-%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">
          2.3.2 分类损失</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-3-3-%E5%B1%9E%E6%80%A7%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.3.3.</span> <span class="toc-text">
          2.3.3 属性损失</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-3-4-%E9%AA%8C%E8%AF%81%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.3.4.</span> <span class="toc-text">
          2.3.4 验证损失</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-3-5-%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.3.5.</span> <span class="toc-text">
          2.3.5 表征学习的总结</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.4.</span> <span class="toc-text">
          2.4 度量学习</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2-4-1-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="toc-number">1.4.4.1.</span> <span class="toc-text">
          2.4.1 度量学习的流程</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-5-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.5.</span> <span class="toc-text">
          2.5 度量学习的损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2-5-1-%E5%AF%B9%E6%AF%94%E6%8D%9F%E5%A4%B1Contrastive-loss"><span class="toc-number">1.4.5.1.</span> <span class="toc-text">
          2.5.1 对比损失Contrastive loss</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-5-2-%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1Triplet-loss"><span class="toc-number">1.4.5.2.</span> <span class="toc-text">
          2.5.2 三元组损失Triplet loss</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-5-3-%E6%94%B9%E8%BF%9B%E7%9A%84%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.5.3.</span> <span class="toc-text">
          2.5.3 改进的三元组损失</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-5-4-%E5%9B%9B%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1-Quadruplet-loss"><span class="toc-number">1.4.5.4.</span> <span class="toc-text">
          2.5.4 四元组损失 Quadruplet loss</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-5-5-TriHard-Loss-Batch-Hard-Loss-%E6%89%B9%E9%9A%BE%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.4.5.5.</span> <span class="toc-text">
          2.5.5 TriHard Loss&#x2F;Batch-Hard Loss 批难三元组损失</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-5-6-Triplet-loss-with-adaptive-weights-%E8%87%AA%E9%80%82%E5%BA%94%E6%9D%83%E9%87%8D%E4%B8%89%E5%85%83%E7%BB%84"><span class="toc-number">1.4.5.6.</span> <span class="toc-text">
          2.5.6 Triplet loss with adaptive weights 自适应权重三元组</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-5-7-%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.5.7.</span> <span class="toc-text">
          2.5.7 度量学习的总结</span></a></li></ol></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/photo.png" alt="avatar"></div><p class="sidebar-ov-author__text">To be a great person.</p></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2022</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Strive</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>