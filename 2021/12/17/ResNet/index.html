<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="经典网络架构实现之ResNet                           ResNet简介       从AlexNet再到VGG，现代卷积神经网络发展的一个重点就是深度Depth。正如AlexNet的论文Imagenet-Classification-with-Deep-Convolutional-Neural-Networks的discussi">
<meta property="og:type" content="article">
<meta property="og:title" content="传统网络实现之ResNet">
<meta property="og:url" content="http://example.com/2021/12/17/ResNet/index.html">
<meta property="og:site_name" content="Strive&#39;s Blog">
<meta property="og:description" content="经典网络架构实现之ResNet                           ResNet简介       从AlexNet再到VGG，现代卷积神经网络发展的一个重点就是深度Depth。正如AlexNet的论文Imagenet-Classification-with-Deep-Convolutional-Neural-Networks的discussi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/ResNet/tu1.png">
<meta property="og:image" content="http://example.com/images/ResNet/tu2.png">
<meta property="og:image" content="http://example.com/images/ResNet/tu3.png">
<meta property="og:image" content="http://example.com/images/ResNet/tu4.png">
<meta property="og:image" content="http://example.com/images/ResNet/tu5.png">
<meta property="og:image" content="http://example.com/images/ResNet/tu6.png">
<meta property="og:image" content="http://example.com/images/ResNet/tu7.png">
<meta property="og:image" content="http://example.com/images/ResNet/tu8.jpg">
<meta property="og:image" content="http://example.com/images/ResNet/image-20211217211206565.png">
<meta property="og:image" content="http://example.com/images/ResNet/image-20211217211249813.png">
<meta property="og:image" content="http://example.com/images/ResNet/image-20211217211811571.png">
<meta property="og:image" content="http://example.com/images/ResNet/image-20211217211842242.png">
<meta property="og:image" content="http://example.com/images/ResNet/image-20211217211953650.png">
<meta property="article:published_time" content="2021-12-16T16:00:00.000Z">
<meta property="article:modified_time" content="2021-12-17T13:20:12.304Z">
<meta property="article:author" content="Strive">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="网络模型实现">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/ResNet/tu1.png"><title>传统网络实现之ResNet | Strive's Blog</title><link ref="canonical" href="http://example.com/2021/12/17/ResNet/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-address-card"></i></span><span class="header-nav-menu-item__text">关于</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-edit"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Strive's Blog</div><div class="header-banner-info__subtitle">你我期许的绝非遥不可及</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">传统网络实现之ResNet</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-17</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-17</span></span></div></header><div class="post-body">
        <h3 id="经典网络架构实现之ResNet">
          <a href="#经典网络架构实现之ResNet" class="heading-link"><i class="fas fa-link"></i></a><a href="#经典网络架构实现之ResNet" class="headerlink" title="经典网络架构实现之ResNet"></a>经典网络架构实现之ResNet</h3>
      
        <h4 id="ResNet简介">
          <a href="#ResNet简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#ResNet简介" class="headerlink" title="ResNet简介"></a>ResNet简介</h4>
      <p>从AlexNet再到VGG，现代卷积神经网络发展的一个重点就是深度<em><strong>Depth</strong></em>。正如AlexNet的论文<em><strong>Imagenet-Classification-with-Deep-Convolutional-Neural-Networks</strong></em>的discussion部分所述：<em>So the depth really is important for achieving our results。</em>从常规的思路上来想，更深的网络不应该比较浅的网络有着更差的性能表现。神经网络模型可以视作一个函数F，假设一个已经训练好的较浅层的模型为f(x)，在这个较浅层的网络模型的基础上再加上一些层数，只要优化器将网络将后续的层数的参数设成0或者接近0，即将此层训练成一个恒等映射，直接输出上层的结果，即实现一个嵌套式的网络，那么深层网络所达到的效果起码不会比原浅层模型差，只会更加接近最优。</p>
<p><img src="/images/ResNet/tu1.png" alt="image-20211217151707369"></p>
<p>​                                                                                            图1 嵌套网络与普通的深层网络的区别</p>
<p>随着网络深度的不断增加，一系列的问题随之而来。其中影响深度难以继续叠加的一个非常重要的问题就是网络的层数越深，训练的难度也更大。具体则表现在梯度消失等问题。ResNet提出的Residual块在一定程度上缓解了这个问题。具体见下文。</p>

        <h4 id="一、ResNet的基本架构–Residual块">
          <a href="#一、ResNet的基本架构–Residual块" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、ResNet的基本架构–Residual块" class="headerlink" title="一、ResNet的基本架构–Residual块"></a>一、ResNet的基本架构–Residual块</h4>
      
        <h5 id="1-1残差块的结构">
          <a href="#1-1残差块的结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1残差块的结构" class="headerlink" title="1.1残差块的结构"></a>1.1残差块的结构</h5>
      <p>上文说到，训练出一个嵌套式的网络，那么深层的模型要比浅层的模型理论上应该有着更好的表现。将新添加的层训练出一个恒等映射（identity mapping），使f(x)=x，新模型和原模型将同样有效。根据这种思路，ResNet提出了残差块（Residual block）的架构。残差块是ResNet的基本组成单位，它的具体结构如下图。</p>
<p><img src="/images/ResNet/tu2.png" alt="image-20211217154218707"></p>
<p>​                                                                图2 一个普通的卷积神经网络块（左）和残差块（右）</p>
<p>假设原神经网络块的函数表示为<strong>f(x)<strong>，即待优化的函数为</strong>f(x)<strong>。而残差块的函数表示为</strong>h(x)=f(x)-x</strong>，二者向下层传递的计算结果都是f(x)，但残差块引入了“shortcut connections”，将网络块的输入通过这个连接直接传输到网络的输出处，与函数块的输出h(x)在通道维度做加和之后作为残差块的总输出。即神经网络需要学习并优化的是函数<strong>h(x)=f(x)-x</strong>。这样的结构设计往往带来优化上的便利。假设之前提到的恒等映射作为我们希望学出的理想映射f(x)，我们只需将上图中右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。 实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。在残差块中，输入可通过跨层数据线路更快地向前传播。</p>

        <h5 id="1-2-残差连接好优化的一种数学上的解释">
          <a href="#1-2-残差连接好优化的一种数学上的解释" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-残差连接好优化的一种数学上的解释" class="headerlink" title="1.2 残差连接好优化的一种数学上的解释"></a>1.2 残差连接好优化的一种数学上的解释</h5>
      <p>下面给出一种残差块优化便利的数学解释。众所周知层次很深的网络容易出现梯度消失的问题。由上文所述神经网络的每一个块都可以看成一个函数f(x)，假设一个两块的网络可以表示成f(g(x))。在反向传播算法里面，f(g(x))对x计算梯度时由链式法则可知需要计算每一层的导数再累乘如图3所示。如果某几层梯度特别小，累乘之下梯度很快就成为一个很小的接近0的数，也就出现了梯度消失问题。梯度消失问题是深层次网络难以训练与优化的一个原因。</p>
<p><img src="/images/ResNet/tu3.png" alt="image-20211217162043566"></p>
<p>​                                                                                图3 普通深层网络计算梯度</p>
<p>让我们来看下假如把深层网络的基本架构换成残差块会出现什么。由残差块的基本架构可以知道残差块需要优化的函数是h(x)，传递给下层的输出为h(x)+x。还是按照同样的假设来对残差块进行表示，不妨设第一块的输出为g(x)，则第二层的输出为f(g(x))+g(x)，对其求导如图4所示</p>
<p><img src="/images/ResNet/tu4.png" alt="image-20211217185939784"></p>
<p>​                                                                                                图4 残差网络计算梯度</p>
<p>可见当一个很深的深度残差网络，方框内的累乘部分可能因为累乘变得很小，但后面的加法部分可以有力的避免总的梯度变成很小的数从而导致网络难以训练优化。</p>

        <h4 id="二、残差网络的结构">
          <a href="#二、残差网络的结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、残差网络的结构" class="headerlink" title="二、残差网络的结构"></a>二、残差网络的结构</h4>
      <p>残差网络根据其网络层数不同有许多的变体，其中论文中给出了五种结构，层数分别为18、34、50、101、152，网络结构大致如下图5所示。</p>
<p><img src="/images/ResNet/tu5.png" alt="image-20211217195609739"></p>
<p>​                                                                                        图5 不同版本的ResNet的网络架构</p>
<p>通过图示可见18和34层的ResNet和较深层次的ResNet-50，ResNet-101，ResNet-152采用了不同的Residual Block设计，分别介绍这两种不同结构的Residual Block。</p>

        <h5 id="2-1-ResNet-18与-ResNet-34的Residual-Block">
          <a href="#2-1-ResNet-18与-ResNet-34的Residual-Block" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-ResNet-18与-ResNet-34的Residual-Block" class="headerlink" title="2.1 ResNet-18与 ResNet-34的Residual Block"></a>2.1 ResNet-18与 ResNet-34的Residual Block</h5>
      <p>ResNet-18与ResNet-34的Residual Block结构采用图6所示的结构。 残差块里首先有2个有相同输出通道数的3×3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。 因此残差连接里的1×1卷积层完成的实际功能就是改变输出通道数。</p>
<p><img src="/images/ResNet/tu6.png" alt="image-20211217200339365"></p>
<p>​                                                        图 6 包含以及不包含1×1卷积层的残差块</p>

        <h5 id="2-2-ResNet-50及以上版本的Residual-Block结构设计">
          <a href="#2-2-ResNet-50及以上版本的Residual-Block结构设计" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-ResNet-50及以上版本的Residual-Block结构设计" class="headerlink" title="2.2 ResNet-50及以上版本的Residual Block结构设计"></a>2.2 ResNet-50及以上版本的Residual Block结构设计</h5>
      <p>在论文中，较深的ResNet的块架构改成了一种叫<em>Bottleneck Architectures</em>的设计。如下图7所示。对于每个块来说将原有的两个3*3卷积层替换成两个1*1卷积层加3×3卷积的架构。其中第一层1×1卷积完成的功能是将输入数据降维到64个通道，后一层的1×1卷积做的是升维操作。对特征先做降维再使用3×3卷积提取特征的好处在于减少了3×3卷积层所需的参数数量并且减少了训练的时间。并且可以减少参数数量同时减少中间特征图的通道数，这样可以使单个Block消耗的显存更少，有利于构建层数更多的网络。正如论文原文所述：考虑到训练时间的限制，因此采用了BottleNeck的结构。</p>
<p><img src="/images/ResNet/tu7.png" alt="image-20211217201953034"></p>
<p>​            图 7 左为ResNet-18，34的设计，右为ResNet-50，101，152的”Bottleneck Architectures“设计</p>

        <h5 id="2-3-Resnet-18与ResNet-50的网络架构设计图示">
          <a href="#2-3-Resnet-18与ResNet-50的网络架构设计图示" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-Resnet-18与ResNet-50的网络架构设计图示" class="headerlink" title="2.3 Resnet-18与ResNet-50的网络架构设计图示"></a>2.3 Resnet-18与ResNet-50的网络架构设计图示</h5>
      <p><img src="/images/ResNet/tu8.jpg" alt="img"></p>

        <h4 id="三、Pytorch的ResNet-50实现FashionMNIST分类">
          <a href="#三、Pytorch的ResNet-50实现FashionMNIST分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、Pytorch的ResNet-50实现FashionMNIST分类" class="headerlink" title="三、Pytorch的ResNet-50实现FashionMNIST分类"></a>三、Pytorch的ResNet-50实现FashionMNIST分类</h4>
      
        <h5 id="3-1相关包的导入">
          <a href="#3-1相关包的导入" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1相关包的导入" class="headerlink" title="3.1相关包的导入"></a>3.1相关包的导入</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="3-2-ResNet-Block的设计">
          <a href="#3-2-ResNet-Block的设计" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2-ResNet-Block的设计" class="headerlink" title="3.2 ResNet Block的设计"></a>3.2 ResNet Block的设计</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_channels, filters, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 用一个Tuple储存BottleNeck层的三个卷积层分别的输出通道数，并用filter1, filter2, filter3取出</span></span><br><span class="line">        <span class="comment"># 每个卷积层后面都跟有一个BN层，再作ReLU, 对于shortcut， 1*1卷积做完之后只需要做BN，与Y累加之后再做ReLU</span></span><br><span class="line">        <span class="comment"># 非残差连接的1*1卷积层（指conv1,conv3）均采用stride=1,padding=0的设计</span></span><br><span class="line">        <span class="comment"># 对于用于改变输出通道数的快速连接上的1*1conv，即conv4，stride使用参数决定，第一个stage为1,后3个stage为2</span></span><br><span class="line">        filter1, filter2, filter3 = filters</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, filter1, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(filter1, filter2, kernel_size=<span class="number">3</span>, stride=strides, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(filter2, filter3, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv4 = nn.Conv2d(input_channels, filter3, kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv4 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(filter1)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(filter2)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(filter3)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = F.relu(self.bn2(self.conv2(Y)))</span><br><span class="line">        Y = self.bn3(self.conv3(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv4:</span><br><span class="line">            X = self.bn3(self.conv4(X))</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">residual_block</span>(<span class="params">input_channels, filters, num_residuals, first_block=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="comment"># 用于实现stage1,2,3,4</span></span><br><span class="line">    <span class="comment"># 每个stage的第一个block均使用1×1conv的快速连接，因此对第一个block需要特殊处理</span></span><br><span class="line">    <span class="comment"># 第一个stage的第一个block使用的1×1conv快速连接的stride为1，其他的stage为2</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> first_block:</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                blk.append(Residual(input_channels, filters, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                blk.append(Residual(filters[<span class="number">2</span>], filters, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                blk.append(Residual(input_channels, filters, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                blk.append(Residual(filters[<span class="number">2</span>], filters, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="3-3-网络结构的设计">
          <a href="#3-3-网络结构的设计" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-网络结构的设计" class="headerlink" title="3.3 网络结构的设计"></a>3.3 网络结构的设计</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">stage1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">stage2 = nn.Sequential(*residual_block(<span class="number">64</span>, (<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>), <span class="number">3</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">stage3 = nn.Sequential(*residual_block(<span class="number">256</span>, (<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>), <span class="number">4</span>))</span><br><span class="line">stage4 = nn.Sequential(*residual_block(<span class="number">512</span>, (<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>), <span class="number">6</span>))</span><br><span class="line">stage5 = nn.Sequential(*residual_block(<span class="number">1024</span>, (<span class="number">512</span>, <span class="number">512</span>, <span class="number">2048</span>), <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    stage1, stage2, stage3, stage4, stage5,</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">2048</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网络参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="3-4-数据加载以及训练的相关代码">
          <a href="#3-4-数据加载以及训练的相关代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-4-数据加载以及训练的相关代码" class="headerlink" title="3.4 数据加载以及训练的相关代码"></a>3.4 数据加载以及训练的相关代码</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据加载部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    train_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">    test_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">    train_iter = data.DataLoader(train_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    test_iter = data.DataLoader(test_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前学习率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cur_lr</span>(<span class="params">optimizer</span>):</span></span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        <span class="keyword">return</span> param_group[<span class="string">&#x27;lr&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 衡量在测试集上的准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">net, test_iter, loss, device</span>):</span></span><br><span class="line">    total, correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            total += y.size(<span class="number">0</span>)</span><br><span class="line">            correct += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    test_acc = <span class="number">100.0</span> * correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;****test*****&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;test_loss:&#123;:.3f&#125; | test_acc:&#123;:6.3f&#125;%&quot;</span>.<span class="built_in">format</span>(l.item(), test_acc))</span><br><span class="line">    net.train()</span><br><span class="line">    <span class="keyword">return</span> test_acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, loss, optimizer, num_epochs, device, num_print, lr_scheduler=<span class="literal">None</span>, test_iter=<span class="literal">None</span></span>):</span></span><br><span class="line">    net.train()</span><br><span class="line">    record_train = <span class="built_in">list</span>()</span><br><span class="line">    record_test = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;epoch:[&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>, num_epochs))</span><br><span class="line">        total, correct, train_loss = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        start = time.time() <span class="comment"># 返回当前时间戳</span></span><br><span class="line">        <span class="comment"># i是train_iter的index，用于计数，计算已经抽取了多少个（x, y）</span></span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            <span class="comment"># 将x, y拷贝到显存进行运算,X,y都是一个batch的数据</span></span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            <span class="comment"># x, y都是tensor,l也是tensor</span></span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># 从tensor中取出相应的数值</span></span><br><span class="line">            train_loss += l.item()</span><br><span class="line">            total += y.size(<span class="number">0</span>)</span><br><span class="line">            correct += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            train_acc = (correct / total) * <span class="number">100.0</span></span><br><span class="line">            <span class="comment"># 每num_print次打印一次loss和train_acc</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % num_print == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;step:[&#123;&#125;/&#123;&#125;], train_loss:&#123;:.3f&#125; | train_acc:&#123;:6.3f&#125; | lr:&#123;:.6f&#125;&quot;</span></span><br><span class="line">                      .<span class="built_in">format</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(train_iter), train_loss / (i + <span class="number">1</span>), train_acc, get_cur_lr(optimizer)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> lr_scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;--cost time:&#123;:.4f&#125;s--&quot;</span>.<span class="built_in">format</span>(time.time() - start))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            record_test.append(test(net, test_iter, loss, device))</span><br><span class="line">        record_train.append(train_acc)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> record_train, record_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_curve</span>(<span class="params">record_train, record_test=<span class="literal">None</span></span>):</span></span><br><span class="line">    plt.style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(record_train)+<span class="number">1</span>), record_train, label=<span class="string">&quot;train_acc&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> record_test <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(record_test) + <span class="number">1</span>), record_test, label=<span class="string">&quot;test_acc&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="number">4</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;learning_curve&quot;</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(record_train) + <span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">101</span>, <span class="number">5</span>))</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">1.0</span></span><br><span class="line">momentum = <span class="number">0.9</span></span><br><span class="line">weight_decay = <span class="number">0.0001</span></span><br><span class="line">num_print = <span class="number">100</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_on_device</span>(<span class="params">net, batch_size, num_epochs, learning_rate, momentum, weight_decay, device</span>):</span></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    train_iter, test_iter = load_dataset(batch_size)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.SGD(</span><br><span class="line">        net.parameters(),</span><br><span class="line">        lr=learning_rate,</span><br><span class="line">        momentum=momentum,</span><br><span class="line">        weight_decay=weight_decay,</span><br><span class="line">        nesterov=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">25</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    record_train, record_test = train(net, train_iter, loss, optimizer, num_epochs</span><br><span class="line">                                      , device, num_print, lr_scheduler, test_iter)</span><br><span class="line">    learning_curve(record_train, record_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_on_device(net, batch_size, num_epochs, learning_rate, momentum, weight_decay, device)</span><br></pre></td></tr></table></div></figure>

<p>`</p>
<p>代码中使用了全局池化nn.AdaptiveAvgPool2d以及动态改变学习率torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)，基本的功能已经了解但是具体的细节没有完全搞清楚，后面会继续了解。</p>

        <h5 id="3-5-实验结果展示">
          <a href="#3-5-实验结果展示" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-5-实验结果展示" class="headerlink" title="3.5 实验结果展示"></a>3.5 实验结果展示</h5>
      <p><img src="/images/ResNet/image-20211217211206565.png" alt="image-20211217211206565"></p>
<p><img src="/images/ResNet/image-20211217211249813.png" alt="image-20211217211249813"></p>
<p>最终达到的测试集准确率为88.13%，对于这个结果我还是不太满意的。根据学习曲线可以看出后期模型出现了过拟合的现象。综合分析个人认为应该还可以达到更好的成绩，对于学习率、权重衰减等相关超参数的设置还是没有什么经验，另外今天逛论坛中看到一篇帖子做的是cifar-10的分类问题，仅仅将图片进行了一些剪裁、均值化等处理就将成绩从88%提升到了95%。而且在ResNet的论文中也提到了对图像进行预处理，感觉问题可能出现在这两个方面，后面有时间的话还会继续做实验探索。</p>

        <h4 id="后记">
          <a href="#后记" class="heading-link"><i class="fas fa-link"></i></a><a href="#后记" class="headerlink" title="后记"></a>后记</h4>
      <p>讲个笑话，10月绝不摆烂。</p>
<p><img src="/images/ResNet/image-20211217211811571.png" alt="image-20211217211811571"></p>
<p><img src="/images/ResNet/image-20211217211842242.png" alt="image-20211217211842242"></p>
<p>摆烂王是谁？哦，是我，那没事了。</p>
<p><img src="/images/ResNet/image-20211217211953650.png" alt="image-20211217211953650"></p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="http://example.com">Strive</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="http://example.com/2021/12/17/ResNet/">http://example.com/2021/12/17/ResNet/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/CNN/">CNN</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/">网络模型实现</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/12/23/object-detect/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">物体识别</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/11/02/labsever/"><span class="paginator-prev__text">实验室服务器探索</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%AE%9E%E7%8E%B0%E4%B9%8BResNet"><span class="toc-number">1.</span> <span class="toc-text">
          经典网络架构实现之ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ResNet%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">
          ResNet简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81ResNet%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E2%80%93Residual%E5%9D%97"><span class="toc-number">1.2.</span> <span class="toc-text">
          一、ResNet的基本架构–Residual块</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1%E6%AE%8B%E5%B7%AE%E5%9D%97%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.1.</span> <span class="toc-text">
          1.1残差块的结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%A5%BD%E4%BC%98%E5%8C%96%E7%9A%84%E4%B8%80%E7%A7%8D%E6%95%B0%E5%AD%A6%E4%B8%8A%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-number">1.2.2.</span> <span class="toc-text">
          1.2 残差连接好优化的一种数学上的解释</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.</span> <span class="toc-text">
          二、残差网络的结构</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-ResNet-18%E4%B8%8E-ResNet-34%E7%9A%84Residual-Block"><span class="toc-number">1.3.1.</span> <span class="toc-text">
          2.1 ResNet-18与 ResNet-34的Residual Block</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-ResNet-50%E5%8F%8A%E4%BB%A5%E4%B8%8A%E7%89%88%E6%9C%AC%E7%9A%84Residual-Block%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.3.2.</span> <span class="toc-text">
          2.2 ResNet-50及以上版本的Residual Block结构设计</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-Resnet-18%E4%B8%8EResNet-50%E7%9A%84%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%9B%BE%E7%A4%BA"><span class="toc-number">1.3.3.</span> <span class="toc-text">
          2.3 Resnet-18与ResNet-50的网络架构设计图示</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E3%80%81Pytorch%E7%9A%84ResNet-50%E5%AE%9E%E7%8E%B0FashionMNIST%E5%88%86%E7%B1%BB"><span class="toc-number">1.4.</span> <span class="toc-text">
          三、Pytorch的ResNet-50实现FashionMNIST分类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1%E7%9B%B8%E5%85%B3%E5%8C%85%E7%9A%84%E5%AF%BC%E5%85%A5"><span class="toc-number">1.4.1.</span> <span class="toc-text">
          3.1相关包的导入</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-ResNet-Block%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.4.2.</span> <span class="toc-text">
          3.2 ResNet Block的设计</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-3-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.4.3.</span> <span class="toc-text">
          3.3 网络结构的设计</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%BB%A5%E5%8F%8A%E8%AE%AD%E7%BB%83%E7%9A%84%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81"><span class="toc-number">1.4.4.</span> <span class="toc-text">
          3.4 数据加载以及训练的相关代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-5-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="toc-number">1.4.5.</span> <span class="toc-text">
          3.5 实验结果展示</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E8%AE%B0"><span class="toc-number">1.5.</span> <span class="toc-text">
          后记</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/photo.png" alt="avatar"></div><p class="sidebar-ov-author__text">To be a great person.</p></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2022</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Strive</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>