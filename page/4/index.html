<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="Strive&#39;s Blog">
<meta property="og:url" content="https://striveizu.top/page/4/index.html">
<meta property="og:site_name" content="Strive&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Strive">
<meta name="twitter:card" content="summary"><title>Strive's Blog</title><link ref="canonical" href="https://striveizu.top/page/4/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-address-card"></i></span><span class="header-nav-menu-item__text">关于</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-edit"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Strive's Blog</div><div class="header-banner-info__subtitle">你我期许的绝非遥不可及</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2022/02/21/object-detect-EVALUATION-METRICS/">目标检测常用的评估指标</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2022-02-21</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-03-06</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>目标检测常用的评估指标</p>

        <h5 id="混淆矩阵">
          <a href="#混淆矩阵" class="heading-link"><i class="fas fa-link"></i></a><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h5>
      <p><img src="/2022/02/21/object-detect-EVALUATION-METRICS/image-20220221163303518.png" alt="image-20220221163303518"></p>
<p>混淆矩阵定义如上所示，对于二分类问题来说，根据预测值结果可以将预测结果分为<strong>真正例（Ture Positive，TP）</strong>、<strong>真负例（True Negative，TN）</strong>、<strong>假正例（False Positive，FP）、假负例（False Negative，FN）</strong>。其中，<strong>TP表示预测为正样本且实际为正样本的样本数</strong>，<strong>TN表示预测为负样本但实际为正样本的样本数，FP为预测为正样本实际为负样本的样本数，FN表示预测为负样本实际为负样本的样本数。</strong></p>

        <h5 id="AP-AR">
          <a href="#AP-AR" class="heading-link"><i class="fas fa-link"></i></a><a href="#AP-AR" class="headerlink" title="AP AR"></a>AP AR</h5>
      <p><strong>准确率Precision又称为查准率</strong>，表示<strong>模型预测为正例的样本占实际为正例的样本的比例</strong>。<strong>召回率Recall又称为查全率</strong>，表示测试集<strong>中所有正样本被正确识别的比例。</strong>计算方法为：</p>
<img src="/2022/02/21/object-detect-EVALUATION-METRICS/image-20220221164603650.png" alt="image-20220221164603650" style="zoom:200%;">

<p>目标检测中判断是正样本还是负样本与两个指标有关，分别为<strong>交并比（IOU）阈值</strong>和<strong>置信度（Confidence）阈值</strong>。置信度即认为算法预测目标框中存在目标的确信度，对于每个预测目标框，都会使用置信度阈值进行过滤，滤除一些低质量的预测框，置信度阈值一般设为0.35。<strong>当预测目标框的置信度大于阈值时，才认为该预测框中存在目标，即该预测框为正样本，否则则认为该预测框中为背景，即该预测框为负样本。</strong>其次我们介绍交并比的定义，交并比是用来衡量真实目标框和算法预测目标框的重合程度，如图3.1所示，IOU的值就等于图3.1中黄色重合部分的面积与红色框和绿色框的面积之和的比值。<strong>当预测框和真实框重合达到一定阈值时，我们就认为预测框预测正确，即真正例True Positive，否则为假正例False Positive，一般该阈值我们称为IOU阈值，常设为0.5左右。</strong></p>
<img src="/2022/02/21/object-detect-EVALUATION-METRICS/image-20220221164742376.png" alt="image-20220221164742376" style="zoom:150%;">

<p><strong>平均准确率AP为表示不同查全率下的查准率的加权平均。</strong></p>
<p><img src="/2022/02/21/object-detect-EVALUATION-METRICS/image-20220221170540931.png" alt="image-20220221170540931"></p>
<p>而均值平均准确率为确定置信度阈值的前提下调整不同IOU阈值得到的平均准确率的和的均值。而IOU阈值一般取0.5,0.55,0.6,…,0.95等10个值，其公式如下所示：</p>
<img src="/2022/02/21/object-detect-EVALUATION-METRICS/image-20220221170654735.png" alt="image-20220221170654735" style="zoom:200%;">
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2022/02/01/Attention/">Attention</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2022-02-01</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-03-06</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>深度学习中的注意力机制</p>

        <h4 id="简介">
          <a href="#简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#简介" class="headerlink" title="简介"></a>简介</h4>
      <p>注意力机制，顾名思义是向人类的注意力学习的结果。人类视野开阔，但是视野只能聚焦在一个小范围，或一个点，这种就叫做注意力。</p>
<p>视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2022/01/28/DLA/">DLA模型</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2022-01-28</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-03-06</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>DLA网络模型：一种深度网络特征融合方法</p>

        <h4 id="DLA简介">
          <a href="#DLA简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#DLA简介" class="headerlink" title="DLA简介"></a>DLA简介</h4>
      <p>DLA网络模型出自CVPR 2018 的论文 Deep Layer Aggregation，中文翻译为深层聚合。CNN为多种计算机视觉任务提供了很好的解决方案。随着视觉任务对高性能算法更严格的追求，Backbone的设计成为了一个很重要的主题。作者在摘要中指出计算机视觉识别任务需要从低到高的层次提取不同层次的信息。因此当前的网络设计方向是如何加宽加深网络层次以提取更多的特征。如ResNet，通过引入跳跃连接使得网络有能力到达很深的层次。设计更深或更宽的网络架构虽然确实可以更好的提取特征，但也带来了更多的参数数量和难优化问题。更深的层提取更多语义和更全局的特征，但这些并不能证明最后一层是任何任务的最终表示。事实上，skip connections已经被证明对于分类和回归以及更结构化的任务是有效的。如何更好的聚合不同层次提取的特征，就像深度和宽度一样，是网络架构的一个关键维度。因此作者认为：设计更深更宽的网络结构固然重要，<strong>但如何更好的聚合层和块更值得研究。</strong></p>
<p>经过复杂的分析，<strong>更深的网络层能提取到更多语义和全局的特征，但是这并不能表明最后一层就是任务需要的表示。</strong>实际上“跳跃连接”已经证明了对于分类、回归以及其他结构化问题的有效性。因此，如何聚合，尤其是深度与宽度上的聚合，对于网络结构的优化是一个非常重要的技术。</p>
<p>作者认为，在计算机视觉任务中，仅是加深网络层次，但层和层之间相互孤立是不行的。<strong>混合和聚合不同层提取的特征表示，可以改进对图像是什么和在哪里的推断。</strong>尽管ResNet加入了skip connections的设计，已经组合了不同层次的特征表示，但这些连接本质上是浅层的连接，仅仅通过相加操作来融合不同层次的特征。</p>
<p>作者通过更深层次的聚合来增强标准体系结构，以便更好地跨层融合信息。我们的深层聚合结构迭代和分层地合并特征层次，使网络具有更高的精度和更少的参数。跨架构和任务的实验表明，与现有的分支和合并方案相比，深层聚合方案提高了识别和分辨率。</p>

        <h4 id="第一章-DLA的基本组成模块–IDA和HDA">
          <a href="#第一章-DLA的基本组成模块–IDA和HDA" class="heading-link"><i class="fas fa-link"></i></a><a href="#第一章-DLA的基本组成模块–IDA和HDA" class="headerlink" title="第一章 DLA的基本组成模块–IDA和HDA"></a>第一章 DLA的基本组成模块–IDA和HDA</h4>
      <p>DLA，即深度聚合。在论文中，作者将聚合<em>Aggregation</em>定义为网络架构中不同层次特征表示的组合。多个layer组合为一个block，多个block再根据分辨率组合为一个stage，<strong>DLA则主要探讨block和stage的组合</strong>（stage间网络保持一致分辨率，那么空间融合发生在stage间，语义融合发生在stage内）。如果一组<em>Aggregation</em>是复合的、非线性的，并且较早的聚合层的输出输入到后面的多个聚合层。作者介绍了两种深层聚合结构：IDA(iterative deep aggregation迭代式深度聚合)和HDA(hierarchical deep aggregation层级深度聚合)。IDA专注于跨分辨率和尺度的融合，而HDA专注于融合所有提取自不同模块和通道的特征。下图1的c和f分别展示了IDA和HDA的结构。</p>
<p><img src="/2022/01/28/DLA/image-20220202095121590.png" alt="image-20220202095121590"></p>
<p>​                                                                              图 1 HDA IDA 基本结构</p>

        <h5 id="1-1-IDA-Iterative-Deep-Aggregation-迭代深度聚合结构">
          <a href="#1-1-IDA-Iterative-Deep-Aggregation-迭代深度聚合结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-IDA-Iterative-Deep-Aggregation-迭代深度聚合结构" class="headerlink" title="1.1 IDA Iterative Deep Aggregation 迭代深度聚合结构"></a>1.1 IDA Iterative Deep Aggregation 迭代深度聚合结构</h5>
      <p>如上文所说，多个layer组合为一个block，多个block再根据分辨率组合为一个stage。更深层次的阶段语义更加丰富，但空间信息更加粗糙。从浅层到深层的skip connection可以融合尺度和分辨率。在IDA结构中，作者建议逐渐的增加和加深IDA结构的特征表示。不同层的聚合从最浅的层次和最小的尺度开始并逐步增加深度和尺度。作者指出不断地聚合不同阶段提取的特征，可以细化浅层的特征。</p>
<p><img src="/2022/01/28/DLA/image-20220203092920174.png" alt="image-20220203092920174"></p>
<p>上图中绿色的方块称作“Aggregation Node”，Aggregation Node在特征由浅到深传播的同时聚集特征。从上图中也可以看出，IDA以stage为基本单位，IDA结构在多个stage外部增加连接和Aggregation Node。</p>
<p><img src="/2022/01/28/DLA/image-20220203093447689.png" alt="image-20220203093447689"></p>
<p>在上面的公式中， <em><strong>I</strong></em>表示整个IDA模块， <em><strong>N</strong></em>表示Aggregation Node。例如***N(x1, x2)***表示一个输入为x1和x2的Aggregation Node，即图中最左侧的绿色方块。</p>

        <h4 id="1-2-HDA-Hierarchical-Deep-Aggregation-层次深度聚合结构">
          <a href="#1-2-HDA-Hierarchical-Deep-Aggregation-层次深度聚合结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-HDA-Hierarchical-Deep-Aggregation-层次深度聚合结构" class="headerlink" title="1.2 HDA (Hierarchical Deep Aggregation)层次深度聚合结构"></a>1.2 HDA (Hierarchical Deep Aggregation)层次深度聚合结构</h4>
      <p>虽然IDA有效地融合了多个stage的特征，但是没有对stage内部多个block的特征进行融合。作者提出了HDA（Hierarchical Deep Aggregation）结构增强stage内部多个block的融合，如下图所示：</p>
<p><img src="/2022/01/28/DLA/image-20220203101438312.png" alt="image-20220203101438312"></p>
<p>在树的中间聚合中，我们将聚合节点的输出返回到主干，作为下一个子树的输入，如图所示。这将传播前面所有块的聚合，而不是单独传播前面的块，以更好地保留特性。  </p>

        <h4 id="第三章-代码实现">
          <a href="#第三章-代码实现" class="heading-link"><i class="fas fa-link"></i></a><a href="#第三章-代码实现" class="headerlink" title="第三章 代码实现"></a>第三章 代码实现</h4>
      
        <h5 id="3-1-聚合节点-Aggregation-Nodes">
          <a href="#3-1-聚合节点-Aggregation-Nodes" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-聚合节点-Aggregation-Nodes" class="headerlink" title="3.1 聚合节点 Aggregation Nodes"></a>3.1 聚合节点 Aggregation Nodes</h5>
      <p>聚合节点的的主要功能是组合和压缩它们的输入。节点学习选择和投射重要的信息，并在它们的输出作为输入时保持相同的维度。可以使用任意网络结构构造Aggregation Node，为减少计算量，作者使用单个“卷积-BN-激活函数”结构来构造Aggregation Node，一般情况下使用1x1卷积。</p>

        <h4 id="第四章-实验设计">
          <a href="#第四章-实验设计" class="heading-link"><i class="fas fa-link"></i></a><a href="#第四章-实验设计" class="headerlink" title="第四章 实验设计"></a>第四章 实验设计</h4>
      <p>用CIFAR-10分类数据集测试DLA模型的效果。论文中分类任务实验使用SGD作为优化器执行120个epoch，动量为0.9，权重衰减10−4，批处理大小为256并以0.1的学习速率开始训练，每30个epoch减少10个学习速率。并Resize成256，并随机裁剪成224×224。根据我可怜的单卡12G的显存取舍，最终采用下面的训练方式。训练集预处理：随机裁剪224×224，随机中心翻转，并作均值化处理。测试集预处理：Resize成256×256，中心裁剪224×224，并均值化处理。batch_size选择64，训练50个epoch，初始学习率采取0.1，动量为0.9，权重衰减10−4，每训练20个epoch，学习率×0.1。</p>

        <h4 id="第五章-结果展示">
          <a href="#第五章-结果展示" class="heading-link"><i class="fas fa-link"></i></a><a href="#第五章-结果展示" class="headerlink" title="第五章 结果展示"></a>第五章 结果展示</h4>
      <p>在CIFAR-10分类任务中达到了93.530%的准确率，比之前尝试过的的ResNet、AlexNet网络模型效果都要出色。有力的证明了DLA网络模型具有优秀的提取图像特征能力。</p>
<p><img src="/2022/01/28/DLA/image-20220129212334052.png" alt="image-20220129212334052"></p>
<p><img src="/2022/01/28/DLA/image-20220129212846106.png" alt="image-20220129212846106"></p>
<p>后又尝试120epoch,batch_size为60，起始学习率0.1，动量0.9，权重衰减0.0001，每30个epoch学习率×0.1，最终达到94.9%准确率。</p>
<p><img src="/2022/01/28/DLA/image-20220203161128542.png" alt="image-20220203161128542"></p>
<p>没有引入预训练模型，充分证明了DLA优秀的提取特征能力。</p>

        <h4 id="结语">
          <a href="#结语" class="heading-link"><i class="fas fa-link"></i></a><a href="#结语" class="headerlink" title="结语"></a>结语</h4>
      <p>DLA的代码还没有来得及认真学习，暂时搁置，后面有时间再来研究。</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2022/01/28/caikeng001/">深度学习踩坑记录001——训练集loss正常下降，测试集一动不动</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2022-01-28</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-03-06</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="踩坑记录001">
          <a href="#踩坑记录001" class="heading-link"><i class="fas fa-link"></i></a><a href="#踩坑记录001" class="headerlink" title="踩坑记录001"></a>踩坑记录001</h3>
      <p>回家混了好几天才开始干点正事，开摆！</p>
<p>新年的第一篇博客本来想写一下Ubuntu系统的安装、Nvidia驱动的安装和CenterNet运行环境的配置。可是在学校的时候一直在混，Liunx上写博客还没有找到很方便的方式，踩过的坑很多都忘记了。</p>
<p><strong>以后踩坑一定要及时记录</strong>！</p>
<p>由于一两个月没有搞代码了，刚回家就碰见了个非常脑残小儿科的问题却一直没有解决。</p>

        <h4 id="问题描述">
          <a href="#问题描述" class="heading-link"><i class="fas fa-link"></i></a><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4>
      <p>回家做一个ResNet50的CIFAR10分类找找之前遗忘的知识，出现了训练集的loss正常下降、准确率正常上升。而测试集的loss却几乎不下降甚至还有反增，准确率保持非常低的水平不变。如下图所示：</p>
<p><img src="/2022/01/28/caikeng001/image-20220128123827445.png" alt="image-20220128123827445"></p>
<p>可见训练集准确度已经到了很高的程度，但是测试集的loss和准确率几乎没有什么变化。仔细检查代码（指查了三天）发现在数据的预处理部分，训练集做了Resize、随机翻转和均值化操作，但是测试集没有作任何处理。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">myTransforms = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>), (<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    train_set = torchvision.datasets.CIFAR10(</span><br><span class="line">        root=<span class="string">&#x27;../data/cifar&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=myTransforms)</span><br><span class="line">    test_set = torchvision.datasets.CIFAR10(</span><br><span class="line">        root=<span class="string">&#x27;../data/cifar&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">    train_iter = data.DataLoader(train_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    test_iter = data.DataLoader(test_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br></pre></td></tr></table></div></figure>

<p>即问题出在训练集和测试集没有作相同的预处理。引用知乎一个很恰当的比喻是：<strong>你复习了一整晚的高数课本，可是第二天考线性代数，虽然二者都属于数学范畴，但是你拿高数知识解线代怎么能做的好呢？</strong>对数据集作预处理，如均值化操作，实际上对数据的分布作了改变。训练集训练的是一个数据分布，直接拿训练的网络对另外一个数据分布的测试集作测试，效果肯定不好。</p>

        <h4 id="问题解决方法">
          <a href="#问题解决方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#问题解决方法" class="headerlink" title="问题解决方法"></a>问题解决方法</h4>
      <p>将数据加载的预处理统一，训练集和测试集使用相同的预处理方式。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据加载部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    train_set = torchvision.datasets.CIFAR10(</span><br><span class="line">        root=<span class="string">&#x27;../data/cifar&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=myTransforms)</span><br><span class="line">    test_set = torchvision.datasets.CIFAR10(</span><br><span class="line">        root=<span class="string">&#x27;../data/cifar&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=myTransforms)</span><br><span class="line">    train_iter = data.DataLoader(train_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    test_iter = data.DataLoader(test_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br></pre></td></tr></table></div></figure>

<p><img src="/2022/01/28/caikeng001/image-20220128115856272.png" alt="image-20220128115856272"></p>
<p>测试集准确率就正常上升了。</p>

        <h4 id="学习与反思">
          <a href="#学习与反思" class="heading-link"><i class="fas fa-link"></i></a><a href="#学习与反思" class="headerlink" title="学习与反思"></a>学习与反思</h4>
      <p>借用知乎回答“深度学习中为什么要对测试集进行与训练集一样的数据预处理？”</p>
<p>作者：深海<br>链接：<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.zhihu.com/question/479694291/answer/2061254022">https://www.zhihu.com/question/479694291/answer/2061254022</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>我们拿标准化举例，标准化也是数据预处理的过程，我们假设将要标准化的某个特征服从高斯分布。</p>
<p>在神经网络中也可以看作是使网络收敛更加稳定的trick（因为标准化后大部分数值落在一定区间内）。</p>
<p>我们只能通过训练集获得到数据中该特征的所有可能的情况，我们假设我们拿到的训练集是足够有代表性的，能够从训练集中尽可能逼近数据本身服从的概率分布（这也就是为什么扩充数据能够提升模型泛化能力，我们拿到的数据越多，越能接近数据本身的分布，如果我们拿到全量数据，那么本身的分布就确定下来了，假设数据的某个特征服从高斯分布，那么它的均值和方差也确定了，如果我们能拿到全量数据，那也就没必要做机器学习了）。</p>
<p>所以我们是拿训练集中数据的某个特征代表了全部数据的这个特征，用它的均值和方差代表了全部数据的均值和方差。这就很容易理解了，测试集要使用训练集的均值和方差进行标准化，不就是在用我们假设的数据特征服从的均值和方差进行标准化吗？</p>
<p>所以不只是深度学习，传统机器学习算法的预处理也需要这么做。深度学习做了标准化后能够对网络的训练有一些益处罢了，这也是为什么做bn，ln的原因，以及网络中用到的各种归一化的trick，对深度学习中网络优化过程中数值的稳定性有一定的帮助。</p>
<p>寒假回家第6天了，今天才刚开始做了点东西（虽然没有什么卵用）。除夕之前一定要把DLA跑通。。。。。</p>
<p><img src="/2022/01/28/caikeng001/image-20220128140116390.png" alt="image-20220128140116390"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2022/01/17/CenterNet/">CenterNet学习笔记</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2022-01-17</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-03-06</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="CenterNet">
          <a href="#CenterNet" class="heading-link"><i class="fas fa-link"></i></a><a href="#CenterNet" class="headerlink" title="CenterNet"></a>CenterNet</h3>
      
        <h4 id="一、AnchorFree方法">
          <a href="#一、AnchorFree方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、AnchorFree方法" class="headerlink" title="一、AnchorFree方法"></a>一、AnchorFree方法</h4>
      <p>根据之前行人再识别相关理论的学习，可以知道一般行人检测网络可分为Anchor Based和Anchor Free两类方法。其中Anchor Based方法是通过在图像上暴力枚举一系列的锚框，然后再通过IOU等计算预测物体在锚框内的概率。而Anchor Free类方法往往不直接枚举锚框，而是通过预测一系列的点，比如物体的中心点、对角点等的位置，然后再通过预测这些点对应的宽高等来获得锚框。常见的Anchor Free方法有：CornerNet、ExtremeNet、CenterNet。</p>
<p>CornerNet是通过预测物体的左上角和右下角从而确定一个物体，左上角和右下角是两个Key Points。</p>
<p><img src="/images/CenterNet/image-20220117134913361.png" alt="image-20220117134913361"></p>
<p>Extreme Net方法与CornerNet类似，只不过预测的点更多，需要预测五个点来确定一个物体。</p>
<p><img src="/images/CenterNet/image-20220117135258162.png" alt="image-20220117135258162"></p>
<p>Extreme Net方法与CornerNet在预测完点之后还需要做一个Group的操作，用于确定哪些预测的点属于同一个物体。而CenterNet并不需要Group的操作。它是直接预测物体的中心点，通过中心点来预测物体的宽高从而确定物体的位置。</p>
<p><img src="/images/CenterNet/image-20220117135353161.png" alt="image-20220117135353161"></p>

        <h4 id="二、CenterNet网络流程">
          <a href="#二、CenterNet网络流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、CenterNet网络流程" class="headerlink" title="二、CenterNet网络流程"></a>二、CenterNet网络流程</h4>
      </div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/29/summary2021/">2021年终总结</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-29</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-01-12</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="2021年终总结">
          <a href="#2021年终总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#2021年终总结" class="headerlink" title="2021年终总结"></a>2021年终总结</h3>
      <p>​    说实话，以前我是没有写年终总结的习惯的。但是最近博客写多了也总想写点和专业无关的小东西，一转眼2021年就要结束了，还是稍微写写吧。</p>
<p>​    虽然称之为2021的年终总结，但思来想去还是想把近三年串在一起来总结。因为这三年都是围绕着一件事来进行的–考研。没有2019的棋差一招的心怀不甘，就没有2020的最后一搏不留遗憾，也就更无从谈起2021的得偿所愿。从结果导向来看，通过考研这场考验我确实收获了果实。但回顾整个备考的过程来看，通过考研我所收获的经历和经验是比结果还要更加有意义的。</p>
<p>​    第一个经验就是“取乎其上，得乎其中；取乎其中，得乎其下；取乎其下，则无所得矣。”一开始就奔着很低的标准去做，最后可能一无所获。第一年由于冲科软心里没底，加上传言科软310复试不刷人，使得自己从一开始就奔着310去想着过线就行，结果自己确实过了310，但复试线却成了320，即为”取乎其下，则无所得矣。”第二年虽然觉得自己离科软差的不远，但也还是少了勇气冲击更高的目标，最后确定工大顺利上岸，即“取乎其上，得乎其中。“</p>
<p>​    第二个经验好像就是非常老道的“千里之行始于足下” 。不管多么难以完成的任务，只要行动起来慢慢摸索并坚持下来总能总结出技巧。在考研中，对我来说最困难的就是数学了。作为一个从小学就没学明白数学，直到现在一百以内加减乘除还偶有算错的数学傻子，考研数学那就是一道天堑。在整个学生生涯中我在数学上栽了无数的跟头，中考数学评级C，高考数学73，第一年考研也不出意料的考了76。想快速把数学补上来无疑是不可能的，唯一能做到的是持之以恒的学习。虽然第一年考的很差，但是总体来说勤勤恳恳的复习还是打下了相对牢靠的数学基础。第二年继续每天做数学，笔记题目写两遍，复习全书写两遍，真题写两遍，虽然最后数学还是没有考到我的期望，但不可否认的是数学确实需要天分。天分决定上限，但是持之以恒的学习可以保证下限。101的分数说高肯定算不上，但是可以肯定的是我实打实写出的题目全都对了。这点经验也在专业上的学习得到了验证。还记得当时做毕业设计接触深度学习感觉需要的前置知识太多而无从下手，但是坚持学习不会就查做好学习笔记。时间一长慢慢看一些代码就能够理解了。这就是考研带来的第二个经验：别想这么多，做下去就对了。</p>
<p>​    考研政治的老师徐涛曾有次在课程中闲聊的时候说为什么一定要考上研。考上研给人带来的精神动力是巨大的，考上研的那一刻的自豪感是无与伦比的，这会使你敢于去做以后的任何事，一事成百事成。我一向是一个较为自卑的人，但现在我逐渐感到我敢于去做一些事情了，希望这种势头可以保持。</p>
<p>​    关于考研的事情也就说到这里，下面就正式开始总结2021吧。</p>
<p>​    有关2021的收获：</p>
<p>​    ①那肯定是一份录取通知书</p>
<p>​    ②一次算贯穿中国东西的旅行</p>
<p>​    ③从208瘦到了182了（虽然最近感冒没有锻炼可能有所反弹）</p>
<p>​    ④通过半年学习，专业方面的相关理论和python代码能力有所提高</p>
<p>​    2021做的不够的地方：</p>
<p>​    ①在家的大半年浪费了太多时间</p>
<p>​    ②学习方面还是有所惰性，经常出现学习动力不足的问题，究其原因可能还在学习的阶段性目标设置的不够清晰，学习存在随心所欲想到哪学到哪。</p>
<p>​    ③人前说话还是有点没底，有些机会还是不太敢争取，希望明年能有所改进。</p>
<p>​    下面是对2022年的计划与期待：</p>
<p>​    ①不挂科。。。。虽然算法还一点都没复习</p>
<p>​    ②重新开始刷刷单词吧，争取6月刷个6级</p>
<p>​    ③寒假期间要把C++的类部分学完，寒假要做刷力扣的准备。</p>
<p>​    ④科研方面的期待就是希望明年把科研方向的基础学懂之后争取有个idea为之努力。</p>
<p>​    ⑤不知道有没有机会出去实习，有的话一定要出去实习。</p>
<p>​    ⑥瘦到170</p>
<p>​    暂时就先这些吧，毕竟路都是一步一步走的，只要一直向前就好了。</p>
<p>​    前几天室友问我，如何评价自己的2021。我当时也没有考虑太多，像是早已打了腹稿似的脱口而出：“2021没有辜负我，我也不算辜负了2021。”2021年对我来说确实是一个非常难忘的一年。以前一直觉得自己是一个一事无成的废物，但前些日子想了一想，好像自己也在慢慢的实现一些以前的梦想。从很小的时候就很想学计算机，大学虽然被录取到了化工专业，但是经过自己的坚持和家人的帮助还真读到了计算机。对自己二本心有不甘决定考研，虽然有二战的挫折也还是读上了一个自己觉得还行的学校和喜欢的专业。以及一些以前梦想想拥有的小物件慢慢的也都有了。总结下来就是：①要有主见，对未来要有自己的想法并不择手段的为之努力。②要坚持，虽然路途不会总是一帆风顺，但只要不停下来持之以恒的向前，总会离想要的东西不远。2021还有不到七个小时就要结束了，希望在2022年写年度总结的时候可以说没有辜负2022。</p>
<p>​                                                                                                                                                                    2021/12/31 17：25于双子科教楼A505</p>
<p><img src="/images/summary2021/image-20211231172501016.png" alt="image-20211231172501016"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/28/ReID-features/">行人重识别常用的局部特征</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-28</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-28</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="行人重识别常用的局部特征">
          <a href="#行人重识别常用的局部特征" class="heading-link"><i class="fas fa-link"></i></a><a href="#行人重识别常用的局部特征" class="headerlink" title="行人重识别常用的局部特征"></a>行人重识别常用的局部特征</h3>
      
        <h4 id="一、全局特征">
          <a href="#一、全局特征" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、全局特征" class="headerlink" title="一、全局特征"></a>一、全局特征</h4>
      <p>指每一张行人图片的全局信息进行一个特征抽取，这个全局特征没有任何的空间信息。</p>
<p><img src="/images/ReID-features/image-20211228091524988.png" alt="image-20211228091524988"></p>
<p>两张图片姿态不对齐在全局特征上有着信息不对称，直接使用全局特征衡量两张图片的相似度效果很差。</p>

        <h4 id="二、局部特征">
          <a href="#二、局部特征" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、局部特征" class="headerlink" title="二、局部特征"></a>二、局部特征</h4>
      <p>局部特征是指对图像中的某一个区域进行特征提取，最终将多个局部特征融合起来作为最终特征。常用的局部特征有切片、姿态、分割、网格。</p>
<p><img src="/images/ReID-features/image-20211228132708584.png" alt="image-20211228132708584"></p>
<p>姿态：</p>
<p><img src="/images/ReID-features/image-20211228132855294.png" alt="image-20211228132855294"></p>
<p><img src="/images/ReID-features/image-20211228133305844.png" alt="image-20211228133305844"></p>
<p>Part有点像锚框一样，是通过一定的规则预设的一组矩形框区域。</p>
<p>Attention应该是利用注意力机制去判断行人区域</p>
<p><img src="/images/ReID-features/image-20211228133551941.png" alt="image-20211228133551941"></p>
<p>将feature map 按预设的区域方向水平切片，切成与切片数量相同的featuremap 对每个切块的feature map做global average pooling</p>
<p><img src="/images/ReID-features/image-20211228133835557.png" alt="image-20211228133835557"></p>

        <h5 id="2-1-局部特征–水平切块常用算法">
          <a href="#2-1-局部特征–水平切块常用算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-局部特征–水平切块常用算法" class="headerlink" title="2.1 局部特征–水平切块常用算法"></a>2.1 局部特征–水平切块常用算法</h5>
      </div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/27/ReID-base/">行人重识别需要的一些基础</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-27</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-27</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="行人重识别需要的一些基础">
          <a href="#行人重识别需要的一些基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#行人重识别需要的一些基础" class="headerlink" title="行人重识别需要的一些基础"></a>行人重识别需要的一些基础</h3>
      
        <h4 id="一、回顾神经网络的一些基础">
          <a href="#一、回顾神经网络的一些基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、回顾神经网络的一些基础" class="headerlink" title="一、回顾神经网络的一些基础"></a>一、回顾神经网络的一些基础</h4>
      
        <h4 id="1-1-卷积回顾">
          <a href="#1-1-卷积回顾" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-卷积回顾" class="headerlink" title="1.1 卷积回顾"></a>1.1 卷积回顾</h4>
      
        <h5 id="1-1-1-普通卷积">
          <a href="#1-1-1-普通卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-1-普通卷积" class="headerlink" title="1.1.1 普通卷积"></a>1.1.1 普通卷积</h5>
      <p>卷积的功能是特征提取。步长不等于0的时候起到下采样的作用。卷积层的参数量的计算公式如下：</p>
<p><img src="/images/ReID-base/image-20211224100331547.png" alt="image-20211224100331547"></p>
<p>一个小Tips：填充padding和kernel_size往往需要配套使用，主要设计是让<strong>2*padding+1=k</strong>。从而使输出size只与stride和输入的尺寸有关。举例如kernel_size选择为3*3，一般选择padding=1，k=5选择padding=2。输出特征图的size计算公式如下</p>
<p><img src="/images/ReID-base/image-20211224100452800.png" alt="image-20211224100452800"></p>

        <h5 id="1-1-2-1-1卷积">
          <a href="#1-1-2-1-1卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-2-1-1卷积" class="headerlink" title="1.1.2 1*1卷积"></a>1.1.2 1*1卷积</h5>
      <p><img src="/images/ReID-base/image-20211224102629624.png" alt="image-20211224102629624"></p>
<p>一般不独立使用。主要是作为降维操作使用，如ResNet的BottleNeck层的快速连接中为了考虑训练时间成本对特征图先用1*1卷积层降维再升维。</p>

        <h5 id="1-1-3-分组卷积">
          <a href="#1-1-3-分组卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-3-分组卷积" class="headerlink" title="1.1.3 分组卷积"></a>1.1.3 分组卷积</h5>
      <p>完成的功能：对输入特征图在通道层面平均分成几组，每组在对应的卷积层做卷积之后再在通道维度上做拼接得到输出。</p>
<p><img src="/images/ReID-base/image-20211224103125148.png" alt="image-20211224103125148"></p>
<p>代码实现：修改Conv2d的group参数值</p>
<p><img src="/images/ReID-base/image-20211224103153758.png" alt="image-20211224103153758"></p>
<p>常用的情形：</p>
<p>由于分组卷积需要的参数量较少，常用于模型特别大作为模型压缩的一种手段。常作为对显存不够的一种妥协。</p>
<p>缺点：</p>
<p>由于分组卷积并不是整个feature map参与卷积特征提取，而是每一部分的输出只与一部分的特征图有关系，特征图之间的局部信息没有完全打通。两部分的输出之间可能存在的信息被忽略。</p>

        <h5 id="1-1-4-Channel-wise-Depthwise-Convolution">
          <a href="#1-1-4-Channel-wise-Depthwise-Convolution" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-4-Channel-wise-Depthwise-Convolution" class="headerlink" title="1.1.4 Channel-wise/Depthwise Convolution"></a>1.1.4 Channel-wise/Depthwise Convolution</h5>
      <p><img src="/images/ReID-base/image-20211224103739036.png" alt="image-20211224103739036"></p>

        <h5 id="1-1-5-空洞卷积（Dilated-Convolution">
          <a href="#1-1-5-空洞卷积（Dilated-Convolution" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-5-空洞卷积（Dilated-Convolution" class="headerlink" title="1.1.5 空洞卷积（Dilated Convolution)"></a>1.1.5 空洞卷积（Dilated Convolution)</h5>
      <p>空洞卷积引入了一个称为 “**扩张率(dilation rate)**”的超参数(hyper-parameter)，该参数定义了卷积核处理数据时各值的间距。扩张率中文也叫空洞数(Hole Size)。空洞卷积完成的操作如下图所示：</p>
<p><img src="/images/ReID-base/image-20211224142608718.png" alt="image-20211224142608718"></p>
<p><img src="/images/ReID-base/dilation.gif" alt="image-20211224142608718"></p>
<p>上图中，黑色的圆点表示3×3卷积核，灰色地带表示卷积之后的感受野。a，b，c为：</p>
<ul>
<li>a是普通的卷积过程**(dilation rate = 1)**,卷积后的感受野为3，正常的卷积过程dilation默认值为1</li>
<li>b是dilation rate = 2的空洞卷积,卷积后的感受野为5</li>
<li>c是dilation rate = 3的空洞卷积,卷积后的感受野为7</li>
</ul>
<p>使用空洞卷积的优点在于：不改变输出特征图的尺寸的情况下，增大了感受野。神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；相反，值越小则表示其所包含的特征越趋向局部和细节。因此感受野的值可以用来大致判断每一层的抽象层次。</p>
<p>当每层的卷积核大小设为3×3不变，每一层设置不同的dilation rate，那么每层的感受野也就不同。每层得到的输出特征图中也就获得了多尺度的信息，而且特征图的尺寸并没有发生变化。如果采用较大的卷积核实现扩大感受野，后续还需要加入池化层等下采样从而丢失信息。空洞卷积就避免了这一信息损失。</p>
<p>感受野计算过程如下：</p>
<p><img src="/images/ReID-base/image-20211224150656786.png" alt="image-20211224150656786"></p>

        <h4 id="1-2-模型压缩与加速">
          <a href="#1-2-模型压缩与加速" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-模型压缩与加速" class="headerlink" title="1.2 模型压缩与加速"></a>1.2 模型压缩与加速</h4>
      
        <h5 id="1-2-1-理论基础">
          <a href="#1-2-1-理论基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-1-理论基础" class="headerlink" title="1.2.1 理论基础"></a>1.2.1 理论基础</h5>
      <p>必要性<br> 在许多网络结构中，如VGG-16网络，参数数量1亿3千多万，占用500MB空间，需要进行309亿次浮点运算才能完成一次图像识别任务。</p>
<p>可行性<br> 论文<Predicting parameters in deep learning>提出，其实在很多深度的神经网络中存在着显著的冗余。仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。</Predicting></p>
<p>最终目的<br> 最大程度的减小模型复杂度，减少模型存储需要的空间，也致力于加速模型的训练和推测</p>

        <h5 id="1-2-2-方法分类">
          <a href="#1-2-2-方法分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-2-方法分类" class="headerlink" title="1.2.2 方法分类"></a>1.2.2 方法分类</h5>
      <p><img src="/images/ReID-base/image-20211224163346434.png" alt="image-20211224163346434"></p>

        <h5 id="1-2-3-前端压缩：">
          <a href="#1-2-3-前端压缩：" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-3-前端压缩：" class="headerlink" title="1.2.3 前端压缩："></a>1.2.3 前端压缩：</h5>
      <p>①、知识蒸馏</p>
<p>采取的方法是迁移学习，通过预训练好的教师模型(Teacher Model)的输出作为监督信号取训练另外一个轻量化的网络(Student Model)</p>
<p><img src="/images/ReID-base/image-20211225090519930.png" alt="image-20211225090519930"></p>
<p><img src="/images/ReID-base/image-20211225090645290.png" alt="image-20211225090645290"></p>
<p>学生和老师都对图片分别预测，目的是让学生和老师预测结果的概率分布尽可能像。老师和学生的预测结果p1,p2。对p1和p2计算KL散度作为一种loss加入到学生网络的loss Lc2中共同优化。</p>

        <h4 id="二-行人重识别-度量学习与表征学习">
          <a href="#二-行人重识别-度量学习与表征学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#二-行人重识别-度量学习与表征学习" class="headerlink" title="二 行人重识别   度量学习与表征学习"></a>二 行人重识别   度量学习与表征学习</h4>
      <p><img src="/images/ReID-base/image-20211225092537689.png" alt="image-20211225092537689"></p>

        <h5 id="2-1-系统构成">
          <a href="#2-1-系统构成" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-系统构成" class="headerlink" title="2.1 系统构成"></a>2.1 系统构成</h5>
      <p><img src="/images/ReID-base/image-20211225092830122.png" alt="image-20211225092830122"></p>
<p>原始视频帧进行行人检测模块生成Gallery集，待检索图片集称之为Probe。虽然系统分为两个模块但是学术研究大多只集中在行人重识别的特征提取部分。</p>
<p><img src="/images/ReID-base/image-20211225100413384.png" alt="image-20211225100413384"></p>

        <h5 id="2-2-评价指标">
          <a href="#2-2-评价指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-评价指标" class="headerlink" title="2.2 评价指标"></a>2.2 评价指标</h5>
      <p>①rank-k/top-k</p>
<p><img src="/images/ReID-base/image-20211225101543620.png" alt="image-20211225101543620"></p>
<p>rank-k计算的是整个探针集Probe中的所有图片的命中概率。</p>
<p>对Probe1，和Probe3，Rank1击中，那么整个探针集的rank1就是2/5=0.4</p>
<p>Probe2 Rank-4击中，与Probe1和Probe3加起来就是3/5=0.6的Rank-5</p>
<p>② CMC曲线</p>
<p>其实就是Rank-k的曲线</p>
<p><img src="/images/ReID-base/image-20211225102201277.png" alt="image-20211225102201277"></p>
<p>③ mAP(mean average precision)</p>
<p>反映检索的人在数据库中所有正确图片排在排序列表前面的成都，更加全面的衡量ReID算法的性能。</p>
<p>以下图Probe1为例，排序列表里面有3张正确的身份图片。分别是Top1、Top4、Top9</p>
<p>ap的计算如下图所示，mAP是对Query集里全部的AP取平均。</p>
<p><img src="/images/ReID-base/image-20211225102415691.png" alt="image-20211225102415691"></p>
<p>④ 评价模式</p>
<p><img src="/images/ReID-base/image-20211225103052466.png" alt="image-20211225103052466"></p>

        <h5 id="2-3-表征学习常用的损失Loss">
          <a href="#2-3-表征学习常用的损失Loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-表征学习常用的损失Loss" class="headerlink" title="2.3 表征学习常用的损失Loss"></a>2.3 表征学习常用的损失Loss</h5>
      
        <h6 id="2-3-1-损失的分类">
          <a href="#2-3-1-损失的分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-1-损失的分类" class="headerlink" title="2.3.1 损失的分类"></a>2.3.1 损失的分类</h6>
      <p><img src="/images/ReID-base/image-20211225143228526.png" alt="image-20211225143228526"></p>
<p><img src="/images/ReID-base/image-20211225143426922.png" alt="image-20211225143426922"></p>

        <h6 id="2-3-2-分类损失">
          <a href="#2-3-2-分类损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-2-分类损失" class="headerlink" title="2.3.2 分类损失"></a>2.3.2 分类损失</h6>
      <p><img src="/images/ReID-base/image-20211225143943301.png" alt="image-20211225143943301"></p>
<p>为什么在测试阶段需要丢弃分类FC层？因为在训练集和测试集中的行人ID并不相同，直接用训练集的FC输出分类ID毫无意义。因此在测试阶段直接使用训练的特征提取层提取的Probe的特征向量与Gallery比对检索。</p>

        <h6 id="2-3-3-属性损失">
          <a href="#2-3-3-属性损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-3-属性损失" class="headerlink" title="2.3.3 属性损失"></a>2.3.3 属性损失</h6>
      <p><img src="/images/ReID-base/image-20211225144337836.png" alt="image-20211225144337836"></p>
<p>由于行人具有一系列的属性，比如头发的颜色，上衣的颜色，裤子颜色，鞋子的种类以及颜色等等。这些每一个属性都可以和ID一样作为一个分类来输出分类结果。因此每个属性都可以通过softmax计算交叉熵损失与ID损失一起组成总的loss进行优化。</p>
<p>当然属性损失作为分类损失的类似，在测试阶段由于ID、属性等等与训练集不同，在测试阶段还是将所有的分类FC全部丢弃掉，指使用特征提取层。</p>

        <h6 id="2-3-4-验证损失">
          <a href="#2-3-4-验证损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-4-验证损失" class="headerlink" title="2.3.4 验证损失"></a>2.3.4 验证损失</h6>
      <p><img src="/images/ReID-base/image-20211225145235952.png" alt="image-20211225145235952"></p>
<p>验证损失一般是训练一个特征提取网络，同时输入两张图片提取特征之后将两个特征进行特征融合(比如直接相减计算特征向量的差异)，融合后的特征输入到后续网络通过特征提取计算作为一个二分类问题输出是/否属于一个ID。用于二分类的损失成为验证损失(如上图右上角的Verification Subnet)。</p>
<p>验证损失往往和ID损失一起使用，共同优化整个网络。如右下角的(Classification Subnet)，两张图片可以分别计算ID Loss。</p>

        <h6 id="2-3-5-表征学习的总结">
          <a href="#2-3-5-表征学习的总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-5-表征学习的总结" class="headerlink" title="2.3.5 表征学习的总结"></a>2.3.5 表征学习的总结</h6>
      <p><img src="/images/ReID-base/image-20211225145942198.png" alt="image-20211225145942198"></p>

        <h5 id="2-4-度量学习">
          <a href="#2-4-度量学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-度量学习" class="headerlink" title="2.4 度量学习"></a>2.4 度量学习</h5>
      <p><img src="/images/ReID-base/image-20211225150221293.png" alt="image-20211225150221293"></p>
<p>使用简单化的描述语言来说就是，度量学习ReID任务主要需要做以下工作</p>
<p>①、训练出一个特征提取网络，对图片提取出特征向量。</p>
<p>②、定义一个距离度量损失函数，计算两张图片特征向量之间的度量距离。</p>
<p>③、计算度量损失函数，最优化度量损失函数使相同行人的图片对之间的距离尽可能小，不同行人的图片对之间的距离尽可能大。通过最优化度量损失函数去优化特征提取网络。</p>
<p><em><strong>深度学习解决ReID问题的目标在于提取更优的特征，更加具有度量属性的特征。</strong></em></p>

        <h6 id="2-4-1-度量学习的流程">
          <a href="#2-4-1-度量学习的流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-1-度量学习的流程" class="headerlink" title="2.4.1 度量学习的流程"></a>2.4.1 度量学习的流程</h6>
      <p>基本也是行人检测–特征提取–训练的流程。</p>
<p><img src="/images/ReID-base/image-20211225151200154.png" alt="image-20211225151200154"></p>

        <h5 id="2-5-度量学习的损失函数">
          <a href="#2-5-度量学习的损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-度量学习的损失函数" class="headerlink" title="2.5 度量学习的损失函数"></a>2.5 度量学习的损失函数</h5>
      
        <h6 id="2-5-1-对比损失Contrastive-loss">
          <a href="#2-5-1-对比损失Contrastive-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-1-对比损失Contrastive-loss" class="headerlink" title="2.5.1 对比损失Contrastive loss"></a>2.5.1 对比损失Contrastive loss</h6>
      <p><img src="/images/ReID-base/image-20211227094927928.png" alt="image-20211227094927928"></p>
<p>对比损失的损失函数如上图<em>Lc</em>所示。</p>
<p>当输入的一对图片是正样本对a,b时，因为正样本y=1，<em>Lc</em>的后半部分为0，优化目标就是<em>Lc</em>的前半部分，也就是a和b特征向量的距离。通过优化器最小化损失函数使正样本对间的距离趋于0。</p>
<p>当输入的一对图片是负样本对a,b时，因为负样本y=0，<em>Lc</em>的前半部分为0，优化目标就是<em>Lc</em>的后半部分。由于<img src="/images/ReID-base/image-20211227102409369.png" alt="image-20211227102409369" style="zoom:40%;"></p>
<p>z = α-距离。当二者之间的距离大于α时, z小于0，max(z, 0) = 0，无需优化。当二者之间的距离越小于α时, z大于0， <em>Lc</em>越大，优化器将优化 <em>Lc</em>使其向0的方向优化，即使二者之间的距离向大于α的方向优化。也就实现了推开负样本的期望功能。其中α是自己设置的一个超参数，作为负样本分开的阈值。</p>

        <h6 id="2-5-2-三元组损失Triplet-loss">
          <a href="#2-5-2-三元组损失Triplet-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-2-三元组损失Triplet-loss" class="headerlink" title="2.5.2 三元组损失Triplet loss"></a>2.5.2 三元组损失Triplet loss</h6>
      <p><img src="/images/ReID-base/image-20211227104924310.png" alt="image-20211227104924310"></p>
<p>三元组损失主要将样本分为Anchor、Positive、Negative。一个三元组的构成方式是从训练集中随机选取一个样本，该样本称为Anchor，记为x_a， 然后再随机抽取一个与Anchor属于同一类的样本x_p和一个与Anchor属于不同类的样本x_n。由此构成一个(Anchor、Positive、Negative)三元组。Triplet Loss的目的在于通过学习，让x_a和x_p特征表达之间的距离尽可能小，而x_a和x_n之间的距离尽可能大。并且要让_a和x_n之间的距离和x_a和x_p之间的距离之间存在一个最小间隔α。即目标函数的优化方向为：</p>
<p><img src="/images/ReID-base/image-20211227105706609.png" alt="image-20211227105706609"></p>
<p>因此损失函数的形式为</p>
<p><img src="/images/ReID-base/image-20211227110756868.png" alt="image-20211227110756868"></p>
<p><img src="/images/ReID-base/image-20211227111029204.png" alt="image-20211227111029204"></p>

        <h6 id="2-5-3-改进的三元组损失">
          <a href="#2-5-3-改进的三元组损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-3-改进的三元组损失" class="headerlink" title="2.5.3 改进的三元组损失"></a>2.5.3 改进的三元组损失</h6>
      <p><img src="/images/ReID-base/image-20211227111131013.png" alt="image-20211227111131013"></p>
<p>其实就在原三元组损失的基础上加上了一项x_a和x_p之间的距离。使得在满足三元组损失优化目标的同时使x_a和x_p之间的距离尽可能的小。</p>

        <h6 id="2-5-4-四元组损失-Quadruplet-loss">
          <a href="#2-5-4-四元组损失-Quadruplet-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-4-四元组损失-Quadruplet-loss" class="headerlink" title="2.5.4 四元组损失 Quadruplet loss"></a>2.5.4 四元组损失 Quadruplet loss</h6>
      <p><img src="/images/ReID-base/image-20211227111507714.png" alt="image-20211227111507714"></p>
<p>四元组损失也是基于三元组损失的基础上，但是需要四张图片组成一个四元组。一个四元组的构成为从训练集中随机选取一个样本，该样本称为Anchor，记为x_a， 然后再随机抽取一个与Anchor属于同一类的样本x_p和<strong>两个</strong>与Anchor属于不同类的样本x_n1和x_n2。并且这两个负样本n1和n2<strong>分别属于两个不同的ID</strong>。</p>
<p>四元组损失的公式如上图所示。第一项使正常的三元组损失。而第二项分别计算Anchor和正样本的距离da,p和两个负样本的距离dn1,n2。使得正样本之间的距离不仅小于正、负样本对的距离，也小于两张来自不同类负样本之间的距离。</p>
<p>功能：</p>
<p>进一步缩小正样本之间的距离，在推开正负样本对的距离的同时推开不同类的负样本对之间的距离。</p>

        <h6 id="2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失">
          <a href="#2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失" class="headerlink" title="2.5.5 TriHard Loss/Batch-Hard Loss 批难三元组损失"></a>2.5.5 TriHard Loss/Batch-Hard Loss 批难三元组损失</h6>
      <p><img src="/images/ReID-base/image-20211227112344030.png" alt="image-20211227112344030"></p>
<p><img src="/images/ReID-base/image-20211227134125333.png" alt="image-20211227134125333"></p>
<p><img src="/images/ReID-base/image-20211227142426071.png" alt="image-20211227142426071"></p>
<p>如图所示，TriHard Loss实现的方式是构建这样一个距离矩阵。如上图，每个batch选取N个ID的行人，每个ID选取3张图片。将这个batch内的全部图片两两计算距离填入上方的矩阵。对角线上表示同一张图片自己到自己的距离，因此为0。红色矩阵块表示同一个ID的三张图片之间的距离，绿色矩阵块表示不同ID的图片之间的距离。衡量是否Hrad然是通过距离来衡量。<strong>对于红色矩阵，也就是正样本对来说，红色矩阵的每一行的最大值，以第一行为例，这一行代表行人1-1这张图片与三个正样本的距离，对于正样本来说，正样本之间的距离越大，说明越Hard。</strong>因此将红色矩阵组合起来求每一行的最大值组成一列，这一列就代表了这个batch里面的每一张图片与其对应的最Hard样本的距离。</p>
<p><strong>同理，对正负样本对来说，距离越小越Hard</strong>。将绿色矩阵拼在一起组成方阵，求每一行的最小值。每一行的最小值组成的一列Tensor就是正负样本对的最难距离。</p>
<p><img src="/images/ReID-base/image-20211227145701227.png" alt="image-20211227145701227"></p>
<p>根据上述公式，累加之后除以批量大小就得到了TriHard loss。</p>

        <h6 id="2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组">
          <a href="#2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组" class="headerlink" title="2.5.6 Triplet loss with adaptive weights 自适应权重三元组"></a>2.5.6 Triplet loss with adaptive weights 自适应权重三元组</h6>
      <p>批难三元组由于只考虑了极端样本的信息，有些信息没有考虑到。</p>
<p>有文章指出批难三元组损失在某些极端情况下，比如数据集有一定数量的标注错误可能会因为训练时梯度特别大而导致网络崩溃。当网络使用批难三元组难以收敛的情况下可以考虑自适应权重三元组。</p>
<p><img src="/images/ReID-base/image-20211227150111074.png" alt="image-20211227150111074"></p>

        <h6 id="2-5-7-度量学习的总结">
          <a href="#2-5-7-度量学习的总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-7-度量学习的总结" class="headerlink" title="2.5.7 度量学习的总结"></a>2.5.7 度量学习的总结</h6>
      <p><img src="/images/ReID-base/image-20211227150524190.png" alt="image-20211227150524190"></p>
<p>度量学习和表征学习往往可以用来共同训练一个模型，将多个损失组合在一起共同优化一个模型。这种做法也是业界精确度比较高的一种做法。</p>
<p><img src="/images/ReID-base/image-20211227151111989.png" alt="image-20211227151111989"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/24/C-Chapter2/">C++Primer 第二章学习笔记</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-30</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="C-Primer第二章–变量和基本类型">
          <a href="#C-Primer第二章–变量和基本类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#C-Primer第二章–变量和基本类型" class="headerlink" title="C++ Primer第二章–变量和基本类型"></a>C++ Primer第二章–变量和基本类型</h3>
      <p><em><strong>数据类型是程序的基础，它告诉了我们数据的意义以及我们能在数据上执行的操作。</strong></em></p>

        <h4 id="一、基本内置类型">
          <a href="#一、基本内置类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、基本内置类型" class="headerlink" title="一、基本内置类型"></a>一、基本内置类型</h4>
      <p>基本内置类型主要包括<strong>算术类型</strong>和<strong>空类型</strong></p>

        <h5 id="1-1-算术类型">
          <a href="#1-1-算术类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-算术类型" class="headerlink" title="1.1 算术类型"></a>1.1 算术类型</h5>
      <p>算术类型主要分为<strong>整型</strong>（integral type），和<strong>浮点型</strong>。其中<strong>字符类型</strong>与<strong>布尔类型</strong>均包括在整型之内。</p>
<p><img src="/images/C++Chapter2/image-20211224201207827.png" alt="image-20211224201207827"></p>
<p><strong>注：由于float类型精度有限，实际应用中常用的是double，并且二者在计算代价上相差无几。</strong></p>

        <h5 id="1-2-带符号类型与不带符号类型">
          <a href="#1-2-带符号类型与不带符号类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-带符号类型与不带符号类型" class="headerlink" title="1.2 带符号类型与不带符号类型"></a>1.2 带符号类型与不带符号类型</h5>
      <p>除去bool和扩展的字符类型之外，其它的整型可以划分为带符号和无符号的。带符号的可以表示正数、负数和0，无符号类型只能表示大于等于0的值。</p>
<p>C++中需要无符号的值的时候，在前面加上<strong>unsinged</strong>即可，如int-&gt;unsigned int。</p>

        <h5 id="1-3-字面值常量">
          <a href="#1-3-字面值常量" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-3-字面值常量" class="headerlink" title="1.3 字面值常量"></a>1.3 字面值常量</h5>
      <p>字面值常量(literal)是指一类仅从字面上理解就能看出值的常量。每个字面值都对应一种数据类型。具体来说，字面值常量分为<strong>整型</strong>和<strong>浮点型</strong>字面值。</p>
<p><strong>整型字面值常量</strong></p>
<p>整型字面值可以写成十进制、八进制或者十六进制。以<strong>0开头的数表示八进制数，以0x开头的代表16进制数。</strong></p>
<p>如：</p>
<p><img src="/images/C++Chapter2/image-20211225195058728.png" alt="image-20211225195058728"></p>
<p><strong>浮点型字面值</strong></p>
<p>表现为一个小鼠或科学计数法表示的指数，指数部分用E或者e标识，但该指数实际上以10为底</p>
<p><img src="/images/C++Chapter2/image-20211225200452808.png" alt="image-20211225200452808"></p>
<p><strong>字符型字面值</strong></p>
<p>表现为单引号括起来的字符或者双引号括起来的字符串。</p>
<p>一个细节是字符串字面值实际上是由常量字符构成的数组，编译器在每个字符串的结尾处会添加一个空字符（’\0’），因此字符串的字面值的实际长度会比字符串的实际内容多1。</p>
<p>如：</p>
<p>‘A’ 表示一个字符型字面值，长度为1</p>
<p>“A”表示一个字符串型字面值，长度为2</p>
<p>可以通过添加前缀和后缀来改变字面值的默认类型</p>
<p><img src="/images/C++Chapter2/image-20211225204253158.png" alt="image-20211225204253158"></p>
<p><img src="/images/C++Chapter2/image-20211225204325836.png" alt="image-20211225204325836"></p>

        <h4 id="二、变量">
          <a href="#二、变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、变量" class="headerlink" title="二、变量"></a>二、变量</h4>
      <p><em><strong>变量实际上提供一个具名的、可供程序操作的存储空间。</strong></em>数据类型决定变量所占据的空间的大小和布局方式、以及该空间能存储的值的范围、变量能参与的运算。</p>
<p><strong>对象是指一块能存储数据并且具有某种类型的内存空间。</strong></p>

        <h5 id="2-1-变量的初始化">
          <a href="#2-1-变量的初始化" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-变量的初始化" class="headerlink" title="2.1 变量的初始化"></a>2.1 变量的初始化</h5>
      
        <h6 id="2-1-1-初始值">
          <a href="#2-1-1-初始值" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-1-初始值" class="headerlink" title="2.1.1 初始值"></a>2.1.1 初始值</h6>
      <p>变量在创建的时候获得了一个特定的值，我们说这个对象被初始化了。</p>
<p><strong>初始化不是赋值，初始化是在创建变量的时候赋予变量一个初始值。而赋值的定义是把对象的当前值擦除掉，用一个新的值替代。</strong></p>

        <h6 id="2-1-2-列表初始化">
          <a href="#2-1-2-列表初始化" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-2-列表初始化" class="headerlink" title="2.1.2 列表初始化"></a>2.1.2 列表初始化</h6>
      <p>使用一个大括号{}来对变量进行初始化的方式叫做列表初始化。主要有以下第二、第三种方式：</p>
<p><img src="/images/C++Chapter2/image-20211225205300870.png" alt="image-20211225205300870"></p>
<p>这种初始化的好处在于：<strong>如果我们使用列表初始化且初始值存在丢失信息的风险，编译器会报错且不予执行。在一定程度上避免了变量的类型与初始值类型不匹配类型强转导致的数值丢失</strong></p>
<p><img src="/images/C++Chapter2/image-20211225205559552.png" alt="image-20211225205559552"></p>
<p><img src="/images/C++Chapter2/image-20211225205729977.png" alt="image-20211225205729977"></p>

        <h5 id="2-2-变量声明和定义">
          <a href="#2-2-变量声明和定义" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-变量声明和定义" class="headerlink" title="2.2 变量声明和定义"></a>2.2 变量声明和定义</h5>
      <p>C++支持分离式编译，程序可以拆分成多个文件，一个文件可以使用其它文件中定义的函数和变量。但是使用其他文件中的变量需要在程序中声明。声明和定义是不同的。定义不仅会规定变量的类型与名字，而且会额外申请存储空间。而声明则不需要申请存储空间。声明需要在变量名前添加<strong>关键字extern</strong>，而且<strong>不能对声明的变量做显式初始化。</strong></p>
<p><strong>extern语句如果包含了初始值就不再是声明，而是定义了。</strong></p>
<p><img src="/images/C++Chapter2/image-20211225215525019.png" alt="image-20211225215525019"></p>
<p><strong>一个变量只能定义一次，但可以多次声明。</strong></p>

        <h5 id="2-3-标识符">
          <a href="#2-3-标识符" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-标识符" class="headerlink" title="2.3 标识符"></a>2.3 标识符</h5>
      <p>C++中的标识符必须以字母或者下划线开头。定义在函数体外的标识符不能以下划线开头。</p>
<p><img src="/images/C++Chapter2/image-20211225215816509.png" alt="image-20211225215816509"></p>

        <h4 id="三、复合类型">
          <a href="#三、复合类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、复合类型" class="headerlink" title="三、复合类型"></a>三、复合类型</h4>
      
        <h5 id="3-1-引用">
          <a href="#3-1-引用" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-引用" class="headerlink" title="3.1 引用"></a>3.1 引用</h5>
      <p>引用类型通过将声明符号写成&amp;d的形式来定义，d是被引用的变量名。但是引用并非创建了一个新对象，它实际上是一个已经定义的对象的<strong>别名</strong>。</p>
<p><img src="/images/C++Chapter2/image-20211225220329035.png" alt="image-20211225220329035"></p>
<p>程序是将引用与它的初始值<strong>绑定</strong>在一起，而不是直接把初始值拷贝过去。因此<strong>引用必须初始化。</strong>并且一旦初始化完成，无法令引用重新绑定到另外一个对象。</p>
<p>定义了引用后，对引用对象所做的所有操作都是在被绑定的原对象上直接进行操作的。</p>
<p>引用只能使用对象对其初始化，而且定义引用的类型要与被绑定的对象的类型完全一致。</p>
<p><img src="/images/C++Chapter2/image-20211225220759531.png" alt="image-20211225220759531"></p>

        <h5 id="3-2-指针">
          <a href="#3-2-指针" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2-指针" class="headerlink" title="3.2 指针"></a>3.2 指针</h5>
      <p>指针也是”指向”另外一种类型的对象的复合类型。它和引用都可以实现对其它对象的间接访问。但是它和指针有很多不同点，具体如下：</p>
<p><strong>①、指针本身就是一个对象，而引用只是一个”别名”，并非创建了一个新对象。</strong></p>
<p><strong>②、指针允许赋值和拷贝，而对引用实际上只是给被引用的对象赋值。</strong></p>
<p><strong>③、指针在生命周期之内可以先后指向不同的对象，而引用只能绑定在一个对象上，不允许改变。</strong></p>
<p><strong>④、指针不需要在定义的时候赋初值</strong></p>
<p><strong>定义方法</strong></p>
<p><img src="/images/C++Chapter2/image-20211225222534490.png" alt="image-20211225222534490"></p>
<p><strong>指针对象实际上存放的是某个对象的地址，想要获取这个地址需要使用取地址符&amp;</strong></p>
<p><img src="/images/C++Chapter2/image-20211225222656042.png" alt="image-20211225222656042"></p>
<p>p是一个指针变量，指向变量ival，并且p实际上存放的是变量ival的地址。</p>
<p><img src="/images/C++Chapter2/image-20211225222930490.png" alt="image-20211225222930490"></p>
<p><img src="/images/C++Chapter2/image-20211225222957674.png" alt="image-20211225222957674"></p>
<p><strong>利用指针访问对象</strong></p>
<p>如果指针指向了一个对象，允许使用解引用符(操作符*)来访问该对象。对指针解引用会得到所指的对象。</p>
<p><img src="/images/C++Chapter2/image-20211226133703807.png" alt="image-20211226133703807"></p>
<p><strong>空指针</strong></p>
<p>当暂时不知道指针应该指向何处的时候，应当先对指针初始化为空，空指针的初始化操作是：int *p = <strong>nullptr</strong>;。nullptr是一种特殊的字面值，它可以转化成任意的指针类型。</p>
<p><strong>指向指针的指针</strong></p>
<p>*的个数可以区分指针的级别，因此**表示指向指针对象的指针。</p>
<p><img src="/images/C++Chapter2/image-20211226135121453.png" alt="image-20211226135121453"></p>
<p>此时要通过ppi来访问原对象，需要两次解引用，即**ppi</p>

        <h5 id="3-3-const限定符">
          <a href="#3-3-const限定符" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-const限定符" class="headerlink" title="3.3 const限定符"></a>3.3 const限定符</h5>
      <p>使用const限定符修饰的变量是一种不可改变值的变量，也可称为const常量。任何对const常量赋值的操作都将报错。因为const常量一经定义就不可改变，因此const对象必须初始化。</p>
<p>如：<code>const int bufSize = 512;</code></p>
<p>默认状态下，const限定符仅对文件内生效，如果需要跨文件使用，需要对const常量的定义前加上extern</p>
<p><code>extern const int bufSize = 512;</code></p>
<p>当在当在其他文件中需要使用该变量的时候，也需要在文件中使用extern进行声明。</p>
<p><code>extern const int bufSize;</code></p>

        <h6 id="3-3-1-const的引用">
          <a href="#3-3-1-const的引用" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-1-const的引用" class="headerlink" title="3.3.1 const的引用"></a>3.3.1 const的引用</h6>
      <p>和普通对象一样，我们也可以把引用绑定到const对象上，称之为<strong>对常量的引用</strong>。但是由于被绑定的对象是一个常量，因此<strong>不可以改变被引用对象的值。</strong></p>
<p><code>const int c1 = 1024; </code></p>
<p><code>const int &amp;r1 = c1;</code></p>
<p>与一般的引用不同的是，常量引用的类型并不一定需要与其所引用的对象的类型完全一致。允许常量引用绑定一个非常量对象、字面值、甚至是一个表达式。<strong>因为编译器会自动创建一个与常量引用类型相同的临时常量，这个并将常量引用绑定的非常量对象、字面值、甚至是一个表达式的值拷贝给这个临时常量。最后将这个常量引用绑定到这个临时常量。</strong></p>
<p><img src="/images/C++Chapter2/image-20211226152752659.png" alt="image-20211226152752659"></p>
<p>常量引用仅仅对引用的操作做出了限定，但是对被引用绑定的对象是否是常量并未作规定。即不能通过常量引用这个别名来改变这个被绑定对象的值，常量引用也不能改变绑定的对象。但是因为被绑定对象并不是一个常量，可以通过其他方式改变这个值。</p>

        <h6 id="3-3-2-指向常量的指针">
          <a href="#3-3-2-指向常量的指针" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-2-指向常量的指针" class="headerlink" title="3.3.2 指向常量的指针"></a>3.3.2 指向常量的指针</h6>
      <p>const与指针结合可以结合成常量指针(const pointer)与指向常量的指针(pointer to const)。</p>
<p><strong>指向常量的指针</strong>，<strong>不能用于改变其所指对象的值。</strong>因为其指向的对象是一个常量。由之前指针的描述，指针是一个对象，用于存放所指对象的地址。指向常量的指针就存放的是常量的地址。</p>
<p>定义方式：</p>
<figure class="highlight c++"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> 指针的数据类型 *指针名 = &amp;指向的对象地址;</span><br></pre></td></tr></table></div></figure>

<p><img src="/images/C++Chapter2/image-20211227163615973.png" alt="image-20211227163615973"></p>
<p><strong>指向常量的指针却允许指向一个非常量。</strong></p>
<p><img src="/images/C++Chapter2/image-20211227163822295.png" alt="image-20211227163822295"></p>
<p><strong>不管是常量的引用还是指向常量的指针，都只是禁止了通过这个引用或者指针去修改被指向的对象。</strong>当它们指向一个非常量时，并不禁止通过其它手段去修改这个非常量的值。</p>

        <h6 id="3-3-3-const指针-常量指针">
          <a href="#3-3-3-const指针-常量指针" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-3-const指针-常量指针" class="headerlink" title="3.3.3 const指针/常量指针"></a>3.3.3 const指针/常量指针</h6>
      <p>引用并不是一个对象，而指针是一个对象，因此指针本身允许被定义成一个常量。既然<strong>常量指针本身是一个常量</strong>，因此<strong>它就必须被初始化。而且它本身的值(即被常量指针指向的对象的地址)不允许改变。</strong></p>
<p>定义方式：</p>
<p>注意 const指针对象本身的类型是const，因此定义const指针应当把*号写在const前。</p>
<figure class="highlight c++"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">被指向对象的类型 *<span class="keyword">const</span> 指针名 = &amp;被指向对象名;</span><br></pre></td></tr></table></div></figure>

<p><img src="/images/C++Chapter2/image-20211227165429436.png" alt="image-20211227165429436"></p>
<p>注意：</p>
<p><strong>常量指针指的是指针本身是一个常量。也就是说这个指针保存的地址，也就是指向哪个对象是不允许改变的，但是这个地址指向的对象的值是可以改变的。</strong>可以通过常量指针修改被指向对象的值。</p>
<p><img src="/images/C++Chapter2/image-20211227165809504.png" alt="image-20211227165809504"></p>

        <h6 id="3-3-4-constexpr-变量">
          <a href="#3-3-4-constexpr-变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-4-constexpr-变量" class="headerlink" title="3.3.4 constexpr 变量"></a>3.3.4 constexpr 变量</h6>
      <p>常量表达式是是一种特殊的表达式，它的计算过程发生在编译阶段，并且它的计算结果不会改变。因此对于一些确定不会改变的计算结果，与其将它的参数声明成一个普通const常量再计算。不如将该表达式声明成constexpr 类型，将计算阶段放在编译。加速代码执行。</p>
<p>一个对象是否是一个常量表达式，由它的数据类型和参与计算结果的初始值共同决定。要求：计算结果的数据类型应当是常量类型，并且参与运算的都是字面值，也就是不需要计算阶段就可以得出的值</p>
<figure class="highlight c++"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> max_files = <span class="number">20</span>;   <span class="comment">//max_files是一个常量表达式</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> limit = max_files + <span class="number">1</span>; <span class="comment">//计算结果的数据类型是const，并且参与运算的max_files是一个确定的常量，故limit是常量表达式</span></span><br><span class="line"><span class="keyword">int</span> staff_size = <span class="number">27</span>;   <span class="comment">//数据类型不是常量，不是常量表达式</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> sz = <span class="built_in">get_size</span>();  <span class="comment">//虽然数据类型声明成const，但是它的值需要在执行阶段计算，故不是常量表达式</span></span><br><span class="line">   </span><br></pre></td></tr></table></div></figure>

<p>当都使用const来声明常量表达式，编译器很难确定变量的值是否是一个常量表达式。也就无法将计算阶段放在编译，加速代码执行。因此C++11引入了constexpr类型</p>
<p><img src="/2021/12/24/C-Chapter2/images/C++Chapter2/image-20211230211752515.png" alt="image-20211230211752515"></p>
<p>需要指出的是，如果constexpr声明中定义了一个指针，限定符constexpr仅对指针生效。也就是说仅仅指针被声明成了常量类型，其保存的地址不变，与它所指向的对象的值无关。</p>

        <h4 id="四、处理类型">
          <a href="#四、处理类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#四、处理类型" class="headerlink" title="四、处理类型"></a>四、处理类型</h4>
      
        <h5 id="4-1-类型别名">
          <a href="#4-1-类型别名" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-类型别名" class="headerlink" title="4.1 类型别名"></a>4.1 类型别名</h5>
      <p>类型别名就是给某种数据类型起一个同义词。它可以使复杂的类型名变得简单明了，便于理解和使用。</p>
<p>两种方式：</p>
<figure class="highlight c++"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">double</span> wages; <span class="comment">//wages是double的同义词</span></span><br><span class="line"><span class="keyword">typedef</span> wages base, *p;  <span class="comment">//等价于double base;和double *p</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> SI = Sales_item;</span><br><span class="line">SI item;  <span class="comment">//等价于Sales_item item;</span></span><br></pre></td></tr></table></div></figure>

</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/23/object-detect/">物体识别</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-23</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-24</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="一、物体识别相关术语解释">
          <a href="#一、物体识别相关术语解释" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、物体识别相关术语解释" class="headerlink" title="一、物体识别相关术语解释"></a>一、物体识别相关术语解释</h3>
      
        <h4 id="1-1-锚框、边缘框">
          <a href="#1-1-锚框、边缘框" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-锚框、边缘框" class="headerlink" title="1.1 锚框、边缘框"></a>1.1 锚框、边缘框</h4>
      <p><em>边界框</em>（bounding box）来描述对象的空间位置，一般描述的是物体的真实所在位置。 边界框是矩形的，由矩形左上角的以及右下角的x和y坐标决定。目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的<em>真实边界框</em>（ground-truth bounding box）。锚框（anchor box）一般是预测算法预先提出的一组框，它是以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框。算法检测预设置的锚框内是否有关注的物体，如果有，预测锚框到真实边缘框的偏移，从而对锚框进行调整。</p>

        <h5 id="1-1-1-锚框标注">
          <a href="#1-1-1-锚框标注" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-1-锚框标注" class="headerlink" title="1.1.1 锚框标注"></a>1.1.1 锚框标注</h5>
      <p>对于一张图片来进行目标检测，我们可能会生成大量的锚框。对每个锚框来说都是一个训练样本。我们需要将每个锚框要么标注成背景，要么关联上一个真实的边缘框。因此我们需要对锚框进行标注，常用的做法是计算每个锚框与每个边缘框的IoU，分别取最大值将锚框标注成边缘框相同的标注。</p>

        <h5 id="1-2-NMS非极大值抑制输出">
          <a href="#1-2-NMS非极大值抑制输出" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-NMS非极大值抑制输出" class="headerlink" title="1.2 NMS非极大值抑制输出"></a>1.2 NMS非极大值抑制输出</h5>
      <p>因为每个锚框都预测一个边缘框，因此可能出现很多锚框预测一个相同的边缘框，如图所示</p>
<p><img src="/2021/12/23/object-detect/images/object-detect/image-20211223100019550.png" alt="image-20211223100019550"></p>
<p>这些框之间的区别仅仅在于覆盖的范围不同。使用NMS可以合并相似的预测，主要的流程是先选中非背景类的最大预测值，然后去掉所有其它和这个最大预测锚框的IoU值大于Theta值(一个自己设置的阈值)的预测，重复以上流程。</p>

        <h4 id="1-2-比较锚框与真实框的相似度指标–IOU">
          <a href="#1-2-比较锚框与真实框的相似度指标–IOU" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-比较锚框与真实框的相似度指标–IOU" class="headerlink" title="1.2 比较锚框与真实框的相似度指标–IOU"></a>1.2 比较锚框与真实框的相似度指标–IOU</h4>
      <p>IoU往往用来计算两个框之间的相似度，计算公式是给定两个集合A和B，用它的交集闭上并集。</p>
<p><img src="/2021/12/23/object-detect/images/object-detect/image-20211223094115028.png" alt="image-20211223094115028"></p>
<p><img src="/2021/12/23/object-detect/images/object-detect/image-20211223094125414.png" alt="image-20211223094125414"></p>

        <h3 id="二、基于锚框的检测算法的流程">
          <a href="#二、基于锚框的检测算法的流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、基于锚框的检测算法的流程" class="headerlink" title="二、基于锚框的检测算法的流程"></a>二、基于锚框的检测算法的流程</h3>
      <h4 id><a href="#" class="headerlink" title></a><img src="/2021/12/23/object-detect/images/object-detect/image-20211223100414222.png" alt="image-20211223100414222"></h4></div></div></article></section><nav class="paginator"><div class="paginator-inner"><a class="extend prev" rel="prev" href="/page/3/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/5/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/photo.png" alt="avatar"></div><p class="sidebar-ov-author__text">To be a great person.</p></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2022</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Strive</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>