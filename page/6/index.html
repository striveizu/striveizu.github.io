<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="Strive&#39;s Blog">
<meta property="og:url" content="https://striveizu.tech/page/6/index.html">
<meta property="og:site_name" content="Strive&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Strive">
<meta name="twitter:card" content="summary"><title>Strive's Blog</title><link ref="canonical" href="https://striveizu.tech/page/6/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-address-card"></i></span><span class="header-nav-menu-item__text">关于</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-edit"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Strive's Blog</div><div class="header-banner-info__subtitle">你我期许的绝非遥不可及</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/27/ReID-base/">行人重识别需要的一些基础</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-27</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-27</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="行人重识别需要的一些基础">
          <a href="#行人重识别需要的一些基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#行人重识别需要的一些基础" class="headerlink" title="行人重识别需要的一些基础"></a>行人重识别需要的一些基础</h3>
      
        <h4 id="一、回顾神经网络的一些基础">
          <a href="#一、回顾神经网络的一些基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、回顾神经网络的一些基础" class="headerlink" title="一、回顾神经网络的一些基础"></a>一、回顾神经网络的一些基础</h4>
      
        <h4 id="1-1-卷积回顾">
          <a href="#1-1-卷积回顾" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-卷积回顾" class="headerlink" title="1.1 卷积回顾"></a>1.1 卷积回顾</h4>
      
        <h5 id="1-1-1-普通卷积">
          <a href="#1-1-1-普通卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-1-普通卷积" class="headerlink" title="1.1.1 普通卷积"></a>1.1.1 普通卷积</h5>
      <p>卷积的功能是特征提取。步长不等于0的时候起到下采样的作用。卷积层的参数量的计算公式如下：</p>
<p><img src="/images/ReID-base/image-20211224100331547.png" alt="image-20211224100331547"></p>
<p>一个小Tips：填充padding和kernel_size往往需要配套使用，主要设计是让<strong>2*padding+1=k</strong>。从而使输出size只与stride和输入的尺寸有关。举例如kernel_size选择为3*3，一般选择padding=1，k=5选择padding=2。输出特征图的size计算公式如下</p>
<p><img src="/images/ReID-base/image-20211224100452800.png" alt="image-20211224100452800"></p>

        <h5 id="1-1-2-1-1卷积">
          <a href="#1-1-2-1-1卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-2-1-1卷积" class="headerlink" title="1.1.2 1*1卷积"></a>1.1.2 1*1卷积</h5>
      <p><img src="/images/ReID-base/image-20211224102629624.png" alt="image-20211224102629624"></p>
<p>一般不独立使用。主要是作为降维操作使用，如ResNet的BottleNeck层的快速连接中为了考虑训练时间成本对特征图先用1*1卷积层降维再升维。</p>

        <h5 id="1-1-3-分组卷积">
          <a href="#1-1-3-分组卷积" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-3-分组卷积" class="headerlink" title="1.1.3 分组卷积"></a>1.1.3 分组卷积</h5>
      <p>完成的功能：对输入特征图在通道层面平均分成几组，每组在对应的卷积层做卷积之后再在通道维度上做拼接得到输出。</p>
<p><img src="/images/ReID-base/image-20211224103125148.png" alt="image-20211224103125148"></p>
<p>代码实现：修改Conv2d的group参数值</p>
<p><img src="/images/ReID-base/image-20211224103153758.png" alt="image-20211224103153758"></p>
<p>常用的情形：</p>
<p>由于分组卷积需要的参数量较少，常用于模型特别大作为模型压缩的一种手段。常作为对显存不够的一种妥协。</p>
<p>缺点：</p>
<p>由于分组卷积并不是整个feature map参与卷积特征提取，而是每一部分的输出只与一部分的特征图有关系，特征图之间的局部信息没有完全打通。两部分的输出之间可能存在的信息被忽略。</p>

        <h5 id="1-1-4-Channel-wise-Depthwise-Convolution">
          <a href="#1-1-4-Channel-wise-Depthwise-Convolution" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-4-Channel-wise-Depthwise-Convolution" class="headerlink" title="1.1.4 Channel-wise/Depthwise Convolution"></a>1.1.4 Channel-wise/Depthwise Convolution</h5>
      <p><img src="/images/ReID-base/image-20211224103739036.png" alt="image-20211224103739036"></p>

        <h5 id="1-1-5-空洞卷积（Dilated-Convolution">
          <a href="#1-1-5-空洞卷积（Dilated-Convolution" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-5-空洞卷积（Dilated-Convolution" class="headerlink" title="1.1.5 空洞卷积（Dilated Convolution)"></a>1.1.5 空洞卷积（Dilated Convolution)</h5>
      <p>空洞卷积引入了一个称为 “**扩张率(dilation rate)**”的超参数(hyper-parameter)，该参数定义了卷积核处理数据时各值的间距。扩张率中文也叫空洞数(Hole Size)。空洞卷积完成的操作如下图所示：</p>
<p><img src="/images/ReID-base/image-20211224142608718.png" alt="image-20211224142608718"></p>
<p><img src="/images/ReID-base/dilation.gif" alt="image-20211224142608718"></p>
<p>上图中，黑色的圆点表示3×3卷积核，灰色地带表示卷积之后的感受野。a，b，c为：</p>
<ul>
<li>a是普通的卷积过程**(dilation rate = 1)**,卷积后的感受野为3，正常的卷积过程dilation默认值为1</li>
<li>b是dilation rate = 2的空洞卷积,卷积后的感受野为5</li>
<li>c是dilation rate = 3的空洞卷积,卷积后的感受野为7</li>
</ul>
<p>使用空洞卷积的优点在于：不改变输出特征图的尺寸的情况下，增大了感受野。神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；相反，值越小则表示其所包含的特征越趋向局部和细节。因此感受野的值可以用来大致判断每一层的抽象层次。</p>
<p>当每层的卷积核大小设为3×3不变，每一层设置不同的dilation rate，那么每层的感受野也就不同。每层得到的输出特征图中也就获得了多尺度的信息，而且特征图的尺寸并没有发生变化。如果采用较大的卷积核实现扩大感受野，后续还需要加入池化层等下采样从而丢失信息。空洞卷积就避免了这一信息损失。</p>
<p>感受野计算过程如下：</p>
<p><img src="/images/ReID-base/image-20211224150656786.png" alt="image-20211224150656786"></p>

        <h4 id="1-2-模型压缩与加速">
          <a href="#1-2-模型压缩与加速" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-模型压缩与加速" class="headerlink" title="1.2 模型压缩与加速"></a>1.2 模型压缩与加速</h4>
      
        <h5 id="1-2-1-理论基础">
          <a href="#1-2-1-理论基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-1-理论基础" class="headerlink" title="1.2.1 理论基础"></a>1.2.1 理论基础</h5>
      <p>必要性<br> 在许多网络结构中，如VGG-16网络，参数数量1亿3千多万，占用500MB空间，需要进行309亿次浮点运算才能完成一次图像识别任务。</p>
<p>可行性<br> 论文<Predicting parameters in deep learning>提出，其实在很多深度的神经网络中存在着显著的冗余。仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。</Predicting></p>
<p>最终目的<br> 最大程度的减小模型复杂度，减少模型存储需要的空间，也致力于加速模型的训练和推测</p>

        <h5 id="1-2-2-方法分类">
          <a href="#1-2-2-方法分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-2-方法分类" class="headerlink" title="1.2.2 方法分类"></a>1.2.2 方法分类</h5>
      <p><img src="/images/ReID-base/image-20211224163346434.png" alt="image-20211224163346434"></p>

        <h5 id="1-2-3-前端压缩：">
          <a href="#1-2-3-前端压缩：" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-3-前端压缩：" class="headerlink" title="1.2.3 前端压缩："></a>1.2.3 前端压缩：</h5>
      <p>①、知识蒸馏</p>
<p>采取的方法是迁移学习，通过预训练好的教师模型(Teacher Model)的输出作为监督信号取训练另外一个轻量化的网络(Student Model)</p>
<p><img src="/images/ReID-base/image-20211225090519930.png" alt="image-20211225090519930"></p>
<p><img src="/images/ReID-base/image-20211225090645290.png" alt="image-20211225090645290"></p>
<p>学生和老师都对图片分别预测，目的是让学生和老师预测结果的概率分布尽可能像。老师和学生的预测结果p1,p2。对p1和p2计算KL散度作为一种loss加入到学生网络的loss Lc2中共同优化。</p>

        <h4 id="二-行人重识别-度量学习与表征学习">
          <a href="#二-行人重识别-度量学习与表征学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#二-行人重识别-度量学习与表征学习" class="headerlink" title="二 行人重识别   度量学习与表征学习"></a>二 行人重识别   度量学习与表征学习</h4>
      <p><img src="/images/ReID-base/image-20211225092537689.png" alt="image-20211225092537689"></p>

        <h5 id="2-1-系统构成">
          <a href="#2-1-系统构成" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-系统构成" class="headerlink" title="2.1 系统构成"></a>2.1 系统构成</h5>
      <p><img src="/images/ReID-base/image-20211225092830122.png" alt="image-20211225092830122"></p>
<p>原始视频帧进行行人检测模块生成Gallery集，待检索图片集称之为Probe。虽然系统分为两个模块但是学术研究大多只集中在行人重识别的特征提取部分。</p>
<p><img src="/images/ReID-base/image-20211225100413384.png" alt="image-20211225100413384"></p>

        <h5 id="2-2-评价指标">
          <a href="#2-2-评价指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-评价指标" class="headerlink" title="2.2 评价指标"></a>2.2 评价指标</h5>
      <p>①rank-k/top-k</p>
<p><img src="/images/ReID-base/image-20211225101543620.png" alt="image-20211225101543620"></p>
<p>rank-k计算的是整个探针集Probe中的所有图片的命中概率。</p>
<p>对Probe1，和Probe3，Rank1击中，那么整个探针集的rank1就是2/5=0.4</p>
<p>Probe2 Rank-4击中，与Probe1和Probe3加起来就是3/5=0.6的Rank-5</p>
<p>② CMC曲线</p>
<p>其实就是Rank-k的曲线</p>
<p><img src="/images/ReID-base/image-20211225102201277.png" alt="image-20211225102201277"></p>
<p>③ mAP(mean average precision)</p>
<p>反映检索的人在数据库中所有正确图片排在排序列表前面的成都，更加全面的衡量ReID算法的性能。</p>
<p>以下图Probe1为例，排序列表里面有3张正确的身份图片。分别是Top1、Top4、Top9</p>
<p>ap的计算如下图所示，mAP是对Query集里全部的AP取平均。</p>
<p><img src="/images/ReID-base/image-20211225102415691.png" alt="image-20211225102415691"></p>
<p>④ 评价模式</p>
<p><img src="/images/ReID-base/image-20211225103052466.png" alt="image-20211225103052466"></p>

        <h5 id="2-3-表征学习常用的损失Loss">
          <a href="#2-3-表征学习常用的损失Loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-表征学习常用的损失Loss" class="headerlink" title="2.3 表征学习常用的损失Loss"></a>2.3 表征学习常用的损失Loss</h5>
      
        <h6 id="2-3-1-损失的分类">
          <a href="#2-3-1-损失的分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-1-损失的分类" class="headerlink" title="2.3.1 损失的分类"></a>2.3.1 损失的分类</h6>
      <p><img src="/images/ReID-base/image-20211225143228526.png" alt="image-20211225143228526"></p>
<p><img src="/images/ReID-base/image-20211225143426922.png" alt="image-20211225143426922"></p>

        <h6 id="2-3-2-分类损失">
          <a href="#2-3-2-分类损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-2-分类损失" class="headerlink" title="2.3.2 分类损失"></a>2.3.2 分类损失</h6>
      <p><img src="/images/ReID-base/image-20211225143943301.png" alt="image-20211225143943301"></p>
<p>为什么在测试阶段需要丢弃分类FC层？因为在训练集和测试集中的行人ID并不相同，直接用训练集的FC输出分类ID毫无意义。因此在测试阶段直接使用训练的特征提取层提取的Probe的特征向量与Gallery比对检索。</p>

        <h6 id="2-3-3-属性损失">
          <a href="#2-3-3-属性损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-3-属性损失" class="headerlink" title="2.3.3 属性损失"></a>2.3.3 属性损失</h6>
      <p><img src="/images/ReID-base/image-20211225144337836.png" alt="image-20211225144337836"></p>
<p>由于行人具有一系列的属性，比如头发的颜色，上衣的颜色，裤子颜色，鞋子的种类以及颜色等等。这些每一个属性都可以和ID一样作为一个分类来输出分类结果。因此每个属性都可以通过softmax计算交叉熵损失与ID损失一起组成总的loss进行优化。</p>
<p>当然属性损失作为分类损失的类似，在测试阶段由于ID、属性等等与训练集不同，在测试阶段还是将所有的分类FC全部丢弃掉，指使用特征提取层。</p>

        <h6 id="2-3-4-验证损失">
          <a href="#2-3-4-验证损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-4-验证损失" class="headerlink" title="2.3.4 验证损失"></a>2.3.4 验证损失</h6>
      <p><img src="/images/ReID-base/image-20211225145235952.png" alt="image-20211225145235952"></p>
<p>验证损失一般是训练一个特征提取网络，同时输入两张图片提取特征之后将两个特征进行特征融合(比如直接相减计算特征向量的差异)，融合后的特征输入到后续网络通过特征提取计算作为一个二分类问题输出是/否属于一个ID。用于二分类的损失成为验证损失(如上图右上角的Verification Subnet)。</p>
<p>验证损失往往和ID损失一起使用，共同优化整个网络。如右下角的(Classification Subnet)，两张图片可以分别计算ID Loss。</p>

        <h6 id="2-3-5-表征学习的总结">
          <a href="#2-3-5-表征学习的总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-5-表征学习的总结" class="headerlink" title="2.3.5 表征学习的总结"></a>2.3.5 表征学习的总结</h6>
      <p><img src="/images/ReID-base/image-20211225145942198.png" alt="image-20211225145942198"></p>

        <h5 id="2-4-度量学习">
          <a href="#2-4-度量学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-度量学习" class="headerlink" title="2.4 度量学习"></a>2.4 度量学习</h5>
      <p><img src="/images/ReID-base/image-20211225150221293.png" alt="image-20211225150221293"></p>
<p>使用简单化的描述语言来说就是，度量学习ReID任务主要需要做以下工作</p>
<p>①、训练出一个特征提取网络，对图片提取出特征向量。</p>
<p>②、定义一个距离度量损失函数，计算两张图片特征向量之间的度量距离。</p>
<p>③、计算度量损失函数，最优化度量损失函数使相同行人的图片对之间的距离尽可能小，不同行人的图片对之间的距离尽可能大。通过最优化度量损失函数去优化特征提取网络。</p>
<p><em><strong>深度学习解决ReID问题的目标在于提取更优的特征，更加具有度量属性的特征。</strong></em></p>

        <h6 id="2-4-1-度量学习的流程">
          <a href="#2-4-1-度量学习的流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-1-度量学习的流程" class="headerlink" title="2.4.1 度量学习的流程"></a>2.4.1 度量学习的流程</h6>
      <p>基本也是行人检测–特征提取–训练的流程。</p>
<p><img src="/images/ReID-base/image-20211225151200154.png" alt="image-20211225151200154"></p>

        <h5 id="2-5-度量学习的损失函数">
          <a href="#2-5-度量学习的损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-度量学习的损失函数" class="headerlink" title="2.5 度量学习的损失函数"></a>2.5 度量学习的损失函数</h5>
      
        <h6 id="2-5-1-对比损失Contrastive-loss">
          <a href="#2-5-1-对比损失Contrastive-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-1-对比损失Contrastive-loss" class="headerlink" title="2.5.1 对比损失Contrastive loss"></a>2.5.1 对比损失Contrastive loss</h6>
      <p><img src="/images/ReID-base/image-20211227094927928.png" alt="image-20211227094927928"></p>
<p>对比损失的损失函数如上图<em>Lc</em>所示。</p>
<p>当输入的一对图片是正样本对a,b时，因为正样本y=1，<em>Lc</em>的后半部分为0，优化目标就是<em>Lc</em>的前半部分，也就是a和b特征向量的距离。通过优化器最小化损失函数使正样本对间的距离趋于0。</p>
<p>当输入的一对图片是负样本对a,b时，因为负样本y=0，<em>Lc</em>的前半部分为0，优化目标就是<em>Lc</em>的后半部分。由于<img src="/images/ReID-base/image-20211227102409369.png" alt="image-20211227102409369" style="zoom:40%;"></p>
<p>z = α-距离。当二者之间的距离大于α时, z小于0，max(z, 0) = 0，无需优化。当二者之间的距离越小于α时, z大于0， <em>Lc</em>越大，优化器将优化 <em>Lc</em>使其向0的方向优化，即使二者之间的距离向大于α的方向优化。也就实现了推开负样本的期望功能。其中α是自己设置的一个超参数，作为负样本分开的阈值。</p>

        <h6 id="2-5-2-三元组损失Triplet-loss">
          <a href="#2-5-2-三元组损失Triplet-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-2-三元组损失Triplet-loss" class="headerlink" title="2.5.2 三元组损失Triplet loss"></a>2.5.2 三元组损失Triplet loss</h6>
      <p><img src="/images/ReID-base/image-20211227104924310.png" alt="image-20211227104924310"></p>
<p>三元组损失主要将样本分为Anchor、Positive、Negative。一个三元组的构成方式是从训练集中随机选取一个样本，该样本称为Anchor，记为x_a， 然后再随机抽取一个与Anchor属于同一类的样本x_p和一个与Anchor属于不同类的样本x_n。由此构成一个(Anchor、Positive、Negative)三元组。Triplet Loss的目的在于通过学习，让x_a和x_p特征表达之间的距离尽可能小，而x_a和x_n之间的距离尽可能大。并且要让_a和x_n之间的距离和x_a和x_p之间的距离之间存在一个最小间隔α。即目标函数的优化方向为：</p>
<p><img src="/images/ReID-base/image-20211227105706609.png" alt="image-20211227105706609"></p>
<p>因此损失函数的形式为</p>
<p><img src="/images/ReID-base/image-20211227110756868.png" alt="image-20211227110756868"></p>
<p><img src="/images/ReID-base/image-20211227111029204.png" alt="image-20211227111029204"></p>

        <h6 id="2-5-3-改进的三元组损失">
          <a href="#2-5-3-改进的三元组损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-3-改进的三元组损失" class="headerlink" title="2.5.3 改进的三元组损失"></a>2.5.3 改进的三元组损失</h6>
      <p><img src="/images/ReID-base/image-20211227111131013.png" alt="image-20211227111131013"></p>
<p>其实就在原三元组损失的基础上加上了一项x_a和x_p之间的距离。使得在满足三元组损失优化目标的同时使x_a和x_p之间的距离尽可能的小。</p>

        <h6 id="2-5-4-四元组损失-Quadruplet-loss">
          <a href="#2-5-4-四元组损失-Quadruplet-loss" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-4-四元组损失-Quadruplet-loss" class="headerlink" title="2.5.4 四元组损失 Quadruplet loss"></a>2.5.4 四元组损失 Quadruplet loss</h6>
      <p><img src="/images/ReID-base/image-20211227111507714.png" alt="image-20211227111507714"></p>
<p>四元组损失也是基于三元组损失的基础上，但是需要四张图片组成一个四元组。一个四元组的构成为从训练集中随机选取一个样本，该样本称为Anchor，记为x_a， 然后再随机抽取一个与Anchor属于同一类的样本x_p和<strong>两个</strong>与Anchor属于不同类的样本x_n1和x_n2。并且这两个负样本n1和n2<strong>分别属于两个不同的ID</strong>。</p>
<p>四元组损失的公式如上图所示。第一项使正常的三元组损失。而第二项分别计算Anchor和正样本的距离da,p和两个负样本的距离dn1,n2。使得正样本之间的距离不仅小于正、负样本对的距离，也小于两张来自不同类负样本之间的距离。</p>
<p>功能：</p>
<p>进一步缩小正样本之间的距离，在推开正负样本对的距离的同时推开不同类的负样本对之间的距离。</p>

        <h6 id="2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失">
          <a href="#2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-5-TriHard-Loss-Batch-Hard-Loss-批难三元组损失" class="headerlink" title="2.5.5 TriHard Loss/Batch-Hard Loss 批难三元组损失"></a>2.5.5 TriHard Loss/Batch-Hard Loss 批难三元组损失</h6>
      <p><img src="/images/ReID-base/image-20211227112344030.png" alt="image-20211227112344030"></p>
<p><img src="/images/ReID-base/image-20211227134125333.png" alt="image-20211227134125333"></p>
<p><img src="/images/ReID-base/image-20211227142426071.png" alt="image-20211227142426071"></p>
<p>如图所示，TriHard Loss实现的方式是构建这样一个距离矩阵。如上图，每个batch选取N个ID的行人，每个ID选取3张图片。将这个batch内的全部图片两两计算距离填入上方的矩阵。对角线上表示同一张图片自己到自己的距离，因此为0。红色矩阵块表示同一个ID的三张图片之间的距离，绿色矩阵块表示不同ID的图片之间的距离。衡量是否Hrad然是通过距离来衡量。<strong>对于红色矩阵，也就是正样本对来说，红色矩阵的每一行的最大值，以第一行为例，这一行代表行人1-1这张图片与三个正样本的距离，对于正样本来说，正样本之间的距离越大，说明越Hard。</strong>因此将红色矩阵组合起来求每一行的最大值组成一列，这一列就代表了这个batch里面的每一张图片与其对应的最Hard样本的距离。</p>
<p><strong>同理，对正负样本对来说，距离越小越Hard</strong>。将绿色矩阵拼在一起组成方阵，求每一行的最小值。每一行的最小值组成的一列Tensor就是正负样本对的最难距离。</p>
<p><img src="/images/ReID-base/image-20211227145701227.png" alt="image-20211227145701227"></p>
<p>根据上述公式，累加之后除以批量大小就得到了TriHard loss。</p>

        <h6 id="2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组">
          <a href="#2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-6-Triplet-loss-with-adaptive-weights-自适应权重三元组" class="headerlink" title="2.5.6 Triplet loss with adaptive weights 自适应权重三元组"></a>2.5.6 Triplet loss with adaptive weights 自适应权重三元组</h6>
      <p>批难三元组由于只考虑了极端样本的信息，有些信息没有考虑到。</p>
<p>有文章指出批难三元组损失在某些极端情况下，比如数据集有一定数量的标注错误可能会因为训练时梯度特别大而导致网络崩溃。当网络使用批难三元组难以收敛的情况下可以考虑自适应权重三元组。</p>
<p><img src="/images/ReID-base/image-20211227150111074.png" alt="image-20211227150111074"></p>

        <h6 id="2-5-7-度量学习的总结">
          <a href="#2-5-7-度量学习的总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-7-度量学习的总结" class="headerlink" title="2.5.7 度量学习的总结"></a>2.5.7 度量学习的总结</h6>
      <p><img src="/images/ReID-base/image-20211227150524190.png" alt="image-20211227150524190"></p>
<p>度量学习和表征学习往往可以用来共同训练一个模型，将多个损失组合在一起共同优化一个模型。这种做法也是业界精确度比较高的一种做法。</p>
<p><img src="/images/ReID-base/image-20211227151111989.png" alt="image-20211227151111989"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/24/C-Chapter2/">C++Primer 第二章学习笔记</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-30</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="C-Primer第二章–变量和基本类型">
          <a href="#C-Primer第二章–变量和基本类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#C-Primer第二章–变量和基本类型" class="headerlink" title="C++ Primer第二章–变量和基本类型"></a>C++ Primer第二章–变量和基本类型</h3>
      <p><em><strong>数据类型是程序的基础，它告诉了我们数据的意义以及我们能在数据上执行的操作。</strong></em></p>

        <h4 id="一、基本内置类型">
          <a href="#一、基本内置类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、基本内置类型" class="headerlink" title="一、基本内置类型"></a>一、基本内置类型</h4>
      <p>基本内置类型主要包括<strong>算术类型</strong>和<strong>空类型</strong></p>

        <h5 id="1-1-算术类型">
          <a href="#1-1-算术类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-算术类型" class="headerlink" title="1.1 算术类型"></a>1.1 算术类型</h5>
      <p>算术类型主要分为<strong>整型</strong>（integral type），和<strong>浮点型</strong>。其中<strong>字符类型</strong>与<strong>布尔类型</strong>均包括在整型之内。</p>
<p><img src="/images/C++Chapter2/image-20211224201207827.png" alt="image-20211224201207827"></p>
<p><strong>注：由于float类型精度有限，实际应用中常用的是double，并且二者在计算代价上相差无几。</strong></p>

        <h5 id="1-2-带符号类型与不带符号类型">
          <a href="#1-2-带符号类型与不带符号类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-带符号类型与不带符号类型" class="headerlink" title="1.2 带符号类型与不带符号类型"></a>1.2 带符号类型与不带符号类型</h5>
      <p>除去bool和扩展的字符类型之外，其它的整型可以划分为带符号和无符号的。带符号的可以表示正数、负数和0，无符号类型只能表示大于等于0的值。</p>
<p>C++中需要无符号的值的时候，在前面加上<strong>unsinged</strong>即可，如int-&gt;unsigned int。</p>

        <h5 id="1-3-字面值常量">
          <a href="#1-3-字面值常量" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-3-字面值常量" class="headerlink" title="1.3 字面值常量"></a>1.3 字面值常量</h5>
      <p>字面值常量(literal)是指一类仅从字面上理解就能看出值的常量。每个字面值都对应一种数据类型。具体来说，字面值常量分为<strong>整型</strong>和<strong>浮点型</strong>字面值。</p>
<p><strong>整型字面值常量</strong></p>
<p>整型字面值可以写成十进制、八进制或者十六进制。以<strong>0开头的数表示八进制数，以0x开头的代表16进制数。</strong></p>
<p>如：</p>
<p><img src="/images/C++Chapter2/image-20211225195058728.png" alt="image-20211225195058728"></p>
<p><strong>浮点型字面值</strong></p>
<p>表现为一个小鼠或科学计数法表示的指数，指数部分用E或者e标识，但该指数实际上以10为底</p>
<p><img src="/images/C++Chapter2/image-20211225200452808.png" alt="image-20211225200452808"></p>
<p><strong>字符型字面值</strong></p>
<p>表现为单引号括起来的字符或者双引号括起来的字符串。</p>
<p>一个细节是字符串字面值实际上是由常量字符构成的数组，编译器在每个字符串的结尾处会添加一个空字符（’\0’），因此字符串的字面值的实际长度会比字符串的实际内容多1。</p>
<p>如：</p>
<p>‘A’ 表示一个字符型字面值，长度为1</p>
<p>“A”表示一个字符串型字面值，长度为2</p>
<p>可以通过添加前缀和后缀来改变字面值的默认类型</p>
<p><img src="/images/C++Chapter2/image-20211225204253158.png" alt="image-20211225204253158"></p>
<p><img src="/images/C++Chapter2/image-20211225204325836.png" alt="image-20211225204325836"></p>

        <h4 id="二、变量">
          <a href="#二、变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、变量" class="headerlink" title="二、变量"></a>二、变量</h4>
      <p><em><strong>变量实际上提供一个具名的、可供程序操作的存储空间。</strong></em>数据类型决定变量所占据的空间的大小和布局方式、以及该空间能存储的值的范围、变量能参与的运算。</p>
<p><strong>对象是指一块能存储数据并且具有某种类型的内存空间。</strong></p>

        <h5 id="2-1-变量的初始化">
          <a href="#2-1-变量的初始化" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-变量的初始化" class="headerlink" title="2.1 变量的初始化"></a>2.1 变量的初始化</h5>
      
        <h6 id="2-1-1-初始值">
          <a href="#2-1-1-初始值" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-1-初始值" class="headerlink" title="2.1.1 初始值"></a>2.1.1 初始值</h6>
      <p>变量在创建的时候获得了一个特定的值，我们说这个对象被初始化了。</p>
<p><strong>初始化不是赋值，初始化是在创建变量的时候赋予变量一个初始值。而赋值的定义是把对象的当前值擦除掉，用一个新的值替代。</strong></p>

        <h6 id="2-1-2-列表初始化">
          <a href="#2-1-2-列表初始化" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-2-列表初始化" class="headerlink" title="2.1.2 列表初始化"></a>2.1.2 列表初始化</h6>
      <p>使用一个大括号{}来对变量进行初始化的方式叫做列表初始化。主要有以下第二、第三种方式：</p>
<p><img src="/images/C++Chapter2/image-20211225205300870.png" alt="image-20211225205300870"></p>
<p>这种初始化的好处在于：<strong>如果我们使用列表初始化且初始值存在丢失信息的风险，编译器会报错且不予执行。在一定程度上避免了变量的类型与初始值类型不匹配类型强转导致的数值丢失</strong></p>
<p><img src="/images/C++Chapter2/image-20211225205559552.png" alt="image-20211225205559552"></p>
<p><img src="/images/C++Chapter2/image-20211225205729977.png" alt="image-20211225205729977"></p>

        <h5 id="2-2-变量声明和定义">
          <a href="#2-2-变量声明和定义" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-变量声明和定义" class="headerlink" title="2.2 变量声明和定义"></a>2.2 变量声明和定义</h5>
      <p>C++支持分离式编译，程序可以拆分成多个文件，一个文件可以使用其它文件中定义的函数和变量。但是使用其他文件中的变量需要在程序中声明。声明和定义是不同的。定义不仅会规定变量的类型与名字，而且会额外申请存储空间。而声明则不需要申请存储空间。声明需要在变量名前添加<strong>关键字extern</strong>，而且<strong>不能对声明的变量做显式初始化。</strong></p>
<p><strong>extern语句如果包含了初始值就不再是声明，而是定义了。</strong></p>
<p><img src="/images/C++Chapter2/image-20211225215525019.png" alt="image-20211225215525019"></p>
<p><strong>一个变量只能定义一次，但可以多次声明。</strong></p>

        <h5 id="2-3-标识符">
          <a href="#2-3-标识符" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-标识符" class="headerlink" title="2.3 标识符"></a>2.3 标识符</h5>
      <p>C++中的标识符必须以字母或者下划线开头。定义在函数体外的标识符不能以下划线开头。</p>
<p><img src="/images/C++Chapter2/image-20211225215816509.png" alt="image-20211225215816509"></p>

        <h4 id="三、复合类型">
          <a href="#三、复合类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、复合类型" class="headerlink" title="三、复合类型"></a>三、复合类型</h4>
      
        <h5 id="3-1-引用">
          <a href="#3-1-引用" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-引用" class="headerlink" title="3.1 引用"></a>3.1 引用</h5>
      <p>引用类型通过将声明符号写成&amp;d的形式来定义，d是被引用的变量名。但是引用并非创建了一个新对象，它实际上是一个已经定义的对象的<strong>别名</strong>。</p>
<p><img src="/images/C++Chapter2/image-20211225220329035.png" alt="image-20211225220329035"></p>
<p>程序是将引用与它的初始值<strong>绑定</strong>在一起，而不是直接把初始值拷贝过去。因此<strong>引用必须初始化。</strong>并且一旦初始化完成，无法令引用重新绑定到另外一个对象。</p>
<p>定义了引用后，对引用对象所做的所有操作都是在被绑定的原对象上直接进行操作的。</p>
<p>引用只能使用对象对其初始化，而且定义引用的类型要与被绑定的对象的类型完全一致。</p>
<p><img src="/images/C++Chapter2/image-20211225220759531.png" alt="image-20211225220759531"></p>

        <h5 id="3-2-指针">
          <a href="#3-2-指针" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2-指针" class="headerlink" title="3.2 指针"></a>3.2 指针</h5>
      <p>指针也是”指向”另外一种类型的对象的复合类型。它和引用都可以实现对其它对象的间接访问。但是它和指针有很多不同点，具体如下：</p>
<p><strong>①、指针本身就是一个对象，而引用只是一个”别名”，并非创建了一个新对象。</strong></p>
<p><strong>②、指针允许赋值和拷贝，而对引用实际上只是给被引用的对象赋值。</strong></p>
<p><strong>③、指针在生命周期之内可以先后指向不同的对象，而引用只能绑定在一个对象上，不允许改变。</strong></p>
<p><strong>④、指针不需要在定义的时候赋初值</strong></p>
<p><strong>定义方法</strong></p>
<p><img src="/images/C++Chapter2/image-20211225222534490.png" alt="image-20211225222534490"></p>
<p><strong>指针对象实际上存放的是某个对象的地址，想要获取这个地址需要使用取地址符&amp;</strong></p>
<p><img src="/images/C++Chapter2/image-20211225222656042.png" alt="image-20211225222656042"></p>
<p>p是一个指针变量，指向变量ival，并且p实际上存放的是变量ival的地址。</p>
<p><img src="/images/C++Chapter2/image-20211225222930490.png" alt="image-20211225222930490"></p>
<p><img src="/images/C++Chapter2/image-20211225222957674.png" alt="image-20211225222957674"></p>
<p><strong>利用指针访问对象</strong></p>
<p>如果指针指向了一个对象，允许使用解引用符(操作符*)来访问该对象。对指针解引用会得到所指的对象。</p>
<p><img src="/images/C++Chapter2/image-20211226133703807.png" alt="image-20211226133703807"></p>
<p><strong>空指针</strong></p>
<p>当暂时不知道指针应该指向何处的时候，应当先对指针初始化为空，空指针的初始化操作是：int *p = <strong>nullptr</strong>;。nullptr是一种特殊的字面值，它可以转化成任意的指针类型。</p>
<p><strong>指向指针的指针</strong></p>
<p>*的个数可以区分指针的级别，因此**表示指向指针对象的指针。</p>
<p><img src="/images/C++Chapter2/image-20211226135121453.png" alt="image-20211226135121453"></p>
<p>此时要通过ppi来访问原对象，需要两次解引用，即**ppi</p>

        <h5 id="3-3-const限定符">
          <a href="#3-3-const限定符" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-const限定符" class="headerlink" title="3.3 const限定符"></a>3.3 const限定符</h5>
      <p>使用const限定符修饰的变量是一种不可改变值的变量，也可称为const常量。任何对const常量赋值的操作都将报错。因为const常量一经定义就不可改变，因此const对象必须初始化。</p>
<p>如：<code>const int bufSize = 512;</code></p>
<p>默认状态下，const限定符仅对文件内生效，如果需要跨文件使用，需要对const常量的定义前加上extern</p>
<p><code>extern const int bufSize = 512;</code></p>
<p>当在当在其他文件中需要使用该变量的时候，也需要在文件中使用extern进行声明。</p>
<p><code>extern const int bufSize;</code></p>

        <h6 id="3-3-1-const的引用">
          <a href="#3-3-1-const的引用" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-1-const的引用" class="headerlink" title="3.3.1 const的引用"></a>3.3.1 const的引用</h6>
      <p>和普通对象一样，我们也可以把引用绑定到const对象上，称之为<strong>对常量的引用</strong>。但是由于被绑定的对象是一个常量，因此<strong>不可以改变被引用对象的值。</strong></p>
<p><code>const int c1 = 1024; </code></p>
<p><code>const int &amp;r1 = c1;</code></p>
<p>与一般的引用不同的是，常量引用的类型并不一定需要与其所引用的对象的类型完全一致。允许常量引用绑定一个非常量对象、字面值、甚至是一个表达式。<strong>因为编译器会自动创建一个与常量引用类型相同的临时常量，这个并将常量引用绑定的非常量对象、字面值、甚至是一个表达式的值拷贝给这个临时常量。最后将这个常量引用绑定到这个临时常量。</strong></p>
<p><img src="/images/C++Chapter2/image-20211226152752659.png" alt="image-20211226152752659"></p>
<p>常量引用仅仅对引用的操作做出了限定，但是对被引用绑定的对象是否是常量并未作规定。即不能通过常量引用这个别名来改变这个被绑定对象的值，常量引用也不能改变绑定的对象。但是因为被绑定对象并不是一个常量，可以通过其他方式改变这个值。</p>

        <h6 id="3-3-2-指向常量的指针">
          <a href="#3-3-2-指向常量的指针" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-2-指向常量的指针" class="headerlink" title="3.3.2 指向常量的指针"></a>3.3.2 指向常量的指针</h6>
      <p>const与指针结合可以结合成常量指针(const pointer)与指向常量的指针(pointer to const)。</p>
<p><strong>指向常量的指针</strong>，<strong>不能用于改变其所指对象的值。</strong>因为其指向的对象是一个常量。由之前指针的描述，指针是一个对象，用于存放所指对象的地址。指向常量的指针就存放的是常量的地址。</p>
<p>定义方式：</p>
<figure class="highlight c++"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> 指针的数据类型 *指针名 = &amp;指向的对象地址;</span><br></pre></td></tr></table></div></figure>

<p><img src="/images/C++Chapter2/image-20211227163615973.png" alt="image-20211227163615973"></p>
<p><strong>指向常量的指针却允许指向一个非常量。</strong></p>
<p><img src="/images/C++Chapter2/image-20211227163822295.png" alt="image-20211227163822295"></p>
<p><strong>不管是常量的引用还是指向常量的指针，都只是禁止了通过这个引用或者指针去修改被指向的对象。</strong>当它们指向一个非常量时，并不禁止通过其它手段去修改这个非常量的值。</p>

        <h6 id="3-3-3-const指针-常量指针">
          <a href="#3-3-3-const指针-常量指针" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-3-const指针-常量指针" class="headerlink" title="3.3.3 const指针/常量指针"></a>3.3.3 const指针/常量指针</h6>
      <p>引用并不是一个对象，而指针是一个对象，因此指针本身允许被定义成一个常量。既然<strong>常量指针本身是一个常量</strong>，因此<strong>它就必须被初始化。而且它本身的值(即被常量指针指向的对象的地址)不允许改变。</strong></p>
<p>定义方式：</p>
<p>注意 const指针对象本身的类型是const，因此定义const指针应当把*号写在const前。</p>
<figure class="highlight c++"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">被指向对象的类型 *<span class="keyword">const</span> 指针名 = &amp;被指向对象名;</span><br></pre></td></tr></table></div></figure>

<p><img src="/images/C++Chapter2/image-20211227165429436.png" alt="image-20211227165429436"></p>
<p>注意：</p>
<p><strong>常量指针指的是指针本身是一个常量。也就是说这个指针保存的地址，也就是指向哪个对象是不允许改变的，但是这个地址指向的对象的值是可以改变的。</strong>可以通过常量指针修改被指向对象的值。</p>
<p><img src="/images/C++Chapter2/image-20211227165809504.png" alt="image-20211227165809504"></p>

        <h6 id="3-3-4-constexpr-变量">
          <a href="#3-3-4-constexpr-变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-4-constexpr-变量" class="headerlink" title="3.3.4 constexpr 变量"></a>3.3.4 constexpr 变量</h6>
      <p>常量表达式是是一种特殊的表达式，它的计算过程发生在编译阶段，并且它的计算结果不会改变。因此对于一些确定不会改变的计算结果，与其将它的参数声明成一个普通const常量再计算。不如将该表达式声明成constexpr 类型，将计算阶段放在编译。加速代码执行。</p>
<p>一个对象是否是一个常量表达式，由它的数据类型和参与计算结果的初始值共同决定。要求：计算结果的数据类型应当是常量类型，并且参与运算的都是字面值，也就是不需要计算阶段就可以得出的值</p>
<figure class="highlight c++"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> max_files = <span class="number">20</span>;   <span class="comment">//max_files是一个常量表达式</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> limit = max_files + <span class="number">1</span>; <span class="comment">//计算结果的数据类型是const，并且参与运算的max_files是一个确定的常量，故limit是常量表达式</span></span><br><span class="line"><span class="keyword">int</span> staff_size = <span class="number">27</span>;   <span class="comment">//数据类型不是常量，不是常量表达式</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> sz = <span class="built_in">get_size</span>();  <span class="comment">//虽然数据类型声明成const，但是它的值需要在执行阶段计算，故不是常量表达式</span></span><br><span class="line">   </span><br></pre></td></tr></table></div></figure>

<p>当都使用const来声明常量表达式，编译器很难确定变量的值是否是一个常量表达式。也就无法将计算阶段放在编译，加速代码执行。因此C++11引入了constexpr类型</p>
<p><img src="/2021/12/24/C-Chapter2/images/C++Chapter2/image-20211230211752515.png" alt="image-20211230211752515"></p>
<p>需要指出的是，如果constexpr声明中定义了一个指针，限定符constexpr仅对指针生效。也就是说仅仅指针被声明成了常量类型，其保存的地址不变，与它所指向的对象的值无关。</p>

        <h4 id="四、处理类型">
          <a href="#四、处理类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#四、处理类型" class="headerlink" title="四、处理类型"></a>四、处理类型</h4>
      
        <h5 id="4-1-类型别名">
          <a href="#4-1-类型别名" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-类型别名" class="headerlink" title="4.1 类型别名"></a>4.1 类型别名</h5>
      <p>类型别名就是给某种数据类型起一个同义词。它可以使复杂的类型名变得简单明了，便于理解和使用。</p>
<p>两种方式：</p>
<figure class="highlight c++"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">double</span> wages; <span class="comment">//wages是double的同义词</span></span><br><span class="line"><span class="keyword">typedef</span> wages base, *p;  <span class="comment">//等价于double base;和double *p</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> SI = Sales_item;</span><br><span class="line">SI item;  <span class="comment">//等价于Sales_item item;</span></span><br></pre></td></tr></table></div></figure>

</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/23/object-detect/">物体识别</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-23</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-24</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="一、物体识别相关术语解释">
          <a href="#一、物体识别相关术语解释" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、物体识别相关术语解释" class="headerlink" title="一、物体识别相关术语解释"></a>一、物体识别相关术语解释</h3>
      
        <h4 id="1-1-锚框、边缘框">
          <a href="#1-1-锚框、边缘框" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-锚框、边缘框" class="headerlink" title="1.1 锚框、边缘框"></a>1.1 锚框、边缘框</h4>
      <p><em>边界框</em>（bounding box）来描述对象的空间位置，一般描述的是物体的真实所在位置。 边界框是矩形的，由矩形左上角的以及右下角的x和y坐标决定。目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的<em>真实边界框</em>（ground-truth bounding box）。锚框（anchor box）一般是预测算法预先提出的一组框，它是以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框。算法检测预设置的锚框内是否有关注的物体，如果有，预测锚框到真实边缘框的偏移，从而对锚框进行调整。</p>

        <h5 id="1-1-1-锚框标注">
          <a href="#1-1-1-锚框标注" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-1-锚框标注" class="headerlink" title="1.1.1 锚框标注"></a>1.1.1 锚框标注</h5>
      <p>对于一张图片来进行目标检测，我们可能会生成大量的锚框。对每个锚框来说都是一个训练样本。我们需要将每个锚框要么标注成背景，要么关联上一个真实的边缘框。因此我们需要对锚框进行标注，常用的做法是计算每个锚框与每个边缘框的IoU，分别取最大值将锚框标注成边缘框相同的标注。</p>

        <h5 id="1-2-NMS非极大值抑制输出">
          <a href="#1-2-NMS非极大值抑制输出" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-NMS非极大值抑制输出" class="headerlink" title="1.2 NMS非极大值抑制输出"></a>1.2 NMS非极大值抑制输出</h5>
      <p>因为每个锚框都预测一个边缘框，因此可能出现很多锚框预测一个相同的边缘框，如图所示</p>
<p><img src="/2021/12/23/object-detect/images/object-detect/image-20211223100019550.png" alt="image-20211223100019550"></p>
<p>这些框之间的区别仅仅在于覆盖的范围不同。使用NMS可以合并相似的预测，主要的流程是先选中非背景类的最大预测值，然后去掉所有其它和这个最大预测锚框的IoU值大于Theta值(一个自己设置的阈值)的预测，重复以上流程。</p>

        <h4 id="1-2-比较锚框与真实框的相似度指标–IOU">
          <a href="#1-2-比较锚框与真实框的相似度指标–IOU" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-比较锚框与真实框的相似度指标–IOU" class="headerlink" title="1.2 比较锚框与真实框的相似度指标–IOU"></a>1.2 比较锚框与真实框的相似度指标–IOU</h4>
      <p>IoU往往用来计算两个框之间的相似度，计算公式是给定两个集合A和B，用它的交集闭上并集。</p>
<p><img src="/2021/12/23/object-detect/images/object-detect/image-20211223094115028.png" alt="image-20211223094115028"></p>
<p><img src="/2021/12/23/object-detect/images/object-detect/image-20211223094125414.png" alt="image-20211223094125414"></p>

        <h3 id="二、基于锚框的检测算法的流程">
          <a href="#二、基于锚框的检测算法的流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、基于锚框的检测算法的流程" class="headerlink" title="二、基于锚框的检测算法的流程"></a>二、基于锚框的检测算法的流程</h3>
      <h4 id><a href="#" class="headerlink" title></a><img src="/2021/12/23/object-detect/images/object-detect/image-20211223100414222.png" alt="image-20211223100414222"></h4></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/12/17/ResNet/">传统网络实现之ResNet</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-17</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-17</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="经典网络架构实现之ResNet">
          <a href="#经典网络架构实现之ResNet" class="heading-link"><i class="fas fa-link"></i></a><a href="#经典网络架构实现之ResNet" class="headerlink" title="经典网络架构实现之ResNet"></a>经典网络架构实现之ResNet</h3>
      
        <h4 id="ResNet简介">
          <a href="#ResNet简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#ResNet简介" class="headerlink" title="ResNet简介"></a>ResNet简介</h4>
      <p>从AlexNet再到VGG，现代卷积神经网络发展的一个重点就是深度<em><strong>Depth</strong></em>。正如AlexNet的论文<em><strong>Imagenet-Classification-with-Deep-Convolutional-Neural-Networks</strong></em>的discussion部分所述：<em>So the depth really is important for achieving our results。</em>从常规的思路上来想，更深的网络不应该比较浅的网络有着更差的性能表现。神经网络模型可以视作一个函数F，假设一个已经训练好的较浅层的模型为f(x)，在这个较浅层的网络模型的基础上再加上一些层数，只要优化器将网络将后续的层数的参数设成0或者接近0，即将此层训练成一个恒等映射，直接输出上层的结果，即实现一个嵌套式的网络，那么深层网络所达到的效果起码不会比原浅层模型差，只会更加接近最优。</p>
<p><img src="/images/ResNet/tu1.png" alt="image-20211217151707369"></p>
<p>​                                                                                            图1 嵌套网络与普通的深层网络的区别</p>
<p>随着网络深度的不断增加，一系列的问题随之而来。其中影响深度难以继续叠加的一个非常重要的问题就是网络的层数越深，训练的难度也更大。具体则表现在梯度消失等问题。ResNet提出的Residual块在一定程度上缓解了这个问题。具体见下文。</p>

        <h4 id="一、ResNet的基本架构–Residual块">
          <a href="#一、ResNet的基本架构–Residual块" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、ResNet的基本架构–Residual块" class="headerlink" title="一、ResNet的基本架构–Residual块"></a>一、ResNet的基本架构–Residual块</h4>
      
        <h5 id="1-1残差块的结构">
          <a href="#1-1残差块的结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1残差块的结构" class="headerlink" title="1.1残差块的结构"></a>1.1残差块的结构</h5>
      <p>上文说到，训练出一个嵌套式的网络，那么深层的模型要比浅层的模型理论上应该有着更好的表现。将新添加的层训练出一个恒等映射（identity mapping），使f(x)=x，新模型和原模型将同样有效。根据这种思路，ResNet提出了残差块（Residual block）的架构。残差块是ResNet的基本组成单位，它的具体结构如下图。</p>
<p><img src="/images/ResNet/tu2.png" alt="image-20211217154218707"></p>
<p>​                                                                图2 一个普通的卷积神经网络块（左）和残差块（右）</p>
<p>假设原神经网络块的函数表示为<strong>f(x)<strong>，即待优化的函数为</strong>f(x)<strong>。而残差块的函数表示为</strong>h(x)=f(x)-x</strong>，二者向下层传递的计算结果都是f(x)，但残差块引入了“shortcut connections”，将网络块的输入通过这个连接直接传输到网络的输出处，与函数块的输出h(x)在通道维度做加和之后作为残差块的总输出。即神经网络需要学习并优化的是函数<strong>h(x)=f(x)-x</strong>。这样的结构设计往往带来优化上的便利。假设之前提到的恒等映射作为我们希望学出的理想映射f(x)，我们只需将上图中右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。 实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。在残差块中，输入可通过跨层数据线路更快地向前传播。</p>

        <h5 id="1-2-残差连接好优化的一种数学上的解释">
          <a href="#1-2-残差连接好优化的一种数学上的解释" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-残差连接好优化的一种数学上的解释" class="headerlink" title="1.2 残差连接好优化的一种数学上的解释"></a>1.2 残差连接好优化的一种数学上的解释</h5>
      <p>下面给出一种残差块优化便利的数学解释。众所周知层次很深的网络容易出现梯度消失的问题。由上文所述神经网络的每一个块都可以看成一个函数f(x)，假设一个两块的网络可以表示成f(g(x))。在反向传播算法里面，f(g(x))对x计算梯度时由链式法则可知需要计算每一层的导数再累乘如图3所示。如果某几层梯度特别小，累乘之下梯度很快就成为一个很小的接近0的数，也就出现了梯度消失问题。梯度消失问题是深层次网络难以训练与优化的一个原因。</p>
<p><img src="/images/ResNet/tu3.png" alt="image-20211217162043566"></p>
<p>​                                                                                图3 普通深层网络计算梯度</p>
<p>让我们来看下假如把深层网络的基本架构换成残差块会出现什么。由残差块的基本架构可以知道残差块需要优化的函数是h(x)，传递给下层的输出为h(x)+x。还是按照同样的假设来对残差块进行表示，不妨设第一块的输出为g(x)，则第二层的输出为f(g(x))+g(x)，对其求导如图4所示</p>
<p><img src="/images/ResNet/tu4.png" alt="image-20211217185939784"></p>
<p>​                                                                                                图4 残差网络计算梯度</p>
<p>可见当一个很深的深度残差网络，方框内的累乘部分可能因为累乘变得很小，但后面的加法部分可以有力的避免总的梯度变成很小的数从而导致网络难以训练优化。</p>

        <h4 id="二、残差网络的结构">
          <a href="#二、残差网络的结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、残差网络的结构" class="headerlink" title="二、残差网络的结构"></a>二、残差网络的结构</h4>
      <p>残差网络根据其网络层数不同有许多的变体，其中论文中给出了五种结构，层数分别为18、34、50、101、152，网络结构大致如下图5所示。</p>
<p><img src="/images/ResNet/tu5.png" alt="image-20211217195609739"></p>
<p>​                                                                                        图5 不同版本的ResNet的网络架构</p>
<p>通过图示可见18和34层的ResNet和较深层次的ResNet-50，ResNet-101，ResNet-152采用了不同的Residual Block设计，分别介绍这两种不同结构的Residual Block。</p>

        <h5 id="2-1-ResNet-18与-ResNet-34的Residual-Block">
          <a href="#2-1-ResNet-18与-ResNet-34的Residual-Block" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-ResNet-18与-ResNet-34的Residual-Block" class="headerlink" title="2.1 ResNet-18与 ResNet-34的Residual Block"></a>2.1 ResNet-18与 ResNet-34的Residual Block</h5>
      <p>ResNet-18与ResNet-34的Residual Block结构采用图6所示的结构。 残差块里首先有2个有相同输出通道数的3×3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。 因此残差连接里的1×1卷积层完成的实际功能就是改变输出通道数。</p>
<p><img src="/images/ResNet/tu6.png" alt="image-20211217200339365"></p>
<p>​                                                        图 6 包含以及不包含1×1卷积层的残差块</p>

        <h5 id="2-2-ResNet-50及以上版本的Residual-Block结构设计">
          <a href="#2-2-ResNet-50及以上版本的Residual-Block结构设计" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-ResNet-50及以上版本的Residual-Block结构设计" class="headerlink" title="2.2 ResNet-50及以上版本的Residual Block结构设计"></a>2.2 ResNet-50及以上版本的Residual Block结构设计</h5>
      <p>在论文中，较深的ResNet的块架构改成了一种叫<em>Bottleneck Architectures</em>的设计。如下图7所示。对于每个块来说将原有的两个3*3卷积层替换成两个1*1卷积层加3×3卷积的架构。其中第一层1×1卷积完成的功能是将输入数据降维到64个通道，后一层的1×1卷积做的是升维操作。对特征先做降维再使用3×3卷积提取特征的好处在于减少了3×3卷积层所需的参数数量并且减少了训练的时间。并且可以减少参数数量同时减少中间特征图的通道数，这样可以使单个Block消耗的显存更少，有利于构建层数更多的网络。正如论文原文所述：考虑到训练时间的限制，因此采用了BottleNeck的结构。</p>
<p><img src="/images/ResNet/tu7.png" alt="image-20211217201953034"></p>
<p>​            图 7 左为ResNet-18，34的设计，右为ResNet-50，101，152的”Bottleneck Architectures“设计</p>

        <h5 id="2-3-Resnet-18与ResNet-50的网络架构设计图示">
          <a href="#2-3-Resnet-18与ResNet-50的网络架构设计图示" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-Resnet-18与ResNet-50的网络架构设计图示" class="headerlink" title="2.3 Resnet-18与ResNet-50的网络架构设计图示"></a>2.3 Resnet-18与ResNet-50的网络架构设计图示</h5>
      <p><img src="/images/ResNet/tu8.jpg" alt="img"></p>

        <h4 id="三、Pytorch的ResNet-50实现FashionMNIST分类">
          <a href="#三、Pytorch的ResNet-50实现FashionMNIST分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、Pytorch的ResNet-50实现FashionMNIST分类" class="headerlink" title="三、Pytorch的ResNet-50实现FashionMNIST分类"></a>三、Pytorch的ResNet-50实现FashionMNIST分类</h4>
      
        <h5 id="3-1相关包的导入">
          <a href="#3-1相关包的导入" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1相关包的导入" class="headerlink" title="3.1相关包的导入"></a>3.1相关包的导入</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="3-2-ResNet-Block的设计">
          <a href="#3-2-ResNet-Block的设计" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2-ResNet-Block的设计" class="headerlink" title="3.2 ResNet Block的设计"></a>3.2 ResNet Block的设计</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_channels, filters, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 用一个Tuple储存BottleNeck层的三个卷积层分别的输出通道数，并用filter1, filter2, filter3取出</span></span><br><span class="line">        <span class="comment"># 每个卷积层后面都跟有一个BN层，再作ReLU, 对于shortcut， 1*1卷积做完之后只需要做BN，与Y累加之后再做ReLU</span></span><br><span class="line">        <span class="comment"># 非残差连接的1*1卷积层（指conv1,conv3）均采用stride=1,padding=0的设计</span></span><br><span class="line">        <span class="comment"># 对于用于改变输出通道数的快速连接上的1*1conv，即conv4，stride使用参数决定，第一个stage为1,后3个stage为2</span></span><br><span class="line">        filter1, filter2, filter3 = filters</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, filter1, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(filter1, filter2, kernel_size=<span class="number">3</span>, stride=strides, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(filter2, filter3, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv4 = nn.Conv2d(input_channels, filter3, kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv4 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(filter1)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(filter2)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(filter3)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = F.relu(self.bn2(self.conv2(Y)))</span><br><span class="line">        Y = self.bn3(self.conv3(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv4:</span><br><span class="line">            X = self.bn3(self.conv4(X))</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">residual_block</span>(<span class="params">input_channels, filters, num_residuals, first_block=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="comment"># 用于实现stage1,2,3,4</span></span><br><span class="line">    <span class="comment"># 每个stage的第一个block均使用1×1conv的快速连接，因此对第一个block需要特殊处理</span></span><br><span class="line">    <span class="comment"># 第一个stage的第一个block使用的1×1conv快速连接的stride为1，其他的stage为2</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> first_block:</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                blk.append(Residual(input_channels, filters, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                blk.append(Residual(filters[<span class="number">2</span>], filters, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                blk.append(Residual(input_channels, filters, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                blk.append(Residual(filters[<span class="number">2</span>], filters, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="3-3-网络结构的设计">
          <a href="#3-3-网络结构的设计" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-网络结构的设计" class="headerlink" title="3.3 网络结构的设计"></a>3.3 网络结构的设计</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">stage1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">stage2 = nn.Sequential(*residual_block(<span class="number">64</span>, (<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>), <span class="number">3</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">stage3 = nn.Sequential(*residual_block(<span class="number">256</span>, (<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>), <span class="number">4</span>))</span><br><span class="line">stage4 = nn.Sequential(*residual_block(<span class="number">512</span>, (<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>), <span class="number">6</span>))</span><br><span class="line">stage5 = nn.Sequential(*residual_block(<span class="number">1024</span>, (<span class="number">512</span>, <span class="number">512</span>, <span class="number">2048</span>), <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    stage1, stage2, stage3, stage4, stage5,</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">2048</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网络参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="3-4-数据加载以及训练的相关代码">
          <a href="#3-4-数据加载以及训练的相关代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-4-数据加载以及训练的相关代码" class="headerlink" title="3.4 数据加载以及训练的相关代码"></a>3.4 数据加载以及训练的相关代码</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据加载部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    train_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">    test_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">    train_iter = data.DataLoader(train_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    test_iter = data.DataLoader(test_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前学习率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cur_lr</span>(<span class="params">optimizer</span>):</span></span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        <span class="keyword">return</span> param_group[<span class="string">&#x27;lr&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 衡量在测试集上的准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">net, test_iter, loss, device</span>):</span></span><br><span class="line">    total, correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            total += y.size(<span class="number">0</span>)</span><br><span class="line">            correct += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    test_acc = <span class="number">100.0</span> * correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;****test*****&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;test_loss:&#123;:.3f&#125; | test_acc:&#123;:6.3f&#125;%&quot;</span>.<span class="built_in">format</span>(l.item(), test_acc))</span><br><span class="line">    net.train()</span><br><span class="line">    <span class="keyword">return</span> test_acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, loss, optimizer, num_epochs, device, num_print, lr_scheduler=<span class="literal">None</span>, test_iter=<span class="literal">None</span></span>):</span></span><br><span class="line">    net.train()</span><br><span class="line">    record_train = <span class="built_in">list</span>()</span><br><span class="line">    record_test = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;epoch:[&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>, num_epochs))</span><br><span class="line">        total, correct, train_loss = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        start = time.time() <span class="comment"># 返回当前时间戳</span></span><br><span class="line">        <span class="comment"># i是train_iter的index，用于计数，计算已经抽取了多少个（x, y）</span></span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            <span class="comment"># 将x, y拷贝到显存进行运算,X,y都是一个batch的数据</span></span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            <span class="comment"># x, y都是tensor,l也是tensor</span></span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># 从tensor中取出相应的数值</span></span><br><span class="line">            train_loss += l.item()</span><br><span class="line">            total += y.size(<span class="number">0</span>)</span><br><span class="line">            correct += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            train_acc = (correct / total) * <span class="number">100.0</span></span><br><span class="line">            <span class="comment"># 每num_print次打印一次loss和train_acc</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % num_print == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;step:[&#123;&#125;/&#123;&#125;], train_loss:&#123;:.3f&#125; | train_acc:&#123;:6.3f&#125; | lr:&#123;:.6f&#125;&quot;</span></span><br><span class="line">                      .<span class="built_in">format</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(train_iter), train_loss / (i + <span class="number">1</span>), train_acc, get_cur_lr(optimizer)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> lr_scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;--cost time:&#123;:.4f&#125;s--&quot;</span>.<span class="built_in">format</span>(time.time() - start))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            record_test.append(test(net, test_iter, loss, device))</span><br><span class="line">        record_train.append(train_acc)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> record_train, record_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_curve</span>(<span class="params">record_train, record_test=<span class="literal">None</span></span>):</span></span><br><span class="line">    plt.style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(record_train)+<span class="number">1</span>), record_train, label=<span class="string">&quot;train_acc&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> record_test <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(record_test) + <span class="number">1</span>), record_test, label=<span class="string">&quot;test_acc&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="number">4</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;learning_curve&quot;</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(record_train) + <span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">101</span>, <span class="number">5</span>))</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">1.0</span></span><br><span class="line">momentum = <span class="number">0.9</span></span><br><span class="line">weight_decay = <span class="number">0.0001</span></span><br><span class="line">num_print = <span class="number">100</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_on_device</span>(<span class="params">net, batch_size, num_epochs, learning_rate, momentum, weight_decay, device</span>):</span></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    train_iter, test_iter = load_dataset(batch_size)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.SGD(</span><br><span class="line">        net.parameters(),</span><br><span class="line">        lr=learning_rate,</span><br><span class="line">        momentum=momentum,</span><br><span class="line">        weight_decay=weight_decay,</span><br><span class="line">        nesterov=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">25</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    record_train, record_test = train(net, train_iter, loss, optimizer, num_epochs</span><br><span class="line">                                      , device, num_print, lr_scheduler, test_iter)</span><br><span class="line">    learning_curve(record_train, record_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_on_device(net, batch_size, num_epochs, learning_rate, momentum, weight_decay, device)</span><br></pre></td></tr></table></div></figure>

<p>`</p>
<p>代码中使用了全局池化nn.AdaptiveAvgPool2d以及动态改变学习率torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)，基本的功能已经了解但是具体的细节没有完全搞清楚，后面会继续了解。</p>

        <h5 id="3-5-实验结果展示">
          <a href="#3-5-实验结果展示" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-5-实验结果展示" class="headerlink" title="3.5 实验结果展示"></a>3.5 实验结果展示</h5>
      <p><img src="/images/ResNet/image-20211217211206565.png" alt="image-20211217211206565"></p>
<p><img src="/images/ResNet/image-20211217211249813.png" alt="image-20211217211249813"></p>
<p>最终达到的测试集准确率为88.13%，对于这个结果我还是不太满意的。根据学习曲线可以看出后期模型出现了过拟合的现象。综合分析个人认为应该还可以达到更好的成绩，对于学习率、权重衰减等相关超参数的设置还是没有什么经验，另外今天逛论坛中看到一篇帖子做的是cifar-10的分类问题，仅仅将图片进行了一些剪裁、均值化等处理就将成绩从88%提升到了95%。而且在ResNet的论文中也提到了对图像进行预处理，感觉问题可能出现在这两个方面，后面有时间的话还会继续做实验探索。</p>

        <h4 id="后记">
          <a href="#后记" class="heading-link"><i class="fas fa-link"></i></a><a href="#后记" class="headerlink" title="后记"></a>后记</h4>
      <p>讲个笑话，10月绝不摆烂。</p>
<p><img src="/images/ResNet/image-20211217211811571.png" alt="image-20211217211811571"></p>
<p><img src="/images/ResNet/image-20211217211842242.png" alt="image-20211217211842242"></p>
<p>摆烂王是谁？哦，是我，那没事了。</p>
<p><img src="/images/ResNet/image-20211217211953650.png" alt="image-20211217211953650"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/11/02/labsever/">实验室服务器探索</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-11-02</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-11</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="前置准备：">
          <a href="#前置准备：" class="heading-link"><i class="fas fa-link"></i></a><a href="#前置准备：" class="headerlink" title="前置准备："></a>前置准备：</h3>
      <p>由于没找到便捷的传文件到服务器的方法，有时需要从自己电脑中粘贴代码到服务器的编译器上运行，可以在登录服务器之前点击显示选项在本地资源上点击剪贴板，这样自己电脑的代码就能复制粘贴到服务器编译器上了。</p>
<p><img src="/images/labsever/image-20211102104735926-16387743342881.png" alt="image-20211102104735926"></p>
<p><img src="/images/labsever/image-20211102104817924.png" alt="image-20211102104817924"></p>
<p>修改终端偏好设置</p>
<p><img src="/images/labsever/image-20211206150245183.png" alt="image-20211206150245183"></p>
<p><img src="/images/labsever/image-20211206150302798.png" alt="image-20211206150302798"></p>

        <h3 id="一、安装Anaconda">
          <a href="#一、安装Anaconda" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、安装Anaconda" class="headerlink" title="一、安装Anaconda"></a>一、安装Anaconda</h3>
      <p>首先进入到具有Anaconda安装包的目录下，记得指令中的dxs是自己的用户名</p>
<p><code>cd /home/dxs/anaconda</code></p>
<p><img src="/images/labsever/image-20211101202849128.png" alt="image-20211101202849128"></p>
<p>输入命令： <code>bash Anaconda3-5.2.0-Linux-x86_64.sh</code></p>
<p>回车开始执行安装程序</p>
<p>出现许可协议后一直按回车到出现是否接受(yes/no)</p>
<p>其中会弹出一个是否添加到环境变量，一定要选yes否则可能会很麻烦。</p>
<p>一直输入yes直到出现最后一步有一个询问你是否安装vscode，emm如果你没有需要就输入no。</p>

        <h3 id="二、安装pycharm">
          <a href="#二、安装pycharm" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、安装pycharm" class="headerlink" title="二、安装pycharm"></a>二、安装pycharm</h3>
      <p>首先进入到具有Anaconda安装包的目录下</p>
<p><code>cd /home/dxs/anaconda</code>, 回车</p>
<p>输入ls查看pycharm文件夹的文件夹名<img src="/images/labsever/image-20211101205032654.png" alt="image-20211101205032654"></p>
<p>linux下的复制快捷键是<strong>ctrl+alt+c</strong></p>
<p>粘贴是<strong>ctrl+alt+v</strong></p>
<p>使用复制快捷键复制这个长文件夹名，输入cd 粘贴文件夹名/bin</p>
<p><code>cd pycharm-community-2020.2.2/bin</code></p>
<p>回车，如上图所示红线下面的命令</p>
<p>然后输入 <code>./pycharm.sh</code> 回车开始安装</p>
<p>然后会弹出对话框</p>
<p><img src="/images/labsever/image-20211101205511516.png" alt="image-20211101205511516"></p>
<p><img src="/images/labsever/image-20211101205539700.png" alt="image-20211101205539700"></p>
<p>选择界面风格</p>
<p><img src="/images/labsever/image-20211101205632388.png" alt="image-20211101205632388"></p>
<p>选一个好看的点击next</p>
<p>下一个界面不要选择创建直接下一步就完成了安装</p>

        <h4 id="下次怎么进入pycharm">
          <a href="#下次怎么进入pycharm" class="heading-link"><i class="fas fa-link"></i></a><a href="#下次怎么进入pycharm" class="headerlink" title="下次怎么进入pycharm"></a>下次怎么进入pycharm</h4>
      <p>先给它创建一个快捷方式，在桌面上右键选择create launcher</p>
<p><img src="/images/labsever/image-20211101211141456.png" alt="image-20211101211141456"></p>
<p>输入名字，点击command旁边的小文件夹，找到pycharm的bin文件夹找到pycharm.sh选中点击open</p>
<p><img src="/images/labsever/image-20211101211401520.png" alt="image-20211101211401520"></p>
<p>点击icon给它选择一个图标，search icon中选择image files，在弹出的选择目录中还在这个bin目录里面可以找到这个图标，选择点击ok就好啦</p>
<p><img src="/images/labsever/image-20211101211556431.png" alt="image-20211101211556431"></p>
<p><img src="/images/labsever/image-20211101211710022.png" alt="image-20211101211710022"></p>
<p><img src="/images/labsever/image-20211101211747217.png" alt="image-20211101211747217"></p>
<p>打上第一个勾点击create，桌面上就有pycharm了，双击打开点击launch anyway就打开了</p>
<p><img src="/images/labsever/image-20211101211825065.png" alt="image-20211101211825065"></p>
<p><img src="/images/labsever/image-20211101211905407.png" alt="image-20211101211905407"></p>

        <h3 id="三、pytorch环境搭建">
          <a href="#三、pytorch环境搭建" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、pytorch环境搭建" class="headerlink" title="三、pytorch环境搭建"></a>三、pytorch环境搭建</h3>
      <p>右击桌面找到Applications-&gt;system-&gt;Xfce Terminal点击打开终端</p>
<p><img src="/images/labsever/image-20211102102926834.png" alt="image-20211102102926834"></p>
<p>1、创建虚拟环境, -n后面的是想要的环境名</p>
<p><code>conda create -n torchgpu python=3.8（torchgpu是想要的环境名，叫什么都可以，2.7、3.6等）</code></p>
<p>2、进入conda环境，在终端中输入下面指令进入到创建好的torchgpu虚拟环境</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate torchgpu</span><br></pre></td></tr></table></div></figure>

<p>3、安装pytorch</p>
<p><img src="/images/labsever/image-20211101221521349.png" alt="image-20211101221521349"></p>
<p><code>conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch</code> 复制这行代码，回车开始安装</p>
<p>出现yes/no全部yes即可</p>
<p>4、安装李沐的d2l包，里面附带有jupyter，推荐安装</p>
<p><code>pip install d2l</code></p>
<p>至此pytorch需要的一些包和环境安装完毕</p>

        <h3 id="四、尝试使用jupyter编写代码">
          <a href="#四、尝试使用jupyter编写代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#四、尝试使用jupyter编写代码" class="headerlink" title="四、尝试使用jupyter编写代码"></a>四、尝试使用jupyter编写代码</h3>
      <p>按照上面的方法打开终端并进入到torchgpu环境下，输入 <code>jupyter notebook</code>回车会弹出浏览器,即可使用jupyter编写代码</p>
<p><img src="/images/labsever/image-20211102104101453.png" alt="image-20211102104101453"></p>
<p><img src="/images/labsever/image-20211102104150415.png" alt="image-20211102104150415"></p>

        <h3 id="五、尝试使用pycharm编写代码">
          <a href="#五、尝试使用pycharm编写代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#五、尝试使用pycharm编写代码" class="headerlink" title="五、尝试使用pycharm编写代码"></a>五、尝试使用pycharm编写代码</h3>
      <p>pycharm还是跟windows上一样设置环境为创建的conda虚拟环境torchgpu</p>
<p>点击file-&gt;settings-&gt;Project Interpreter-&gt;小齿轮按钮</p>
<p><img src="/images/labsever/image-20211102110238056.png" alt="image-20211102110238056"></p>
<p>conda environments-&gt;existing enviornment点击小文件夹按钮</p>
<p>依次找到路径 /home/dxs(自己的用户名)/anaconda3/envs/torchgpu/bin/python</p>
<p>点击ok apply</p>
<p>返回之后右下角出现（torchgpu）说明环境已经切换完毕</p>
<p><img src="/images/labsever/image-20211102110856616.png" alt="image-20211102110856616"></p>

        <h3 id="六、使用gpu之前的一些注意事项">
          <a href="#六、使用gpu之前的一些注意事项" class="heading-link"><i class="fas fa-link"></i></a><a href="#六、使用gpu之前的一些注意事项" class="headerlink" title="六、使用gpu之前的一些注意事项"></a>六、使用gpu之前的一些注意事项</h3>
      <p>使用gpu之前先打开终端输入 <code>nvidia-smi</code>查看显卡的使用状态</p>
<p><img src="/images/labsever/image-20211102111112560.png" alt="image-20211102111112560"></p>
<p>可以看到第一块显卡（GPU0）的显存使用很大，我们尽量选择较为空闲的显卡比如GPU7，以避免与师兄师姐争用显存，从GPU7显存使用2MiB可见较为空闲，因此在训练的代码中注意将训练设备设置成CUDA:7 即GPU7</p>
<p>原代码：</p>
<p><img src="/images/labsever/image-20211102111531575.png" alt="image-20211102111531575"></p>
<p>只使用GPU7的改正的代码：</p>
<p><img src="/images/labsever/image-20211102111612414.png" alt="image-20211102111612414"></p>

        <h3 id="七、利用XFTP7从本地Windows传文件到服务器">
          <a href="#七、利用XFTP7从本地Windows传文件到服务器" class="heading-link"><i class="fas fa-link"></i></a><a href="#七、利用XFTP7从本地Windows传文件到服务器" class="headerlink" title="七、利用XFTP7从本地Windows传文件到服务器"></a>七、利用XFTP7从本地Windows传文件到服务器</h3>
      <p>Xftp7安装：</p>
<p><span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.netsarang.com/zh/free-for-home-school/">家庭/学校免费 - NetSarang Website</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p><img src="/images/labsever/image-20211211102014039.png" alt="image-20211211102014039"></p>
<p>勾选只需Xftp即可，会向所留邮箱发送下载链接</p>
<p>正常安装一直下一步即可</p>
<p>打开软件，在弹出的会话窗口点击新建</p>
<p><img src="/images/labsever/image-20211211102501213.png" alt="image-20211211102501213"></p>
<p><img src="/images/labsever/image-20211211102842186.png" alt="image-20211211102842186"></p>
<p>点击确定后连接，在左上角点击文件-&gt;新建本地选项卡后出现以下界面</p>
<p><img src="/images/labsever/image-20211211103242759.png" alt="image-20211211103242759"></p>
<p>此时左边就是windows，右边是Linux服务器文件系统，两边可以互相直接拖曳文件以及文件夹实现文件互传</p>
<p>到此，实验室服务器探索暂时告一段落，以后有需要会接着尝试。</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/10/24/dalunwen/">《**基于深度学习跨模态行人再识别系统的研究与实现**》阅读</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-10-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-10-28</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="师兄大论文《基于深度学习跨模态行人再识别系统的研究与实现》阅读">
          <a href="#师兄大论文《基于深度学习跨模态行人再识别系统的研究与实现》阅读" class="heading-link"><i class="fas fa-link"></i></a><a href="#师兄大论文《基于深度学习跨模态行人再识别系统的研究与实现》阅读" class="headerlink" title="师兄大论文《基于深度学习跨模态行人再识别系统的研究与实现》阅读"></a>师兄大论文《<strong>基于深度学习跨模态行人再识别系统的研究与实现</strong>》阅读</h3>
      
        <h4 id="一、论文提出问题与名词解释">
          <a href="#一、论文提出问题与名词解释" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、论文提出问题与名词解释" class="headerlink" title="一、论文提出问题与名词解释"></a>一、论文提出问题与名词解释</h4>
      
        <h5 id="1-1-行人再识别-Re-ID">
          <a href="#1-1-行人再识别-Re-ID" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-行人再识别-Re-ID" class="headerlink" title="1.1 行人再识别 Re-ID"></a>1.1 行人再识别 Re-ID</h5>
      <p>行人再识别的背景主要是判利用计算机视觉技术判断图像或者视频序列中是否存在特定的行人。其主要应当包括两个方面：行人检测与行人识别。</p>
<p>行人检测方面主要完成利用计算机视觉相关技术判断图像或视频序列中是否包含行人，如果包含行人则对每个行人标注独立的行人框，并将这些行人框裁剪提供给行人再识别系统进行身份识别。</p>
<p>行人再识别系统完成的功能是根据裁剪后输入的多个行人图片判断是否为同一行人。</p>

        <h5 id="1-2-跨模态行人再识别-IV-ReID">
          <a href="#1-2-跨模态行人再识别-IV-ReID" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-跨模态行人再识别-IV-ReID" class="headerlink" title="1.2 跨模态行人再识别 IV-ReID"></a>1.2 跨模态行人再识别 IV-ReID</h5>
      <p>当照明条件不佳的时候，RGB摄像头往往表现不佳，但是红外摄像头则可以良好工作。如果给定一个特定的人的可见（或红外图像），系统跨模态的从其他光谱相机中捕获的图库中搜索相应的红外（或可见光）图像可以实现更好的效果。这种交叉模态图像匹配任务称为跨模态行人再识别(IV-REID)</p>

        <h4 id="二、行人检测部分">
          <a href="#二、行人检测部分" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、行人检测部分" class="headerlink" title="二、行人检测部分"></a>二、行人检测部分</h4>
      <p>对于行人检测部分，应当归类为目标检测问题。师兄采用的行人检测网络是anchor free检测网络中的CenterNet网络。</p>

        <h5 id="2-1-Anchor-free网络模型">
          <a href="#2-1-Anchor-free网络模型" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-Anchor-free网络模型" class="headerlink" title="2.1 Anchor free网络模型"></a>2.1 Anchor free网络模型</h5>
      
        <h6 id="2-1-1-什么是Anchor">
          <a href="#2-1-1-什么是Anchor" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-1-什么是Anchor" class="headerlink" title="2.1.1 什么是Anchor"></a>2.1.1 什么是Anchor</h6>
      <p>Anchor是在图像上预设好的不同大小，不同长宽比的参照框。借助神经网络强大的拟合能力，不需要再计算Haar/Hog等特征。网络直接输出每个anchor包含（或者说与物体有较大重叠，也就是IoU较大的）物体的概率，以及被检测物体相对于本Anchor的中心点偏移以及长宽比例。如下图</p>
<p><img src="/images/dalunwen/image-20211012094040232.png" alt="image-20211012094040232"></p>
<p>因为anchor的位置都是固定的，所以就可以很容易的换算出来实际物体的位置。以图中的小猫为例，红色的anchor就以99%的概率认为它是一只猫，并同时给出了猫的实际位置相对于该anchor的偏移量，这样，我们将输出解码后就得到了实际猫的位置，如果它能通过NMS（非最大抑制）筛选，它就能顺利的输出来。但是，绿色的anchor就认为它是猫的概率就很小，紫色的anchor虽然与猫有重叠，但是概率只有26%。在训练的时候，也就是给每张图片的物体的Bounding Box，相对于anchor进行编码，如果物体的Bounding Box与某个anchor的IoU较大，例如大于0.5就认为是正样本，否则是负样本（当然，也有算法将大于0.7的设为正样本，小于0.3的算负样本，中间的不计算损失）。</p>

        <h6 id="2-1-2-什么是IoU（Intersection-over-Union）">
          <a href="#2-1-2-什么是IoU（Intersection-over-Union）" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-2-什么是IoU（Intersection-over-Union）" class="headerlink" title="2.1.2 什么是IoU（Intersection over Union）"></a>2.1.2 什么是IoU（Intersection over Union）</h6>
      <p>IoU是一种测量在特定数据集中检测相应物体准确度的一个标准。IoU是一个简单的测量标准，只要是在输出中得出一个预测范围(bounding boxex)的任务都可以用IoU来进行测量。为了可以使IoU用于测量任意大小形状的物体检测，我们需要：</p>
<ul>
<li><p>ground-truth bounding boxes（人为在训练集图像中标出要检测物体的大概范围）</p>
</li>
<li><p>我们的算法得出的结果范围。</p>
</li>
</ul>
<p><strong>这个标准用于测量真实和预测之间的相关度，相关度越高该值越高。</strong>其<strong>计算方法是两个区域重叠的部分除以两个区域的集合部分得出的结果</strong>，通过设定的IoU阈值，与IoU计算结果进行比较。如图2所示：</p>
<img src="/images/dalunwen/image-20211012202618543.png" alt="image-20211012202618543" style="zoom:50%;">

<p>​                                                                                                                        图2 IoU的计算公式</p>
<img src="/images/dalunwen/image-20211012202816848.png" alt="image-20211012202816848" style="zoom:75%;">


        <h6 id="2-1-3-CenterNet">
          <a href="#2-1-3-CenterNet" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-3-CenterNet" class="headerlink" title="2.1.3 CenterNet"></a>2.1.3 CenterNet</h6>
      <p>CenterNet正是Anchor Free的网络模型。CenterNet 首次提出了运用关键点检测算法确定目标中心点。<strong>CenterNet的基本思想是在确定目标中心点的前提下预测目标的宽高。</strong>CenterNet采用了关键点检测的方法，对特征的每个区域取8邻域最大值作为极值点，然后保留100个极值点，作为目标框的中心点，并且通过设置一定的阈值滤除低质量的目标中心点。CenterNet的网络框架如图所示：</p>
<p><img src="/images/dalunwen/image-20211014212357048.png" alt="image-20211014212357048"></p>

        <h5 id="2-2-本论文对CenterNet的改进">
          <a href="#2-2-本论文对CenterNet的改进" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-本论文对CenterNet的改进" class="headerlink" title="2.2 本论文对CenterNet的改进"></a>2.2 本论文对CenterNet的改进</h5>
      <p>论文提出了两个对CenterNet的改进方向。由于CenterNet的基本思路是先利用预测中心点算法预测目标中心点，再在中心点的基础上预测宽高。出现检测不准的情况可能是1、网络未预测正确的目标中心点，后面宽高预测分支即使十分精准也会出现误检框，2、目标中心点预测精准但是而宽高预测分支未输出正确的宽高，那么也会造成误检框。并为此改进了网络的结构与损失函数。</p>

        <h6 id="2-2-1-网络结构的改进">
          <a href="#2-2-1-网络结构的改进" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-1-网络结构的改进" class="headerlink" title="2.2.1 网络结构的改进"></a>2.2.1 网络结构的改进</h6>
      <p>对于中心预测分支主要采取的方法是提取特征图8-临域内最大值作为目标的中心点。由于有时会出现极值点并不是目标的中心点的情况，师兄在上图的原有CenterNet的结构中加入了注意力网络中的全局上下文模块（Global Context Block）。</p>
<p>注意力网络可以使局部区域加权后特征值变大，我们利用此特性进行训练并约束网络，使训练图片的目标中心点分配较高的权重，非目标中心点分配较低的权重，使特征图的极值点均为目标中心点，避免产生中心点误检的情况。</p>

        <h6 id="2-2-2-损失函数改进">
          <a href="#2-2-2-损失函数改进" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-2-损失函数改进" class="headerlink" title="2.2.2 损失函数改进"></a>2.2.2 损失函数改进</h6>
      <p>CenterNet的预测分支的损失函数是Smooth L1 loss，原理是对检测框进行宽高的回归优化，但是没有将检测框视作一个整体进行优化，导致检测网络的精度较低。</p>
<p>CIOU loss，此损失函数兼顾了预测框与真实框的相交程度、欧式距离、长宽比等多个因素，且较为容易收敛，旨在使预测框更加符合真实框,其公式为：<img src="/images/dalunwen/image-20211024143007780.png" alt="image-20211024143007780">。</p>
<p>其中，<img src="/images/dalunwen/image-20211024143042745.png" alt="image-20211024143042745" style="zoom:65%;">表示预测框与真实框中心点的欧式距离，c表示预测框和真实框的最小外接矩形的对角线距离。<img src="/images/dalunwen/image-20211024143117350.png" alt="image-20211024143117350" style="zoom:67%;">表示真实框与目标框长宽比的距离。</p>

        <h4 id="三、跨模态行人再识别部分">
          <a href="#三、跨模态行人再识别部分" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、跨模态行人再识别部分" class="headerlink" title="三、跨模态行人再识别部分"></a>三、跨模态行人再识别部分</h4>
      
        <h5 id="3-1-评估指标">
          <a href="#3-1-评估指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-评估指标" class="headerlink" title="3.1 评估指标"></a>3.1 评估指标</h5>
      
        <h6 id="3-1-1-CMC曲线">
          <a href="#3-1-1-CMC曲线" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-1-CMC曲线" class="headerlink" title="3.1.1 CMC曲线"></a>3.1.1 CMC曲线</h6>
      <p>CMC曲线全称为累计匹配曲线，是图像检索领域的重要检测指标。在行人再识别测试时，分别输入查询目标库的图片（query）和候选库图片（gallery），计算查询库中每一个行人与候选库中的每一个行人的相似度，并根据相似度进行排序，相似度列表由一个二维矩阵表示。相似度列表的横坐标为n，表示相似度排名，纵坐标为Rank_n，表示排序靠前的行人与目标行人具有相同ID的概率，例如，某个模型进行测试，取排序前十的图片进行分析，前十中共有5张正确图片被召回，这5张正确图片的排序下标为1，2，5，7，8，那么该模型的rank_1为100%，rank_5为60%，rank_10为50%。因此，可以根据此指标来判断模型的分类能力。</p>

        <h6 id="3-1-2-mAP-平均检索精度">
          <a href="#3-1-2-mAP-平均检索精度" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-2-mAP-平均检索精度" class="headerlink" title="3.1.2 mAP 平均检索精度"></a>3.1.2 mAP 平均检索精度</h6>
      <p>当gallery图库中出现大量的同一个行人的图片时，被召回率会大大提高，在rank机制下，误判的图片几乎不起作用，此时Rank就不能很好地判断模型的好坏，于是研究人员提出使用平均检索精度（mAP）来评估算法的优劣。mAP为查询目标库中所有图片的检索精度的平均值，即query中所有图片查准率的平均值。例如，有两个模型验证性能的好坏，我们进行测试，第一个模型有4个正确图片被召回，第二个模型也有4个正确图片被召回，第一个模型被召回的正确图片排序下标为1，2，3，5，第二个模型被召回的正确图片排序下标为1，3，5，6。那么第一个模型的平均准确率为（1/1+2/2+3/3+4/5）/4=0.95，第二个模型的平均准确率为（1/1+2/3+3/5+4/6）/4=0.75，从上述计算可以看出第一个模型效果更好，因此，此指标有力的弥补了Rank指标的缺陷。</p>

        <h5 id="3-2-跨模态行人再识别算法的思路">
          <a href="#3-2-跨模态行人再识别算法的思路" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2-跨模态行人再识别算法的思路" class="headerlink" title="3.2 跨模态行人再识别算法的思路"></a>3.2 跨模态行人再识别算法的思路</h5>
      <p>跨模态行人再识别主要是使用RGB图片与红外图片做跨模态学习。RGB图片通常具有较高的空间分辨率和可观的细节和明暗对比，因此，它们适合于人类的视觉感知。然而，这些图像很容易受到恶劣条件的影响，如光照差、雾和恶劣天气。但是，描述物体热辐射的红外图像具有一定抗干扰的能力，但其通常分辨率较低，纹理较差。因此，红外图片与RGB图片所具有的共性集中在纹理、轮廓、图案等外观信息，比如同一个行人穿有一件带有logo图案的衣服，那么不管是他的红外图片还是他的RGB图片都会有显眼的logo图案，且图案纹理轮廓相似度极大。</p>
<p>神经网络的较浅层提取的特征主要关注图像的纹理颜色等外观细节，而提取的深层特征则包含语义信息。我们认为红外图片和可见光图片的低层次的外观特征具有更高相似性，所以学习低层次特征能够得到更具有可辨性的共性特征。</p>

        <h5 id="3-3-具体算法框架">
          <a href="#3-3-具体算法框架" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-具体算法框架" class="headerlink" title="3.3 具体算法框架"></a>3.3 具体算法框架</h5>
      <p><img src="/images/dalunwen/image-20211024190307293.png" alt="image-20211024190307293"></p>
<p>网络架构是采用常用的跨模态网络结构——双流网络，模型的骨干网络是ResNet-50，以前的方法只采用最深层的特征来编码图片，例如来自ResNet-50最后一个卷积层的输出。最后一个卷积层输出的是深层特征，即有关于图片的语义信息。尽管高级特征对于形成抽象概念用于物体识别确实有用，但它们可能会丢弃颜色和纹理等低级信号，这些信息是人物识别的重要线索。此外，卷积神经网络深层特征的分辨率较小，可能无法看到细节，如衣服上的图案，面部特征，细微的姿势差异等。这表明提取多层次特征，并且利用多层次特征的信息优势互补有利于行人重识别任务。</p>

        <h6 id="3-3-1-特征提取">
          <a href="#3-3-1-特征提取" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-1-特征提取" class="headerlink" title="3.3.1 特征提取"></a>3.3.1 特征提取</h6>
      <p>如上文所述，师兄认为深层特征会忽略图片的一些纹理细节，而这些细节对于身份识别是有帮助的。因此师兄分别提取RGB图像、红外图像在ResNet-50的block_2、block_3、block_4层的输出作为浅层特征、中层特征、深层特征，分别用X1、X2、X3、X4、X5、X6表示。然后分别对两种图片的浅层、中层、深层特征表示做向量拼接作为特征融合。融合后的特征分别用B1、B2、B3表示。公式如下。</p>
<p><img src="/images/dalunwen/image-20211024204302667.png" alt="image-20211024204302667"></p>
<p>concatenate表示向量拼接，相比特征直接相加，其主要优势是特征融合前后特征维度不变，保证信息不会丢失，X1、X4表示RGB图片和红外图片的浅层特征，X2、X5表示RGB图片和红外图片的中层特征，X3、X6表示RGB图片和红外图片的深层特征，0表示在batch维度上进行拼接，即RGB特征和红外特征进行融合，消除模态差异。</p>

        <h6 id="3-3-2-特征分割">
          <a href="#3-3-2-特征分割" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-2-特征分割" class="headerlink" title="3.3.2 特征分割"></a>3.3.2 特征分割</h6>
      <p><strong>将图像特征分割成不同的局部特征能够显著提高行人再识别的准确性。其次，全局特征能够捕捉最显著的外观特征，而局部特征可以捕捉图像的细节</strong>，将图像特征分成局部特征和全局特征已经得到广泛的应用。由于<strong>低层特征分辨率更高，包含更多位置、细节信息，但是由于经过的卷积更少，其语义性更低，噪声更多，而高层特征具有更强的语义信息，但是分辨率很低，对细节的感知能力较差，中层特征介于两者之间。</strong>因此对高层层和中层特征X3,X2做特征分割。对于X1不分割，对于X2分割为两个局部特征，对于X3分割为三个局部特征，这样分割更加符合人体结构构造，例如上半身，下半身（或头、上半身、下半身）。且通过实验结果表明，这种分割的方法达到的效果最好。其公式如下，</p>
<p><img src="/images/dalunwen/image-20211024211727622.png" alt="image-20211024211727622"></p>
<p>P1、P2为中层特征的局部特征，P3、P4、P5为深层特征的局部特征。同时将低层次特征、中层次特征和高层次特征的全局特征保留，其公式如下，</p>
<p><img src="/images/dalunwen/image-20211024211859494.png" alt="image-20211024211859494"></p>
<p>正如之前写的卷积神经网络那篇博文，池化操作的<strong>首要作用是降采样汇合结果中的一个元素对应于原输入数据的一个子区域（sub-region），因此汇合相当于在空间范围内做了维度约减（spatially dimension reduction），从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。</strong>其次池化操作还具有<strong>降维、去除冗余信息、对特征进行压缩、简化网络复杂度、减小计算量、减小内存消耗等等。</strong>因此对B1,B2,B3做池化操作提取出三个层次的全局特征G1,G2,G3。</p>
<p>同时，对分割出的局部特征P1-P5作降维操作得到局部特征。旨在减少运算量并滤除冗余信息，其公式如下，</p>
<p><img src="/images/dalunwen/image-20211024212619478.png" alt="image-20211024212619478"></p>
<p>其中，R1、R2、R3、R4、R5表示三个层次的局部特征，即分割后的特征在通道维度上降维为256得到局部特征。</p>

        <h6 id="3-3-3-对于全局特征的损失函数">
          <a href="#3-3-3-对于全局特征的损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-3-对于全局特征的损失函数" class="headerlink" title="3.3.3 对于全局特征的损失函数"></a>3.3.3 对于全局特征的损失函数</h6>
      <p>对于全局特征求两个损失函数，分别是用于度量学习的三元组损失（Triplet loss）和用于分类的交叉熵损失（Softmax loss）。对于全局特征B1、B2、B3采用交叉熵损失函数和三元组损失函数联合优化，其公式如下，</p>
<p><img src="/images/dalunwen/image-20211024212738320.png" alt="image-20211024212738320"></p>
<p>其中，L1、L3、L7表示三个层次的全局特征的三元组损失，L2、L4、L8表示三个层次的全局特征的交叉熵损失。</p>

        <h6 id="3-3-4-对于局部特征的损失函数">
          <a href="#3-3-4-对于局部特征的损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-4-对于局部特征的损失函数" class="headerlink" title="3.3.4 对于局部特征的损失函数"></a>3.3.4 对于局部特征的损失函数</h6>
      <p>对于局部特征仅仅采用交叉熵损失函数进行优化，原因是<strong>局部特征可能会出现特征未对齐问题，导致局部特征可能存在巨大变化，因此，三元组损失在训练期间可能会破坏模型优化。</strong>其公式如下，</p>
<p><img src="/images/dalunwen/image-20211024212909969.png" alt="image-20211024212909969"></p>
<p>其中，L5、L6表示中层局部特征的交叉熵损失，L9、L10、L11表示深层局部特征的交叉熵损失。</p>

        <h6 id="3-3-5-总损失函数">
          <a href="#3-3-5-总损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3-5-总损失函数" class="headerlink" title="3.3.5 总损失函数"></a>3.3.5 总损失函数</h6>
      <p>由于学习低层次特征可以获得更多的跨模态共性特征，所以我们提取了网络中的多层次特征。然而，我们发现<strong>不同任务损失的尺度差异非常大，如果采取简单相加的方式，整体损失函数将不会是最佳的，导致网络模型得不到充分的优化</strong>。</p>
<p>因此，我们采用多任务学习的方法，结合多个损失函数，利用同<strong>方差不确定性</strong>同时学习多个目标。我们将同方差不确定性解释为依赖于任务的加权。我们<strong>设置了三个可学习的超参数，分别集成到每个任务的损失中，三个噪声参数分别作为低层特征、中层特征和高层特征损失的权重因子。然后，将所有经过适当加权的损失相加，得到最优的总损失，从而达到对不同层次特征进行优化的目的</strong>。因此，我们最终的总损失函数为:</p>
<p><img src="/images/dalunwen/image-20211024213329932.png" alt="image-20211024213329932"></p>

        <h4 id="四-结论">
          <a href="#四-结论" class="heading-link"><i class="fas fa-link"></i></a><a href="#四-结论" class="headerlink" title="四 结论"></a>四 结论</h4>
      <p>师兄分别在行人再识别的两个部分进行改进，在行人检测部分对网络结构进行改进，在CenterNet的基础上加入了注意力模块，提升了预测目标中心点的精度。在损失函数部分将CenterNet的损失函数改进为CIOU，将检测框视作一个整体进行优化。</p>
<p>在行人识别部分使用了跨模态的学习方法。采用了以ResNet-50为主干网络的双流网络去分别提取RGB图像与红外图像的低、中、高层特征。并采取向量拼接作为特征融合消除模态间差异。并采取特征分割将特征分为全局特征与局部特征。全局特征的提取方法是分别对特征融合后的低中高层特征作最大值池化操作。为了更好的提取细节特征，对中层与高层特征作特征分割之后将分割后的深层特征降维处理滤除冗余信息得到局部特征。并分别对局部特征与全局特征提出了对应的损失函数。最终取得了较好的效果</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/10/24/ReID-summary/">《无监督领域自适应行人重识别研究进展》阅读</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-10-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-07</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="《无监督领域自适应行人重识别研究进展》阅读">
          <a href="#《无监督领域自适应行人重识别研究进展》阅读" class="heading-link"><i class="fas fa-link"></i></a><a href="#《无监督领域自适应行人重识别研究进展》阅读" class="headerlink" title="《无监督领域自适应行人重识别研究进展》阅读"></a>《<strong>无监督领域自适应行人重识别研究进展</strong>》阅读</h3>
      
        <h4 id="一、无监督领域自适应行人重识别（Unsupervised-Domain-Adaptation-Person-Re">
          <a href="#一、无监督领域自适应行人重识别（Unsupervised-Domain-Adaptation-Person-Re" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、无监督领域自适应行人重识别（Unsupervised-Domain-Adaptation-Person-Re" class="headerlink" title="一、无监督领域自适应行人重识别（Unsupervised  Domain  Adaptation Person  Re-"></a>一、无监督领域自适应行人重识别（Unsupervised  Domain  Adaptation Person  Re-</h4>
      <p>identification, UDA Re-ID）</p>

        <h5 id="1-1-领域自适应-Domain-Adaption">
          <a href="#1-1-领域自适应-Domain-Adaption" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-领域自适应-Domain-Adaption" class="headerlink" title="1.1 领域自适应 Domain Adaption"></a>1.1 领域自适应 Domain Adaption</h5>
      <p><em>Domain Adaption</em>是<em>transfer leanring</em>（迁移学习）中很重要的一项内容。主要目的是将具有不同分布的（<em>data distribution</em>）的具有标签（<em>label</em>）的源域（<em>source domain</em>）和不带标签的目标域（<em>target domain</em>） 映射（<em>map</em>）到同一个特征空间（<em>embedding mainfold</em>）。即使训练好的模型能够很好的泛化到其他领域中。</p>
<p><em><strong>Domain</strong></em>可以看作是一个服从相同分布的一类数据，而训练集由一个或多个<em>Domain</em>组成。</p>
<p>DA的一个基础理论如下图所示</p>
<p><img src="/images/ReID-summary/image-20211206143021042.png" alt="image-20211206143021042"></p>
<p>其中<em>Target risk</em>的上界由<em>Source risk，Complexity of H</em>,与<em>Source-target distribution divergence</em>组成，其中<em>Source risk</em>直接由源域给出，<em>Complexity of H</em>指的是模型的复杂程度，一般来说是一个常量，因此一般来说方便优化或者下降的点就是<em>Source-target distribution divergence</em>，即想办法减小源域与目标域分布的差距，所以在训练的过程中<em>DA</em>需要直接访问目标域的数据。</p>

        <h5 id="1-2-无监督">
          <a href="#1-2-无监督" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-无监督" class="headerlink" title="1.2 无监督"></a>1.2 无监督</h5>
      <p>要求其它领域中的数据样本没有与任务有关的标签信息,  无法参与对模型的微调。由于不同领域存在不同的数据分布,  一般将数据的分布差异认为是无监督领域自适应行人重识别的关键问题,  因此研究的重点是让模型可以适应不同领域之间的差异(<em>Domain Gap</em>),减少域差对模型性能的影响。</p>
<p>无监督领域自适应一般包含两个领域（数据集），即训练集是带标签的（有监督的）源域（<em>source domain</em>），和无监督的目标域（<em>Target Domain</em>）。任务目标是能够在无重叠视阈的目标域中能够检索出同一行人.</p>

        <h4 id="二、无监督领域自适应行人重识别的算法">
          <a href="#二、无监督领域自适应行人重识别的算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、无监督领域自适应行人重识别的算法" class="headerlink" title="二、无监督领域自适应行人重识别的算法"></a>二、无监督领域自适应行人重识别的算法</h4>
      <p>无监督领域自适应行人重识别的研究方向大致可以分为以下三类：</p>
<p>1）生成满足其它领域数据分布的伪样本,在数据增广的同时,  能够缩小不同领域之间的领域差异；常见的方法是基于图像的风格迁移，即通过图像的风格迁移模型将有标签的源域目标通过图像的风格迁移生成与目标域相近的风格图片作为满足目标领域数据分布的伪样本投入模型的训练。</p>
<p>2）训练后的模型具有提取高鲁棒领域不变性特征的能力,  缓解领域间的差异；常见的方法是基于表示学习的无监<br>督领域自适应行人重识别。</p>
<p>3）提高目标域中样本特征伪标签生成的准确度,  用于目标域中模型有监督的微调.常见的方法是基于伪标签生成的无监督领域自适应行人重识别。</p>
<p><img src="/images/ReID-summary/image-20211206095839729.png" alt="image-20211206095839729"></p>
<p>​                                                 图1 无监督领域自适应行人重识别的三类研究方向</p>

        <h5 id="2-1-基于图像风格迁移的方法">
          <a href="#2-1-基于图像风格迁移的方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-基于图像风格迁移的方法" class="headerlink" title="2.1 基于图像风格迁移的方法"></a>2.1 基于图像风格迁移的方法</h5>
      
        <h6 id="2-1-1-问题的提出">
          <a href="#2-1-1-问题的提出" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-1-问题的提出" class="headerlink" title="2.1.1 问题的提出"></a>2.1.1 问题的提出</h6>
      <p>一直以来,  在基于无监督的领域自适应行人重识别中,  不同领域内的光照、行人背景、相机角度、行人姿势、分辨率以及行人衣着风格等差异造成的领域差异被专家学者认为是造成重识别性能低的首要原因。因此如何缩小领域差异成为基于无监督的领域自适应行人重识别研究的一项关键科学问题。</p>

        <h6 id="2-1-2-主要方法">
          <a href="#2-1-2-主要方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-2-主要方法" class="headerlink" title="2.1.2 主要方法"></a>2.1.2 主要方法</h6>
      <p>基于图像风格迁移的主要思路是将有监督的源域图像通过神经网络生成具有目标域图像风格的源域图像投入训练，实现减小源域与目标域分布的差距。主要的思路大致有：</p>
<p><strong>1、基于领域差异的风格迁移</strong></p>
<p>生成较高质量的有着目标域图像风格的源域图像,  方便特征提取网络可以有监督的进行训练。</p>
<p><strong>2、基于背景差异的风格迁移</strong></p>
<p>背景差异被认为是造成领域差异的主要因素之一,  将解决无监督领域自适应行人重识别问题的工作重点放在行人图像的背景上。该方法主要训练了一个应用于行人重识别的行人转换生成对抗网络(Person  Transfer  GAN, PTGAN),  该网络能够在尽可能保证行人前景不变的前提下,  将背景转换成期望数据集的图像背景风格。</p>
<p><strong>3、基于相机差异的风格迁移</strong></p>
<p>域间和域内摄像机间由于所在位置和角度的不同而产生的视角差异也是影响识别效果的重要因素。该种方法是对源域相同行人的图片作不同视角差异的风格迁移，生成的图片将用于数据增强以提升网络对因相机视角变化导致样本间特征差异的适应能力。</p>
<p><strong>4、基于其他差异的风格迁移</strong></p>
<p>基于无监督的领域自适应行人重识别中,  除了行人背景差异和相机差异对领域差异造成的巨大影响,  不同领域间行人图像也存在着其它差异, 诸如光照、行人姿势、分辨率的变化.这些差异也是加大领域间差异的原因之一。</p>

        <h5 id="2-2-基于表示学习的方法">
          <a href="#2-2-基于表示学习的方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-基于表示学习的方法" class="headerlink" title="2.2 基于表示学习的方法"></a>2.2 基于表示学习的方法</h5>
      <p>基于表示学习的无监督领域自适应行人重识别<strong>旨在让模型从未标记的跨域数据中学习有效的嵌入空间</strong>,  获得的嵌入特征具有与领域变化无关的高鉴别力和很好的领域适应性。</p>

        <h6 id="2-2-1-基于损失函数的表示学习方法">
          <a href="#2-2-1-基于损失函数的表示学习方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-1-基于损失函数的表示学习方法" class="headerlink" title="2.2.1 基于损失函数的表示学习方法"></a>2.2.1 基于损失函数的表示学习方法</h6>
      <p>基于损失函数的表示学习方法是数据训练过程中利用损失函数来约束网络,  让网络朝着损失函数减小的方向进行优化.基于无监督的领域自适应行人重识别常用的损失函数根据功能进行分类, 可分为分类损失函数、度量学习损失函数和分布损失函数三种类型。</p>
<p>（1）、分类损失函数</p>
<p>分类损失函数是将行人再识别问题看作一个多分类问题。分类损失函数是衡量网络预测和真实分类的一种损失函数,  通过训练使网络具有预测分类的能力。常见的有行人身份损失、行人属性损失和视角不变损失。</p>
<p>行人身份损失是将源域中k个行人身份的N张图片看作K分类问题，用SoftMax进行计算分类损失函数。</p>
<p>行人属性损失是将每张图片都具有M个属性的标注，则行人属性损失可看作是属性二分类预测的累加形式,  即预测图片𝑥是否拥有第𝑚个属性，该损失采用Sigmoid交叉熵损失函数。</p>
<p>视角不变损失是为了增强补贴相机视角下的模型泛化性，减轻目标中不同视角风格的影响。实际工作是将不同摄像机的视角加入考虑，对摄像机进行标签标注，标注源域每张图片来自的摄像机标签。即预测图片来自的相机的索引。</p>
<p>（2）、度量学习损失函数</p>
<p>度量学习损失函数常用于评价数据分布中的距离,  其中对比损失函数(Contrastive loss)、三元组损失函数(Triplet  loss)和四元组损失函数（Quadruplet loss）[31]是行人重识别最常使用度量损失函数。</p>

        <h6 id="2-2-2-基于注意力机制的表示学习方法">
          <a href="#2-2-2-基于注意力机制的表示学习方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-2-基于注意力机制的表示学习方法" class="headerlink" title="2.2.2 基于注意力机制的表示学习方法"></a>2.2.2 基于注意力机制的表示学习方法</h6>
      <p>注意力机制（Attention Mechanism）是机器学习常用的数据处理方法之一,  被广泛应用于自然语言处理、计算机视觉、语音识别 等深度学习任务中。通过对人类注意机制的模仿和模型在大量数据上的训练,  能让模型具备在众多信息中关注对当前任务更关键的信息提取能力。</p>
<p>注意力机制大多应用在弱化行人图像背景、增强行人目标信息等方面。</p>

        <h6 id="2-2-3-基于局部特征的表示方法">
          <a href="#2-2-3-基于局部特征的表示方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-3-基于局部特征的表示方法" class="headerlink" title="2.2.3 基于局部特征的表示方法"></a>2.2.3 基于局部特征的表示方法</h6>
      <p>在无监督领域自适应行人重识别研究中,  一些工作认为局部特征相比全局特征,  不容易受到领域差异的影响,  具有较高的领域适应能力。因此基于局部特征的方法也常作为有效的手段被研究人员所采用。局部特征最常见的获取方式有手动分块或人体关键点定位分块两种形式。</p>

        <h5 id="2-3-基于伪标签生成的方法">
          <a href="#2-3-基于伪标签生成的方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-基于伪标签生成的方法" class="headerlink" title="2.3 基于伪标签生成的方法"></a>2.3 基于伪标签生成的方法</h5>
      <p>由于基于无监督的领域自适应行人重识别中目标域图像不包含行人身份标签,  给基于无监督的领域自适应行人<br>重识别研究带来极大的挑战。</p>
<p>基于伪标签生成的方法大多是训练模型自动在目标域上生成较为可靠的伪标签，利用生成的伪标签去有监督的指导网络优化，提高模型的泛化水平。这里的伪标签不仅仅局限在给予行人对应身份的身份标签（ID label）,  也可以以能够正确的区分样本的正例样本对和负例样本的样本关系形式存在以满足样本度量学习的需要。</p>
<p>一类是基于样本特征间距离比较的排序建立伪标签；第二类是利用在特征空间聚类后形成的簇给予对应的伪标签；最后一类则是综合前两类的优势,  联合排序和聚类的伪标签生成方法。生成的这些伪标签将作为目标域中行人身份标签或正负样本对对已在源域训练后的模型进行微调,  最终获得模型。</p>

        <h6 id="2-3-1-基于排序的伪标签生成方法">
          <a href="#2-3-1-基于排序的伪标签生成方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-1-基于排序的伪标签生成方法" class="headerlink" title="2.3.1 基于排序的伪标签生成方法"></a>2.3.1 基于排序的伪标签生成方法</h6>
      <p>基于排序（Ranking-based）的伪标签生成方法也称为基于邻近（Neighbor-based）的伪标签生成方法,  是一种较易理解的伪标签生成方法,  即根据样本在特征空间间的距离排序来划分类别的一种方法。经过由源域训练后的神经网络模型提取出目标域行人图片的全部特征。并对目标域样本的特征以两两对应的形式作距离（相似度）的计算。距离可以使用马氏距离、欧氏距离以及余弦距离。距离计算之后对距离进行排序，常用的算法有K-近邻（K-NN),相互近邻（𝑘-reciprocal nearest neighbors, 𝑘-RNN ）排序算法以及𝑘互近邻编码（𝑘-reciprocal encoding）,大致的思路均是将距离最邻近的K个样本视为有可能相同的一类，并以此作为伪标签从而微调网络。</p>
<p>而基于距离排序的伪标签生成工作又可以分为基于目标域与源域间的距离排序以及基于目标域的距离排序两种。基于目标域与源域之间的距离排序，由于源域中是有身份标签的。 对目标域无标签的行人图像可根据该图像特征与源域间的多细粒度特征间的余弦距离排序给予对应的源域身份标签.这些筛选过的目标域图像将和源域一起组成训练集,  共同指导模型微调。</p>
<p>其它的基于距离排序的伪标签方法使用的是目标域间距离排序的方式。由于目标域的样本无身份标签,  因此基于目标域的距离排序生成的伪标签是伪关系标签。</p>

        <h6 id="2-3-2-基于聚类的伪标签生成">
          <a href="#2-3-2-基于聚类的伪标签生成" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-2-基于聚类的伪标签生成" class="headerlink" title="2.3.2 基于聚类的伪标签生成"></a>2.3.2 基于聚类的伪标签生成</h6>
      <p>基于聚类的伪标签生成算法会使用聚类算法, 例如𝐾-mean、DBSCAN,  通过样本间距离或者样本分布密度迭代完成聚类,  最终形成的同簇内样本对应相同的行人身份标签,  不同的簇间样本对应不同的行人身份标签。</p>
<p>基于聚类的伪标签生成方法有着理解简单、容易实现的优势,  能够预测出具体的伪身份标签。但是聚类算法对噪声样本较为敏感,  容易对模型的优化和后续的聚类产生影响.因此噪声样本是所有基于聚类的伪标签生成方法亟待解决的挑战之一。</p>

        <h4 id="三、数据集与性能评估指标">
          <a href="#三、数据集与性能评估指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、数据集与性能评估指标" class="headerlink" title="三、数据集与性能评估指标"></a>三、数据集与性能评估指标</h4>
      
        <h5 id="3-1-什么是图库集（Gallery-Set）和探针集-Probe-Set">
          <a href="#3-1-什么是图库集（Gallery-Set）和探针集-Probe-Set" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1-什么是图库集（Gallery-Set）和探针集-Probe-Set" class="headerlink" title="3.1 什么是图库集（Gallery Set）和探针集(Probe Set)"></a>3.1 什么是图库集（Gallery Set）和探针集(Probe Set)</h5>
      <p>一般来说图片数据集会分为<strong>训练集（Train Set）与测试集（Test Set)<strong>。而在Re-ID的数据集中一般</strong>测试集</strong>又会细分成<strong>图库集（Gallery Set）和探针集(Probe Set)或查询集</strong>。</p>
<p><strong>无论是gallery还是probe都是仅在测试集出现的概念。</strong>Re-ID的任务是提供一张行人照片，从众多的数据库中寻找与之具有相同身份的图片。</p>
<p>下面参考博客：原文链接：<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44273380/article/details/108949031">https://blog.csdn.net/weixin_44273380/article/details/108949031</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>gallery<br>gallery原意：画廊，这里博主喜欢将它翻译为注册集，也有同学翻译为参考集。就像我们说的“注册”，它的作用就好比一个人脸识别系统，每个人都进去注册了几张自己脸的图像，并和自己的身份绑定起来，从而形成我们上面提到的这个巨大系统的“数据库”。测试的时候，我们需要把一张新的照片去“画廊”，也就是注册数据库中一个一个匹配，得到结果。</p>
<p>这里注意，gallery是只有测试集才有的，因为训练的时候我们的期望是模型能根据两张标注好的图像更好地提取特征，以及判断相似度。这个过程的数据来源是标注好的图像，目标仅仅是训练模型对提供的图片的特征提取能力，也就不需要gallery来提供参考。</p>
<p>probe<br>probe原意：探针、调查。这里博主就通俗地翻译为查询集，就是说，我们在测试的时候是在probe中选取元素来到gallery寻找的，最终测试阶段对模型性能的评估是根据probe中元素查询的效果来反映的。</p>

        <h5 id="3-2-常用数据集">
          <a href="#3-2-常用数据集" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2-常用数据集" class="headerlink" title="3.2 常用数据集"></a>3.2 常用数据集</h5>
      <ul>
<li><p>CUHK03 图像数据集<br>CUHK03是香港中文大学2014 年开源的第一个可供深度学习使用的大型图像行人重识别数据集.数据集采集自香港中文大学校园内的 10 个摄像头,总共收集包含 1467 个行人的 13164 张图片, 行人包围框由 DPM 和手工检测并标示. 在CUHK03 的行人重识别中,分为两种训练测试标准.第一种是随机选出 100 个行人作为测试集,1160个行人作为训练集,100 个行人作为验证集,重复二十次,这种训练测试标准被称为 Single-shot setting;第二种则是类似于 Market-1501,它将数据集分为包含 767 个行人的训练集和包含 700 个行人的测试集。</p>
</li>
<li><p>Market-1501 图像数据集</p>
<p>Market-1501是清华大学 2015 年开源的图像行人重识别数据集。数据集中的行人图像采集自校园超市的6 个摄像头,包含1501 个行人的32668张行人图片。开源者采用 DPM 算法将图片检测和裁剪成128×64 大小的行人图片。数据集分为3 个集合,包括由751 个行人,总计12936 张图片组成的训练集;测试集中则包含剩余 750 个人,总计 12936张图片组成的图库集(Gallery Set),以及750 人的其它3368 张图片组成的探针集(Probe Set)。</p>
</li>
<li><p>DukeMTMC-Re-ID 图像数据集</p>
<p>DukeMTMC-Re-ID是悉尼科技大学2017 年开源的图像行人重识别数据集。数据集采集自 8 个不同角度的摄像头,总计包含1812 个行人的36411张图片。数据集中行人边界框由人工手动裁剪。与Market1501 相同,数据集分为3 个集合,包括由702个行人的 16522 张图像组成的训练集;测试集包含1110 个行人的 17661 张图片组成的图库集和图库集中存在的702 个行人的其它2228 张图片组成探针集。</p>
</li>
<li><p>MSMT17 图像数据集</p>
<p>MSMT17是由北京大学 2019 年开源的大型图像行人重识别数据集.数据集的采集涵盖了多个场景和多个时段,更接近于真实场景.数据集由 15个摄像头采集,包含 12 个户外摄像头和 3 个室内摄像头.采集过程中,选择了具有不同天气条件的 4天时间,涵盖每天早上、中午和下午三个时间段的3 个小时的视频,总计 180 个小时的视频.在数据集整理和标注上,采用 FasterRCNN[81]作为行人检测器,并用了 2 个月的时间进行人工标注,最终得到4101 个行人的 126441 张图片.与 Market1501 和DukeMTMC-Re-ID 不同,数据集中训练集和测试集的行人大约按照 1:3 的比例划分,其中训练集包含1041 个行人的 32621 个包围框,测试集包含 3060个行人的93820 个包围框.其中测试集中,随机选择11659 个包围框作为 ProbeSet,其它 82161 个包围框作为Gallery。        </p>
</li>
</ul>

        <h5 id="3-3性能评价指标">
          <a href="#3-3性能评价指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3性能评价指标" class="headerlink" title="3.3性能评价指标"></a>3.3性能评价指标</h5>
      <p>行人重识别模型性能的评价指标通常采用累计匹配曲线(Cumulative  Match  Characteristics,CMC)和平均精度均值(Mean Average Precision,mAP)进行评估。</p>
<p>累积匹配特征曲线 CMC[k]或 Rank-k表示在测试集中当查找的探针（Probe）在图库集（Gallery Set）中进行距离比较后,  将查询集中行人按照距离的远近由小到大进行排序,  前 k个搜索结果中行人匹配到的概率。例如 Rank-1 表示第一次就能在 Gallery  Set 中正确匹配的 Probe 数量与 Probe Set 数量之比,  Rank-5 表示前五次能在 Gallery  Set中正确匹配的Probe 数量与Probe Set 数量之比。假设测试集探针集总共包含𝑁个行人,  即共进行𝑁次查询和排序后,  每次查询目标行人能匹配到的排序结果用𝑟 = (𝑟1,𝑟2,…,𝑟A)表示,  𝑟i表示探针集中第𝑖个样本在Gallery Set 中正确匹配排序, CMC 曲线表示为：</p>
<p><img src="/images/ReID-summary/image-20211207161911344.png" alt="image-20211207161911344"></p>
<p>平均精度均值是对查询集中查询样本的平均精度（Average  Precision, AP）取平均值,  表示为： </p>
<p><img src="/images/ReID-summary/image-20211207161941214.png" alt="image-20211207161941214"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/10/20/LeNet/">传统网络实现之LeNet</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-10-20</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-10-22</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h3 id="传统网络实现之LeNet">
          <a href="#传统网络实现之LeNet" class="heading-link"><i class="fas fa-link"></i></a><a href="#传统网络实现之LeNet" class="headerlink" title="传统网络实现之LeNet"></a>传统网络实现之LeNet</h3>
      
        <h4 id="LeNet简介">
          <a href="#LeNet简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#LeNet简介" class="headerlink" title="LeNet简介"></a>LeNet简介</h4>
      <p>LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由 AT&amp;T 贝尔实验室的研究员 Yann LeCun 在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeCun 发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p>

        <h4 id="1-网络结构">
          <a href="#1-网络结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-网络结构" class="headerlink" title="1 网络结构"></a>1 网络结构</h4>
      <p>LeNet-5的网络结构如下图所示，由结构图可知LeNet是一个较为简单的神经网络，它包含了深度学习的基本模块如卷积层、池化层、全连接层等等。</p>
<img src="/images/LeNet/image-20211020191956320.png" alt="image-20211020191956320" style="zoom:50%;">

<p>​                                图1 LeNet-5的简化版示意图</p>
<p>LeNet的每个卷积块的基本单元是一个卷积层、一个sigmod激活函数和一个平均池化层。每个卷积层使用5×5的卷积核和一个sigmoid激活函数。</p>
<p>本次实验使用的数据集仍然是Fashion-mnist数据集，输入大小为28×28的图片。</p>

        <h4 id="2-代码实现">
          <a href="#2-代码实现" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-代码实现" class="headerlink" title="2 代码实现"></a>2 代码实现</h4>
      
        <h5 id="2-1-使用到的库">
          <a href="#2-1-使用到的库" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-使用到的库" class="headerlink" title="2.1 使用到的库"></a>2.1 使用到的库</h5>
      <p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="2-2-定义网络结构">
          <a href="#2-2-定义网络结构" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-定义网络结构" class="headerlink" title="2.2 定义网络结构"></a>2.2 定义网络结构</h5>
      <p>为了避免错误输入尺寸不是28×28的图片，因此定义的输入层应当具有将数据resize成28×28的格式的功能。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义reshape层，实现的功能为继承Module类，将输入reshape成单通道28*28的黑白图片，第一维是数据的批量大小</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reshape</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># LeNet</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    Reshape(),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(), <span class="comment"># &quot;(28+4-5+1)=28&quot; 公式中的ph实际上是2倍的padding值</span></span><br><span class="line">    <span class="comment"># 因为torch分别在上下左右都加上padding值</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># (28+2-2)/2=14</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),  <span class="comment"># (14-5+1)=10</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># (10+2-2)/2=5</span></span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="2-3-工具函数类">
          <a href="#2-3-工具函数类" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-工具函数类" class="headerlink" title="2.3 工具函数类"></a>2.3 工具函数类</h5>
      <p>定义在SoftMax那章的工具函数类，实现的功能是创建一个Accumulator类，Accumulator类创建的对象有着data属性，它是一个长度为n的列表，可以调用add（）函数实现data列表对应下标累加。在本代码中作用为创建一个Accumulator(3)对象，该列表的两个下标分别保存训练集总损失、训练集预测正确的总数与全部标签的数量，这样就可以利用前两个除以第三个来计算平均loss与预测正确率。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Accumulator</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br></pre></td></tr></table></div></figure>

<p>`</p>
<p>定义两个计算准确率的函数</p>
<p><code>accuracy(y_hat, y)</code>函数实现的功能是统计一批输出的预测的数量。实现的功能是对一个batch_size个数据做预测，输出的y_hat是batch_size个10分类one_hot编码，对它做行方向上的argmax可以得到类别，如[0,0,1,0,0,0,0,0,0,0]作argmax得到预测的类别是第2类。y是标签数据，标注编号描述0T-shirt/top（T恤）1Trouser（裤子）2Pullover（套衫）3Dress（裙子）4Coat（外套）5Sandal（凉鞋）6Shirt（汗衫）7Sneaker（运动鞋）8Bag（包）9Ankle boot（踝靴）。cmp是一个保存y和y_hat相等结果布尔值的tensor，对这个tensor做sum（）可以得到预测正确的数量。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;y_hat为一批预测的张量，为batch_size个长度为10的one_hot编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;对one_hot编码在行（axis0为这批预测的数量）上做argmax得出预测的类别，y为batch_size个实数标签&quot;&quot;&quot;</span></span><br><span class="line">    y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y <span class="comment"># argmax之后的y_hat是一个0-9的实数，与y作比较，相同的</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></div></figure>

<p>`</p>
<p><code>evaluate_accuracy(net, data_iter)</code>函数实现的功能是评估网络在测试集上的准确率。创建一个长度为2的Accumulator()对象，它的data第一个元素保存预测正确的数量，第二个利用y.numel()得到预测的总数并累加到第二个元素，最后利用第一个元素÷第二个元素的到正确率。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>]/metric[<span class="number">1</span>]</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="2-4-初始化网络参数与优化器">
          <a href="#2-4-初始化网络参数与优化器" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-4-初始化网络参数与优化器" class="headerlink" title="2.4 初始化网络参数与优化器"></a>2.4 初始化网络参数与优化器</h5>
      <p>对线性层和卷积层的参数做Xavier初始化，采用随机梯度下降SGD算法作为优化器，并采用交叉熵损失作为损失函数。</p>
<p>`</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化网络参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></div></figure>

<p>`</p>

        <h5 id="2-5-训练函数">
          <a href="#2-5-训练函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-5-训练函数" class="headerlink" title="2.5 训练函数"></a>2.5 训练函数</h5>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_eopch</span>(<span class="params">net, train_iter, loss, updater</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        updater.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        updater.step()</span><br><span class="line">        metric.add(l, accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, updater, loss, train_iter, test_iter, num_epoch</span>):</span></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">            train_metrics = train_eopch(net, train_iter, loss, updater)</span><br><span class="line">            test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch:<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,训练集损失:<span class="subst">&#123;train_metrics[<span class="number">0</span>]&#125;</span>,训练集准确率:<span class="subst">&#123;train_metrics[<span class="number">1</span>]&#125;</span>,测试集准确率:<span class="subst">&#123;test_acc&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">20</span></span><br><span class="line">train(net, optimizer, loss, train_iter, test_iter, num_epochs)</span><br></pre></td></tr></table></div></figure>




        <h4 id="3-结果展示">
          <a href="#3-结果展示" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-结果展示" class="headerlink" title="3 结果展示"></a>3 结果展示</h4>
      
        <h5 id="原网络的数据比较">
          <a href="#原网络的数据比较" class="heading-link"><i class="fas fa-link"></i></a><a href="#原网络的数据比较" class="headerlink" title="原网络的数据比较"></a>原网络的数据比较</h5>
      <p>原网络使用了Sigmoid激活函数与平均值池化，20轮epoch，学习率采用0.9，可以见到最终的测试集准确率来到了0.845左右，相较于softmax似乎提升不大</p>
<p><img src="/images/LeNet/image-20211020192838633.png" alt="image-20211020192838633"></p>

        <h5 id="将平均池化改成最大值池化，其他不变，上涨了一点">
          <a href="#将平均池化改成最大值池化，其他不变，上涨了一点" class="heading-link"><i class="fas fa-link"></i></a><a href="#将平均池化改成最大值池化，其他不变，上涨了一点" class="headerlink" title="将平均池化改成最大值池化，其他不变，上涨了一点"></a>将平均池化改成最大值池化，其他不变，上涨了一点</h5>
      <p><img src="/images/LeNet/image-20211020195102120.png" alt="image-20211020195102120"></p>

        <h5 id="保持学习率0-9不变，采取最大值池化-ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况">
          <a href="#保持学习率0-9不变，采取最大值池化-ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况" class="heading-link"><i class="fas fa-link"></i></a><a href="#保持学习率0-9不变，采取最大值池化-ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况" class="headerlink" title="保持学习率0.9不变，采取最大值池化+ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况"></a>保持学习率0.9不变，采取最大值池化+ReLU激活函数，出现了损失很大完全不降低，测试集准确率一直很低的情况</h5>
      <p><img src="/images/LeNet/image-20211022205227138.png" alt="image-20211022205227138"></p>
<p>百度了相关资料，推测可能是学习率过大且ReLU函数对学习率敏感，较大的学习率可能出现较大的梯度，较大梯度冲击导致神经元死亡。</p>

        <h5 id="尝试降低学习率至0-3">
          <a href="#尝试降低学习率至0-3" class="heading-link"><i class="fas fa-link"></i></a><a href="#尝试降低学习率至0-3" class="headerlink" title="尝试降低学习率至0.3"></a>尝试降低学习率至0.3</h5>
      <p><img src="/images/LeNet/image-20211022210608129.png" alt="image-20211022210608129"></p>
<p>达到了最好成绩约0.89</p>

        <h4 id="4-总结">
          <a href="#4-总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h4>
      <p>LeNet作为较早的卷积神经网络模型，比起只采用一层softmax的方法有了一定的提升。接下来可以尝试采取更深层的网络模型进一步提升精度。</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/10/17/week-summary002/">每周计划与总结002</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-10-17</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-10-17</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h4 id="上周总结">
          <a href="#上周总结" class="heading-link"><i class="fas fa-link"></i></a><a href="#上周总结" class="headerlink" title="上周总结"></a>上周总结</h4>
      <p>十月直接摆烂，现在时间是2021年10月17日晚上8点59分，傻逼RNG直接跟我一起摆烂。。。。。。</p>
<p><img src="/images/week%20summary001/image-20211009160317243.png" alt="image-20211009160317243"></p>
<p>除了基本上看完了动手学深度学习的第六章，其他的啥也没干。理由找是肯定能找，但其实并不是什么借口，<del>下周一定不摆烂，少打原神少看TMD RNG</del>。</p>

        <h4 id="下周（10-18-10-24）计划">
          <a href="#下周（10-18-10-24）计划" class="heading-link"><i class="fas fa-link"></i></a><a href="#下周（10-18-10-24）计划" class="headerlink" title="下周（10.18-10.24）计划"></a>下周（10.18-10.24）计划</h4>
      <ul>
<li><p>《动手学深度学习》第七章实现到VGG（需要学习checkpoints的使用和尝试使用Colab），看沐神的精读论文第一和第二个视频，读AlexNet那篇论文（中特课）</p>
</li>
<li><p>至少把师兄的大论文读完，并把读论文笔记写完</p>
</li>
<li><p>六级试卷*1（还有tm不到两个月能不能别裸考）</p>
</li>
<li><p>《算法导论》1-3章（周一上午读两章）</p>
</li>
<li><p>《c++ Primer》3-4章</p>
</li>
<li><p>矩阵论、数理统计复习</p>
</li>
</ul>
<p>下周再摆烂直接tm抽自己。</p>
<p><img src="/images/week%20summary001/image-20211009160233635.png" alt="image-20211009160233635"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/10/17/zip/">Python中的zip()函数</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-10-17</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-03-06</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h4 id="Zip-函数">
          <a href="#Zip-函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#Zip-函数" class="headerlink" title="Zip()函数"></a>Zip()函数</h4>
      <p><strong>zip()</strong> 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。</p>
<p>如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。</p>
<p><img src="/images/zip/image-20211017104901386.png" alt="image-20211017104901386"></p>
</div></div></article></section><nav class="paginator"><div class="paginator-inner"><a class="extend prev" rel="prev" href="/page/5/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/7/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/photo.png" alt="avatar"></div><p class="sidebar-ov-author__text">To be a great person.</p></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Strive</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>